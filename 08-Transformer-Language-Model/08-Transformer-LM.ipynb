{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Language Model - Code Autocompleter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torchtext, datasets, math\n",
    "from tqdm import tqdm\n",
    "\n",
    "from queue import PriorityQueue\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/MARC/.cache/huggingface/datasets/codeparrot___parquet/codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (C:/Users/MARC/.cache/huggingface/datasets/codeparrot___parquet/codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['repo_name', 'path', 'license', 'content'],\n",
      "    num_rows: 47452\n",
      "})\n",
      "Dataset({\n",
      "    features: ['repo_name', 'path', 'license', 'content'],\n",
      "    num_rows: 11864\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load CodeParrot's Jupyter-Code-to-Text from HuggingFace  \n",
    "train_set = datasets.load_dataset('codeparrot/github-jupyter-code-to-text', split='train')\n",
    "test_set  = datasets.load_dataset('codeparrot/github-jupyter-code-to-text', split='test')\n",
    "\n",
    "print(train_set)\n",
    "print(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\n",
      "from tensorflow import keras\n",
      "from tensorflow.keras import layers\n",
      "\n",
      "\"\"\"\n",
      "Explanation: Simple MNIST convnet\n",
      "Author: fchollet<br>\n",
      "Date created: 2015/06/19<br>\n",
      "Last modified: 2020/04/21<br>\n",
      "Description: A simple convnet that achieves ~99% test accuracy on MNIST.\n",
      "Setup\n",
      "End of explanation\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "# Model / data parameters\n",
      "num_classes = 10\n",
      "input_shape = (28, 28, 1)\n",
      "\n",
      "# the data, split between train and test sets\n",
      "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
      "\n",
      "# Scale images to the [0, 1] range\n",
      "x_train = x_train.astype(\"float32\") / 255\n",
      "x_test = x_test.astype(\"float32\") / 255\n",
      "# Make sure images have shape (28, 28, 1)\n",
      "x_train = np.expand_dims(x_train, -1)\n",
      "x_test = np.expand_dims(x_test, -1)\n",
      "print(\"x_train shape:\", x_train.shape)\n",
      "print(x_train.shape[0], \"train samples\")\n",
      "print(x_test.shape[0], \"test samples\")\n",
      "\n",
      "\n",
      "# convert class vectors to binary class matrices\n",
      "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
      "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
      "\n",
      "\"\"\"\n",
      "Explanation: Prepare the data\n",
      "End of explanation\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "model = keras.Sequential(\n",
      "    [\n",
      "        keras.Input(shape=input_shape),\n",
      "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
      "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
      "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
      "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
      "        layers.Flatten(),\n",
      "        layers.Dropout(0.5),\n",
      "        layers.Dense(num_classes, activation=\"softmax\"),\n",
      "    ]\n",
      ")\n",
      "\n",
      "model.summary()\n",
      "\n",
      "\"\"\"\n",
      "Explanation: Build the model\n",
      "End of explanation\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "batch_size = 128\n",
      "epochs = 15\n",
      "\n",
      "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
      "\n",
      "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
      "\n",
      "\"\"\"\n",
      "Explanation: Train the model\n",
      "End of explanation\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "score = model.evaluate(x_test, y_test, verbose=0)\n",
      "print(\"Test loss:\", score[0])\n",
      "print(\"Test accuracy:\", score[1])\n",
      "\n",
      "\"\"\"\n",
      "Explanation: Evaluate the trained model\n",
      "End of explanation\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_set['content'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "# https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "\"\"\"\n",
      "Explanation: Copyright 2020 The TensorFlow Authors.\n",
      "End of explanation\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "# Import Tokenizer and pad_sequences\n",
      "import tensorflow as tf\n",
      "from tensorflow.keras.preprocessing.text import Tokenizer\n",
      "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
      "\n",
      "# Import numpy and pandas\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "Explanation: Tokenize and sequence a bigger corpus of text\n",
      "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
      "  <td>\n",
      "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l09c03_nlp_prepare_larger_text_corpus.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
      "  </td>\n",
      "  <td>\n",
      "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l09c03_nlp_prepare_larger_text_corpus.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
      "  </td>\n",
      "</table>\n",
      "\n",
      "So far, you have written some test sentences and generated a word index and then created sequences for the sentences. \n",
      "Now you will tokenize and sequence a larger body of text, specifically reviews from Amazon and Yelp. \n",
      "About the dataset\n",
      "You will use a dataset containing Amazon and Yelp reviews of products and restaurants. This dataset was originally extracted from Kaggle.\n",
      "The dataset includes reviews, and each review is labelled as 0 (bad) or 1 (good). However, in this exercise, you will only work with the reviews, not the labels, to practice tokenizing and sequencing the text. \n",
      "Example good reviews:\n",
      "\n",
      "This is hands down the best phone I've ever had.\n",
      "Four stars for the food & the guy in the blue shirt for his great vibe & still letting us in to eat !\n",
      "\n",
      "Example bad reviews:\n",
      "\n",
      "A lady at the table next to us found a live green caterpillar In her salad\n",
      "If you plan to use this in a car forget about it.\n",
      "\n",
      "See more reviews\n",
      "Feel free to download the dataset from a drive folder belonging to Udacity and open it on your local machine to see more reviews.\n",
      "End of explanation\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "path = tf.keras.utils.get_file('reviews.csv', \n",
      "                               'https://drive.google.com/uc?id=13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P')\n",
      "print (path)\n",
      "\n",
      "\"\"\"\n",
      "Explanation: Get the corpus of text\n",
      "The combined dataset of reviews has been saved in a Google drive belonging to Udacity. You can download it from there.\n",
      "End of explanation\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "# Read the csv file\n",
      "dataset = pd.read_csv(path)\n",
      "\n",
      "# Review the first few entries in the dataset\n",
      "dataset.head()\n",
      "\n",
      "\"\"\"\n",
      "Explanation: Get the dataset\n",
      "Each row in the csv file is a separate review.\n",
      "The csv file has 2 columns:\n",
      "\n",
      "text (the review)\n",
      "sentiment (0 or 1 indicating a bad or good review)\n",
      "End of explanation\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "# Get the reviews from the text column\n",
      "reviews = dataset['text'].tolist()\n",
      "\n",
      "\"\"\"\n",
      "Explanation: Get the reviews from the csv file\n",
      "End of explanation\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
      "tokenizer.fit_on_texts(reviews)\n",
      "\n",
      "word_index = tokenizer.word_index\n",
      "print(len(word_index))\n",
      "print(word_index)\n",
      "\n",
      "\n",
      "\"\"\"\n",
      "Explanation: Tokenize the text\n",
      "Create the tokenizer, specify the OOV token, tokenize the text, then inspect the word index.\n",
      "End of explanation\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "sequences = tokenizer.texts_to_sequences(reviews)\n",
      "padded_sequences = pad_sequences(sequences, padding='post')\n",
      "\n",
      "# What is the shape of the vector containing the padded sequences?\n",
      "# The shape shows the number of sequences and the length of each one.\n",
      "print(padded_sequences.shape)\n",
      "\n",
      "# What is the first review?\n",
      "print (reviews[0])\n",
      "\n",
      "# Show the sequence for the first review\n",
      "print(padded_sequences[0])\n",
      "\n",
      "# Try printing the review and padded sequence for other elements.\n",
      "\n",
      "\"\"\"\n",
      "Explanation: Generate sequences for the reviews\n",
      "Generate a sequence for each review. Set the max length to match the longest review. Add the padding zeros at the end of the review for reviews that are not as long as the longest one.\n",
      "End of explanation\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_set['content'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove comments from the codes\n",
    "import re\n",
    "\n",
    "comment_pattern = r\"(^\\s*#.*$)\"\n",
    "block_comment_pattern = r\"(\\\"\\\"\\\")(.*?)(\\\"\\\"\\\")\"\n",
    "\n",
    "train_clean = list()\n",
    "for code in train_set['content']:\n",
    "    code = re.sub(comment_pattern, \"\", code, flags=re.MULTILINE)\n",
    "    code = re.sub(block_comment_pattern, \"\", code, flags=re.DOTALL)\n",
    "    train_clean.append(code)\n",
    "\n",
    "test_clean = list()\n",
    "for code in test_set['content']:\n",
    "    code = re.sub(comment_pattern, \"\", code, flags=re.MULTILINE)\n",
    "    code = re.sub(block_comment_pattern, \"\", code, flags=re.DOTALL)\n",
    "    test_clean.append(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\n",
      "from tensorflow import keras\n",
      "from tensorflow.keras import layers\n",
      "\n",
      "\n",
      "\n",
      "num_classes = 10\n",
      "input_shape = (28, 28, 1)\n",
      "\n",
      "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
      "\n",
      "x_train = x_train.astype(\"float32\") / 255\n",
      "x_test = x_test.astype(\"float32\") / 255\n",
      "\n",
      "x_train = np.expand_dims(x_train, -1)\n",
      "x_test = np.expand_dims(x_test, -1)\n",
      "print(\"x_train shape:\", x_train.shape)\n",
      "print(x_train.shape[0], \"train samples\")\n",
      "print(x_test.shape[0], \"test samples\")\n",
      "\n",
      "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
      "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "model = keras.Sequential(\n",
      "    [\n",
      "        keras.Input(shape=input_shape),\n",
      "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
      "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
      "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
      "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
      "        layers.Flatten(),\n",
      "        layers.Dropout(0.5),\n",
      "        layers.Dense(num_classes, activation=\"softmax\"),\n",
      "    ]\n",
      ")\n",
      "\n",
      "model.summary()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "batch_size = 128\n",
      "epochs = 15\n",
      "\n",
      "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
      "\n",
      "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "score = model.evaluate(x_test, y_test, verbose=0)\n",
      "print(\"Test loss:\", score[0])\n",
      "print(\"Test accuracy:\", score[1])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_clean[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "import tensorflow as tf\n",
      "from tensorflow.keras.preprocessing.text import Tokenizer\n",
      "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "path = tf.keras.utils.get_file('reviews.csv', \n",
      "                               'https://drive.google.com/uc?id=13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P')\n",
      "print (path)\n",
      "\n",
      "\n",
      "\n",
      "dataset = pd.read_csv(path)\n",
      "\n",
      "dataset.head()\n",
      "\n",
      "\n",
      "\n",
      "reviews = dataset['text'].tolist()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
      "tokenizer.fit_on_texts(reviews)\n",
      "\n",
      "word_index = tokenizer.word_index\n",
      "print(len(word_index))\n",
      "print(word_index)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "sequences = tokenizer.texts_to_sequences(reviews)\n",
      "padded_sequences = pad_sequences(sequences, padding='post')\n",
      "\n",
      "\n",
      "print(padded_sequences.shape)\n",
      "\n",
      "print (reviews[0])\n",
      "\n",
      "print(padded_sequences[0])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_clean[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['import numpy as np', 'from tensorflow import keras', 'from tensorflow.keras import layers', 'num_classes = 10', 'input_shape = (28, 28, 1)']\n",
      "['import tensorflow as tf', 'from tensorflow.keras.preprocessing.text import Tokenizer', 'from tensorflow.keras.preprocessing.sequence import pad_sequences', 'import numpy as np', 'import pandas as pd']\n",
      "4984055 1238709\n"
     ]
    }
   ],
   "source": [
    "# Divide the codes into sentences\n",
    "train_sents = [sent for code in train_clean for sent in code.split('\\n') if sent != '']\n",
    "test_sents  = [sent for code in test_clean for sent in code.split('\\n') if sent != '']\n",
    "\n",
    "print(train_sents[:5])\n",
    "print(test_sents[:5])\n",
    "print(len(train_sents), len(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295359\n",
      "73939\n"
     ]
    }
   ],
   "source": [
    "# Select only the sentences starting with \"import\" or \"from\"\n",
    "# because the full datasets are too big to train or test\n",
    "\n",
    "small_train = [sent for sent in train_sents if re.match(r'^(import|from)', sent)]\n",
    "small_test  = [sent for sent in test_sents if re.match(r'^(import|from)', sent)]\n",
    "\n",
    "print(len(small_train))\n",
    "print(len(small_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['import', 'numpy', 'as', 'np'], ['from', 'tensorflow', 'import', 'keras'], ['from', 'tensorflow.keras', 'import', 'layers'], ['import', 'sys'], ['import', 'os']]\n",
      "[['import', 'tensorflow', 'as', 'tf'], ['from', 'tensorflow.keras.preprocessing.text', 'import', 'Tokenizer'], ['from', 'tensorflow.keras.preprocessing.sequence', 'import', 'pad_sequences'], ['import', 'numpy', 'as', 'np'], ['import', 'pandas', 'as', 'pd']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the selected sentences\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "train_tokenized = [tokenizer(sent) for sent in small_train]\n",
    "test_tokenized  = [tokenizer(sent) for sent in small_test]\n",
    "\n",
    "print(train_tokenized[:5])\n",
    "print(test_tokenized[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['import', 'numpy', 'as', 'np'], ['from', 'tensorflow', 'import', 'keras'], ['from', 'tensorflow.keras', 'import', 'layers'], ['import', 'sys'], ['import', 'os']]\n",
      "[['import', 'tensorflow', 'as', 'tf'], ['from', 'tensorflow.keras.preprocessing.text', 'import', 'Tokenizer'], ['from', 'tensorflow.keras.preprocessing.sequence', 'import', 'pad_sequences'], ['import', 'numpy', 'as', 'np'], ['import', 'pandas', 'as', 'pd']]\n"
     ]
    }
   ],
   "source": [
    "# Remove \",\" from the tokens\n",
    "train_tokenized = [[token for token in sent if token != \",\"] for sent in train_tokenized]\n",
    "test_tokenized  = [[token for token in sent if token != \",\"] for sent in test_tokenized]\n",
    "\n",
    "print(train_tokenized[:5])\n",
    "print(test_tokenized[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(236287, 59072, 73939)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the train tokens into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_tokenized, val_tokenized = train_test_split(train_tokenized, test_size=0.2, random_state=SEED)\n",
    "\n",
    "len(train_tokenized), len(val_tokenized), len(test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# Create torchtext's Vocab object \n",
    "vocab = build_vocab_from_iterator(train_tokenized, \n",
    "                                  min_freq=1,\n",
    "                                  specials=special_symbols,\n",
    "                                  special_first=True)\n",
    "                                       \n",
    "# Set UNK_IDX as the default index\n",
    "vocab.set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34863\n"
     ]
    }
   ],
   "source": [
    "# Check the size of the vocabulary\n",
    "vocab_size = len(vocab) \n",
    "print(vocab_size)                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', '<sos>', '<eos>', 'import', 'from', 'as', 'numpy', 'np', 'plt']\n"
     ]
    }
   ],
   "source": [
    "print(vocab.get_itos()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./vocab.pkl\", \"wb\") as file:\n",
    "    pickle.dump(vocab, file)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from', 'scipy.spatial.distance', 'import', 'pdist']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenized[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Preparing data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []                                                   \n",
    "    for example in dataset:\n",
    "        #appends eos so we know it ends....so model learn how to end...                             \n",
    "        tokens = example + ['<eos>']   \n",
    "        #numericalize          \n",
    "        tokens = [vocab[token] for token in tokens] \n",
    "        data.extend(tokens)                                    \n",
    "    data = torch.LongTensor(data)                                 \n",
    "    num_batches = data.shape[0] // batch_size \n",
    "    data = data[:num_batches * batch_size]                       \n",
    "    data = data.view(batch_size, num_batches)          \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 8979]) torch.Size([128, 2247]) torch.Size([128, 2819])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_data = get_data(train_tokenized, vocab, batch_size)\n",
    "valid_data = get_data(val_tokenized, vocab, batch_size)\n",
    "test_data = get_data(test_tokenized, vocab, batch_size)\n",
    "\n",
    "print(train_data.shape, valid_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  5, 311,   4, 816,   3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from\n",
      "scipy.spatial.distance\n",
      "import\n",
      "pdist\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "mapping = vocab.get_itos()\n",
    "\n",
    "for tokens in train_data[0][:5]:\n",
    "    print(mapping[tokens.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        #Q = [batch size, query len, hid dim]\n",
    "        #K = [batch size, key len, hid dim]\n",
    "        #V = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        #Q = [batch size, n heads, query len, head dim]\n",
    "        #K = [batch size, n heads, key len, head dim]\n",
    "        #V = [batch size, n heads, value len, head dim]\n",
    "                \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        #energy = [batch size, n heads, query len, key len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        #attention = [batch size, n heads, query len, key len]\n",
    "                \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #x = [batch size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        #x = [batch size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        #x = [batch size, seq len, pf dim]\n",
    "        \n",
    "        x = self.fc_2(x)\n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am using Batched Beam Search, where instead of feeding each hypothesis one by one, which takes a lot of time;  I simply concat everything into one list and feed them all at once, which is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, device, pad_idx, max_length = 100):\n",
    "                \n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim, \n",
    "                                                  dropout, \n",
    "                                                  device)\n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def make_mask(self, x):\n",
    "        \n",
    "        #x = [batch size, len]\n",
    "        \n",
    "        pad_mask = (x != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #pad_mask = [batch size, 1, 1, len]\n",
    "        \n",
    "        x_len = x.shape[1]\n",
    "        \n",
    "        sub_mask = torch.tril(torch.ones((x_len, x_len), device = self.device)).bool()\n",
    "        #sub_mask = [len, len]\n",
    "            \n",
    "        mask = pad_mask & sub_mask\n",
    "        #mask = [batch size, 1, len, len]\n",
    "        \n",
    "        return mask \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, len]\n",
    "                \n",
    "        batch_size = x.shape[0]\n",
    "        x_len      = x.shape[1]\n",
    "        \n",
    "        #get mask here since we remove seq2seq class\n",
    "        mask   = self.make_mask(x)\n",
    "        #mask = [batch size, 1, len, len]\n",
    "\n",
    "        pos = torch.arange(0, x_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)          \n",
    "            \n",
    "        x = self.dropout((self.tok_embedding(x) * self.scale) + self.pos_embedding(pos))\n",
    "        #x = [batch size, len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x, attention = layer(x, mask)\n",
    "        \n",
    "        #x = [batch size, len, hid dim]\n",
    "        #attention = [batch size, n heads, len, len]\n",
    "        \n",
    "        output = self.fc_out(x)\n",
    "        #output = [batch size, len, output dim]\n",
    "            \n",
    "        return output, attention\n",
    "\n",
    "    def beam_decode(self, penalty_alpha = 0.9, max_length = 5, beam_size = 5):\n",
    "        \n",
    "        # Start with SOS Import numpy as\n",
    "        prompt = 'Import numpy as '\n",
    "        \n",
    "        tokens = tokenizer(prompt)\n",
    "        indices = [SOS_IDX] + [vocab[t] for t in tokens]\n",
    "\n",
    "        decoder_input = torch.Tensor([indices]).long().to(device)\n",
    "        #decoder_input: [batch size, len] = [1, 1]\n",
    "        scores = torch.Tensor([0.]).to(device)\n",
    "        #scores: [1]\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            \n",
    "            # print(f\"========Length: {i}\")\n",
    "            \n",
    "            # Decoder prediction\n",
    "            logits, _ = self.forward(decoder_input)\n",
    "            #[beam_size, current dec len=i, vocab_size]\n",
    "                        \n",
    "            logits = logits[:, -1] \n",
    "            # Last sequence step: [beam_size, current dec len=i, vocab_size] => [beam_size, vocab_size]\n",
    "            \n",
    "            # print(f\"{logits.shape=}\")\n",
    "\n",
    "            # Softmax\n",
    "            # Log softmax is better, since beam search accumulates probability\n",
    "            # if simply softmax, the probability can get too small and then become unstable\n",
    "            log_probs = torch.log_softmax(logits, dim=1)\n",
    "    \n",
    "            # Add length penalty, otherwise, always very short sentence will win...\n",
    "            penalty   = ((5 + (i+1)) / (5 + 1)) ** penalty_alpha #see https://arxiv.org/abs/1609.08144\n",
    "            log_probs = log_probs / penalty\n",
    "            \n",
    "            # print(f\"{decoder_input[:, -1]=}\")\n",
    "            \n",
    "            # Update score where EOS has not been reached\n",
    "            log_probs[decoder_input[:, -1]==EOS_IDX, :] = -2 #discouraged it to end\n",
    "            log_probs[decoder_input[:, -1]==UNK_IDX, :] = -10 #very discouraged to spit out unk\n",
    "            scores = scores.unsqueeze(1) + log_probs \n",
    "            # scores: [beam_size, vocab_size]\n",
    "            # log_probs: [beam_size, vocab_size]\n",
    "\n",
    "            # print(f\"{log_probs.shape=}\")\n",
    "            # print(f\"{scores.shape=}\")\n",
    "            #log_probs: torch.Size([1, 29475])\n",
    "            #scores.shape=torch.Size([1, 29475])\n",
    "            \n",
    "            # Flatten scores from [beams, vocab_size] to [beams * vocab_size] to get top k, and reconstruct beam indices and token indices\n",
    "            # Since we flatten it, we have to retrieve the actual beam indices and token_indices using floor division and remainder\n",
    "            # You can try on paper; it will make sense\n",
    "            scores, indices = torch.topk(scores.reshape(-1), beam_size) #scores: [beam_size]; #indices: [beam_size]\n",
    "            beam_indices  = torch.divide   (indices, self.output_dim, rounding_mode='floor') # indices // vocab_size\n",
    "            token_indices = torch.remainder(indices, self.output_dim)                        # indices %  vocab_size\n",
    "            \n",
    "            # print(f\"{scores=}\")\n",
    "            # print(f\"{indices.shape=}\")\n",
    "            \n",
    "            # print(f\"{indices=}\")\n",
    "            # print(f\"{beam_indices=}\")\n",
    "            # print(f\"{token_indices=}\")\n",
    "            \n",
    "            # Build the next decoder input\n",
    "            # For efficiency, the trick is to concatenate all hypotheses into one string and sent to decoder at once\n",
    "            # We can later chop it ...\n",
    "            next_decoder_input = []\n",
    "            for beam_index, token_index in zip(beam_indices, token_indices):\n",
    "                # print(f\"{beam_index=}\")\n",
    "                prev_decoder_input = decoder_input[beam_index]\n",
    "                # print(f\"{prev_decoder_input=}\")\n",
    "                if prev_decoder_input[-1]==EOS_IDX:\n",
    "                    token_index = EOS_IDX # once EOS, always EOS\n",
    "                token_index = torch.LongTensor([token_index]).long().to(device)\n",
    "                next_decoder_input.append(torch.cat([prev_decoder_input, token_index]))\n",
    "                # print(\"here: \" + \" \".join([vocab.lookup_token(i) for i in next_decoder_input[-1]]) + \"; score: \" + str(scores[beam_index].item()))\n",
    "            decoder_input = torch.vstack(next_decoder_input)\n",
    "            \n",
    "            # print(f\"{decoder_input=}\")\n",
    "            \n",
    "             # If all beams are finished, and the length is at least 5, exit\n",
    "            if i > 5:\n",
    "                if (decoder_input[:, -1]==EOS_IDX).sum() == beam_size:\n",
    "                    break\n",
    "                \n",
    "        # convert the top scored sequence to a list of text tokens\n",
    "        decoder_output, _ = max(zip(decoder_input, scores), key=lambda x: x[1])\n",
    "        decoder_output = decoder_output[1:].cpu().numpy() # remove SOS\n",
    "        \n",
    "        return [vocab.lookup_token(i) for i in decoder_output if i != EOS_IDX] # remove EOS if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        \n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)        \n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \n",
    "        #x = [batch size, len, hid dim]\n",
    "        #mask = [batch size, 1, len, len]\n",
    "        \n",
    "        #multi attention, skip and then norm\n",
    "        _x, attention = self.self_attention(x, x, x, mask)\n",
    "        x = self.self_attn_layer_norm(x + self.dropout(_x))\n",
    "        #x = [batch size, len, hid dim]\n",
    "        #attention = [batch size, n heads, len, len]\n",
    "    \n",
    "        #positionwise feedforward\n",
    "        _x = self.positionwise_feedforward(x)\n",
    "        x = self.ff_layer_norm(x + self.dropout(_x))\n",
    "        #x = [batch size, len, hid dim]\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hid_dim    = 256                \n",
    "dec_layers = 3               \n",
    "dec_heads  = 8\n",
    "dec_pf_dim = 512\n",
    "dec_dropout = 0.1     \n",
    "lr = 1e-3                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 19,491,631 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = Decoder(vocab_size, hid_dim, dec_layers, dec_heads, dec_pf_dim, dec_dropout, device, PAD_IDX).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    #data #[batch size, bunch of tokens]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    # data #[batch size, bunch of tokens]\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n",
    "    num_batches = data.shape[-1]\n",
    "        \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, _ = model(src)               \n",
    "\n",
    "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    decoded_batch_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            #target = [batch size, dec len]\n",
    "\n",
    "            batch_size= src.shape[0]\n",
    "            prediction, _ = model(src)\n",
    "            #prediction = [batch size, dec len, output_dim]\n",
    "            \n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "            \n",
    "    #decoding using beam_search as example (you don't need to put here, because beam_search is for intference)\n",
    "    decoded_batch = model.beam_decode()\n",
    "    print(\"Sample beam sentence: \" + \" \".join(decoded_batch))\n",
    "            \n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be using a `ReduceLROnPlateau` learning scheduler which decreases the learning rate by a factor, if the loss don't improve by a certain epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: Import numpy as np\n",
      "Epoch: 1:\n",
      "\tTrain Perplexity: 12.671\n",
      "\tValid Perplexity: 8.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: Import numpy as np\n",
      "Epoch: 2:\n",
      "\tTrain Perplexity: 7.817\n",
      "\tValid Perplexity: 8.087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: Import numpy as np\n",
      "Epoch: 3:\n",
      "\tTrain Perplexity: 6.630\n",
      "\tValid Perplexity: 7.983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: Import numpy as np\n",
      "Epoch: 4:\n",
      "\tTrain Perplexity: 6.017\n",
      "\tValid Perplexity: 8.069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: Import numpy as np\n",
      "Epoch: 5:\n",
      "\tTrain Perplexity: 5.459\n",
      "\tValid Perplexity: 7.980\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "seq_len  = 10 #<----decoding length\n",
    "clip    = 0.25\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, \n",
    "                batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
    "                seq_len, device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-val-tr_lm.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1}:')\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best-val-tr_lm.pt',  map_location=device))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. Real-world inference\n",
    "\n",
    "Here I only use pure sampling.  You may want to put the beam search here and compare.  I will leave them as your practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, _ = model(src)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "            \n",
    "            #####################################################################\n",
    "            #I only do pure sampling....\n",
    "            #you may want to compare here with top-k, top-p, and beam search here\n",
    "            #####################################################################\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = 'Harry Potter is '\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
