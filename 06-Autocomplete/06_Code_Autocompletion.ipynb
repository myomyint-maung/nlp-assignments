{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myomyint-maung/nlp-assignments/blob/main/06-Autocomplete/06_Code_Autocompletion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HG6ckKe6538"
      },
      "source": [
        "## Feb 16 - Code Autocompletion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [],
        "id": "2PyGSoAF653_"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext, datasets, math\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPTZHMo0654A",
        "outputId": "ac92ea74-92de-412d-c30b-cbf6463aadf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Choose the computing device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set SEED for reproducibility\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "YZhOnOJWJKei"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0LU9gwy654C"
      },
      "source": [
        "## 1. Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qPeE5bM654D",
        "outputId": "9ecfa580-e013-45bb-d45c-d89110285924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/codeparrot___parquet/codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "WARNING:datasets.builder:Using custom data configuration codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/codeparrot___parquet/codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['repo_name', 'path', 'license', 'content'],\n",
            "    num_rows: 47452\n",
            "})\n",
            "Dataset({\n",
            "    features: ['repo_name', 'path', 'license', 'content'],\n",
            "    num_rows: 11864\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Load CodeParrot's Jupyter-Code-to-Text from HuggingFace  \n",
        "train = datasets.load_dataset('codeparrot/github-jupyter-code-to-text', split='train')\n",
        "test  = datasets.load_dataset('codeparrot/github-jupyter-code-to-text', split='test')\n",
        "\n",
        "print(train)\n",
        "print(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmoFBa5S654E"
      },
      "source": [
        "### 2. Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove comments from the codes\n",
        "import re\n",
        "\n",
        "comment_pattern = r\"(^\\s*#.*$)\"\n",
        "block_comment_pattern = r\"(\\\"\\\"\\\")(.*?)(\\\"\\\"\\\")\"\n",
        "\n",
        "train_clean = list()\n",
        "for code in train['content']:\n",
        "    code = re.sub(comment_pattern, \"\", code, flags=re.MULTILINE)\n",
        "    code = re.sub(block_comment_pattern, \"\", code, flags=re.DOTALL)\n",
        "    train_clean.append(code)\n",
        "\n",
        "test_clean = list()\n",
        "for code in test['content']:\n",
        "    code = re.sub(comment_pattern, \"\", code, flags=re.MULTILINE)\n",
        "    code = re.sub(block_comment_pattern, \"\", code, flags=re.DOTALL)\n",
        "    test_clean.append(code)"
      ],
      "metadata": {
        "id": "YlxNSx9FKj30"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide the codes into sentences\n",
        "train_sents = [sent for code in train_clean for sent in code.split('\\n') if sent != '']\n",
        "test_sents  = [sent for code in test_clean for sent in code.split('\\n') if sent != '']\n",
        "\n",
        "print(train_sents[0], len(train_sents))\n",
        "print(test_sents[0], len(test_sents))"
      ],
      "metadata": {
        "id": "_7Dtvv0_QbQz",
        "outputId": "63595483-83a8-466e-9238-521894935c2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import numpy as np 4984055\n",
            "import tensorflow as tf 1238709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "AuG9FBx1654E",
        "outputId": "7987307c-4c2a-426e-a0bc-49ae4d88a575",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['import', 'numpy', 'as', 'np']\n",
            "['import', 'tensorflow', 'as', 'tf']\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the datasets\n",
        "tokenizer = torchtext.data.utils.get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "train_tokenized = yield_tokens(train_sents)\n",
        "test_tokenized  = yield_tokens(test_sents)\n",
        "\n",
        "print(next(iter(train_tokenized)))\n",
        "print(next(iter(test_tokenized)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove non-word, numeric and underscore strings from the train tokens\n",
        "non_word = re.compile('[\\W\\d]+')\n",
        "numeric_ = re.compile(r'[^0-9_]')\n",
        "\n",
        "train_tokens = []\n",
        "for string_list in train_tokenized:\n",
        "\n",
        "    # Use a list comprehension to remove non-word strings\n",
        "    word_list = [non_word.sub('', string) for string in string_list]\n",
        "\n",
        "    # Use a list comprehension to filter out numeric and underscore strings\n",
        "    filtered_list = [string for string in word_list if numeric_.match(string)]\n",
        "\n",
        "    # Use a list comprehension to remove empty strings\n",
        "    cleaned_list = [string for string in filtered_list if string != '']\n",
        "    \n",
        "    # Create a list of train tokens with the cleaned lists\n",
        "    train_tokens.append(cleaned_list)\n",
        "\n",
        "print(len(train_tokens))"
      ],
      "metadata": {
        "id": "1Hw5hZfhWzyO",
        "outputId": "b2f83abe-78bf-4484-c961-be67591ffd7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4984054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xYTeiVEY654F",
        "outputId": "67d9d1e9-d051-4c33-a03a-9e7446df3f09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1515043\n",
            "['<unk>', '<eos>', 'import', 'in', 'for', 'from', 'def', 'as', 'x', 'return']\n"
          ]
        }
      ],
      "source": [
        "# Numericalize the train tokens\n",
        "vocab = torchtext.vocab.build_vocab_from_iterator(train_tokens) \n",
        "vocab.insert_token('<unk>', 0)           \n",
        "vocab.insert_token('<eos>', 1)            \n",
        "vocab.set_default_index(vocab['<unk>'])   \n",
        "print(len(vocab))                         \n",
        "print(vocab.get_itos()[:10])       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAvvjYDm654G"
      },
      "source": [
        "### 3. Preparing Data Loaders  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "tags": [],
        "id": "labIREFp654G"
      },
      "outputs": [],
      "source": [
        "def get_data(dataset, vocab, batch_size):\n",
        "    data = []                                                   \n",
        "    for example in dataset:\n",
        "        #appends eos so we know it ends....so model learn how to end...                             \n",
        "        tokens = example.append('<eos>')   \n",
        "        #numericalize          \n",
        "        tokens = [vocab[token] for token in example] \n",
        "        data.extend(tokens)                                    \n",
        "    data = torch.LongTensor(data)                                 \n",
        "    num_batches = data.shape[0] // batch_size \n",
        "    data = data[:num_batches * batch_size]                       \n",
        "    data = data.view(batch_size, num_batches)          \n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2gd12ddX654H",
        "outputId": "58c06c8b-1f1f-47f1-82cb-90bbcad4b0f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 307006]) torch.Size([128, 76197])\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "train_data = get_data(train_tokenized, vocab, batch_size)\n",
        "valid_data = get_data(test_tokenized, vocab, batch_size)\n",
        "\n",
        "print(train_data.shape, valid_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrnY40i1654I"
      },
      "source": [
        "## 4. Modeling "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "7FzsvuNi654I"
      },
      "outputs": [],
      "source": [
        "class LSTMLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
        "                \n",
        "        super().__init__()\n",
        "        self.hid_dim   = hid_dim\n",
        "        self.num_layers= num_layers\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.lstm      = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers,\n",
        "                                 dropout=dropout_rate, batch_first=True)\n",
        "        self.dropout   = nn.Dropout(dropout_rate)\n",
        "        #when you do LM, you look forward, so it does not make sense to do bidirectionality\n",
        "        self.fc        = nn.Linear(hid_dim, vocab_size)\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        #this function gonna be run in the beginning of the epoch\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
        "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
        "        \n",
        "        return hidden, cell #return as tuple\n",
        "    \n",
        "    def detach_hidden(self, hidden):\n",
        "        #this gonna run in every batch\n",
        "        hidden, cell = hidden\n",
        "        hidden = hidden.detach() #removing this hidden from gradients graph\n",
        "        cell   = cell.detach()   #removing this cell from gradients graph\n",
        "        return hidden, cell\n",
        "        \n",
        "    def forward(self, src, hidden):\n",
        "        #src: [batch size, seq len]\n",
        "        \n",
        "        #embed\n",
        "        embed = self.embedding(src)\n",
        "        #embed: [batch size, seq len, emb_dim]\n",
        "        \n",
        "        #send this to the lstm\n",
        "        #we want to put hidden here...because we want to reset hidden....\n",
        "        output, hidden = self.lstm(embed, hidden)\n",
        "        #output: [batch size, seq len, hid_dim] ==> all hidden states\n",
        "        #hidden: [num layer, batch size, hid_dim]  ===> last hidden states from each layer\n",
        "        \n",
        "        output = self.dropout(output)\n",
        "        prediction = self.fc(output)\n",
        "        #prediction: [batch size, seq len, vocab size]\n",
        "        \n",
        "        return prediction, hidden\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh2Ux_J0654J"
      },
      "source": [
        "## 5. Training \n",
        "\n",
        "Follows very basic procedure.  One note is that some of the sequences that will be fed to the model may involve parts from different sequences in the original dataset or be a subset of one (depending on the decoding length). For this reason we will reset the hidden state every epoch, this is like assuming that the next batch of sequences is probably always a follow up on the previous in the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9ezL0J9654J"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocab)\n",
        "emb_dim = 1024                # 400 in the paper\n",
        "hid_dim = 1024                # 1150 in the paper\n",
        "num_layers = 2                # 3 in the paper\n",
        "dropout_rate = 0.65              \n",
        "lr = 1e-3                     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2yMNQxf654K",
        "outputId": "72687c8c-bc17-4efc-d1e8-131efeba5af2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 77,183,777 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {num_params:,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ai_4kMd7654K"
      },
      "outputs": [],
      "source": [
        "def get_batch(data, seq_len, idx):\n",
        "    #this data is from get_data()\n",
        "    #train_data.shape #[batch size, number of batches....]\n",
        "    src    = data[:, idx:idx+seq_len]                   \n",
        "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
        "    return src, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5uENIJb654K"
      },
      "outputs": [],
      "source": [
        "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    # drop all batches that are not a multiple of seq_len\n",
        "    num_batches = data.shape[-1]\n",
        "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
        "    num_batches = data.shape[-1]\n",
        "\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "    \n",
        "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
        "        optimizer.zero_grad()\n",
        "        hidden = model.detach_hidden(hidden)\n",
        "\n",
        "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
        "        src, target = src.to(device), target.to(device)\n",
        "        batch_size = src.shape[0]\n",
        "        prediction, hidden = model(src, hidden)               \n",
        "\n",
        "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
        "        target = target.reshape(-1)\n",
        "        loss = criterion(prediction, target)\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  #prevents gradient explosion - clip is basically the threshold.....\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQeIa7YG654L"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    model.eval()\n",
        "    num_batches = data.shape[-1]\n",
        "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
        "    num_batches = data.shape[-1]\n",
        "\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in range(0, num_batches - 1, seq_len):\n",
        "            hidden = model.detach_hidden(hidden)\n",
        "            src, target = get_batch(data, seq_len, idx)\n",
        "            src, target = src.to(device), target.to(device)\n",
        "            batch_size= src.shape[0]\n",
        "\n",
        "            prediction, hidden = model(src, hidden)\n",
        "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
        "            target = target.reshape(-1)\n",
        "\n",
        "            loss = criterion(prediction, target)\n",
        "            epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_V57INO654L",
        "outputId": "bc47e9c9-ee80-498c-9f28-88eb8d6cf170"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                 \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m best_valid_loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_epochs):\n\u001b[0;32m---> 10\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model, train_data, optimizer, criterion, \n\u001b[1;32m     11\u001b[0m                 batch_size, seq_len, clip, device)\n\u001b[1;32m     12\u001b[0m     valid_loss \u001b[39m=\u001b[39m evaluate(model, valid_data, criterion, batch_size, \n\u001b[1;32m     13\u001b[0m                 seq_len, device)\n\u001b[1;32m     15\u001b[0m     lr_scheduler\u001b[39m.\u001b[39mstep(valid_loss)\n",
            "Cell \u001b[0;32mIn[16], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data, optimizer, criterion, batch_size, seq_len, clip, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m loss \u001b[39m=\u001b[39m criterion(prediction, target)\n\u001b[0;32m---> 25\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     26\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), clip)  \u001b[39m#prevents gradient explosion - clip is basically the threshold.....\u001b[39;00m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
            "File \u001b[0;32m~/DSAI/dsai/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
            "File \u001b[0;32m~/DSAI/dsai/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "n_epochs = 50\n",
        "seq_len  = 50\n",
        "clip     = 0.25\n",
        "\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, train_data, optimizer, criterion, \n",
        "                batch_size, seq_len, clip, device)\n",
        "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
        "                seq_len, device)\n",
        "\n",
        "    lr_scheduler.step(valid_loss)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best-val-lstm_lm.pt')\n",
        "\n",
        "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
        "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMUPw3lX654L"
      },
      "source": [
        "## 6. Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hRU8RIV654M"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n",
        "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
        "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34HmlP9u654M"
      },
      "source": [
        "## 7. Real-world inference\n",
        "\n",
        "Here we take the prompt, tokenize, encode and feed it into the model to get the predictions.  We then apply softmax while specifying that we want the output due to the last word in the sequence which represents the prediction for the next word.  We divide the logits by a temperature value to alter the model’s confidence by adjusting the softmax probability distribution.\n",
        "\n",
        "Once we have the Softmax distribution, we randomly sample it to make our prediction on the next word. If we get <unk> then we give that another try.  Once we get <eos> we stop predicting.\n",
        "    \n",
        "We decode the prediction back to strings last lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShK_1BwN654M"
      },
      "outputs": [],
      "source": [
        "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "    model.eval()\n",
        "    tokens = tokenizer(prompt)\n",
        "    indices = [vocab[t] for t in tokens]\n",
        "    batch_size = 1\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(max_seq_len):\n",
        "            src = torch.LongTensor([indices]).to(device)\n",
        "            prediction, hidden = model(src, hidden)\n",
        "            \n",
        "            #prediction: [batch size, seq len, vocab size]\n",
        "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
        "            \n",
        "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
        "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
        "            \n",
        "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
        "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
        "                break\n",
        "\n",
        "            indices.append(prediction) #autoregressive, thus output becomes input\n",
        "\n",
        "    itos = vocab.get_itos()\n",
        "    tokens = [itos[i] for i in indices]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5z3r88N654N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNdoncDA654N"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9 (default, Oct 26 2021, 07:25:54) \n[Clang 13.0.0 (clang-1300.0.29.30)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}