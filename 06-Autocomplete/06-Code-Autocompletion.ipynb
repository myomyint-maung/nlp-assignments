{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myomyint-maung/nlp-assignments/blob/main/06-Autocomplete/06-Code-Autocompletion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HG6ckKe6538"
      },
      "source": [
        "## Feb 16 - Code Autocompletion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [],
        "id": "2PyGSoAF653_"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext, datasets, math\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPTZHMo0654A",
        "outputId": "b90c252f-9c76-4b37-f569-080586f3ea4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Choose the computing device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set SEED for reproducibility\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "YZhOnOJWJKei"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0LU9gwy654C"
      },
      "source": [
        "## 1. Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qPeE5bM654D",
        "outputId": "beee5b9f-980d-4acb-926c-db4fc134297f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/codeparrot___parquet/codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/codeparrot___parquet/codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['repo_name', 'path', 'license', 'content'],\n",
            "    num_rows: 47452\n",
            "})\n",
            "Dataset({\n",
            "    features: ['repo_name', 'path', 'license', 'content'],\n",
            "    num_rows: 11864\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Load CodeParrot's Jupyter-Code-to-Text from HuggingFace  \n",
        "train_set = datasets.load_dataset('codeparrot/github-jupyter-code-to-text', split='train')\n",
        "test_set  = datasets.load_dataset('codeparrot/github-jupyter-code-to-text', split='test')\n",
        "\n",
        "print(train_set)\n",
        "print(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_set['content'][0])"
      ],
      "metadata": {
        "id": "wwljcstzoSB8",
        "outputId": "245b3227-5788-4635-a7cb-004b7926e674",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "# https://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "\n",
            "\"\"\"\n",
            "Explanation: Copyright 2020 The TensorFlow Authors.\n",
            "End of explanation\n",
            "\"\"\"\n",
            "\n",
            "\n",
            "# Import Tokenizer and pad_sequences\n",
            "import tensorflow as tf\n",
            "from tensorflow.keras.preprocessing.text import Tokenizer\n",
            "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
            "\n",
            "# Import numpy and pandas\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "\n",
            "\n",
            "\"\"\"\n",
            "Explanation: Tokenize and sequence a bigger corpus of text\n",
            "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
            "  <td>\n",
            "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l09c03_nlp_prepare_larger_text_corpus.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
            "  </td>\n",
            "  <td>\n",
            "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l09c03_nlp_prepare_larger_text_corpus.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
            "  </td>\n",
            "</table>\n",
            "\n",
            "So far, you have written some test sentences and generated a word index and then created sequences for the sentences. \n",
            "Now you will tokenize and sequence a larger body of text, specifically reviews from Amazon and Yelp. \n",
            "About the dataset\n",
            "You will use a dataset containing Amazon and Yelp reviews of products and restaurants. This dataset was originally extracted from Kaggle.\n",
            "The dataset includes reviews, and each review is labelled as 0 (bad) or 1 (good). However, in this exercise, you will only work with the reviews, not the labels, to practice tokenizing and sequencing the text. \n",
            "Example good reviews:\n",
            "\n",
            "This is hands down the best phone I've ever had.\n",
            "Four stars for the food & the guy in the blue shirt for his great vibe & still letting us in to eat !\n",
            "\n",
            "Example bad reviews:\n",
            "\n",
            "A lady at the table next to us found a live green caterpillar In her salad\n",
            "If you plan to use this in a car forget about it.\n",
            "\n",
            "See more reviews\n",
            "Feel free to download the dataset from a drive folder belonging to Udacity and open it on your local machine to see more reviews.\n",
            "End of explanation\n",
            "\"\"\"\n",
            "\n",
            "\n",
            "path = tf.keras.utils.get_file('reviews.csv', \n",
            "                               'https://drive.google.com/uc?id=13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P')\n",
            "print (path)\n",
            "\n",
            "\"\"\"\n",
            "Explanation: Get the corpus of text\n",
            "The combined dataset of reviews has been saved in a Google drive belonging to Udacity. You can download it from there.\n",
            "End of explanation\n",
            "\"\"\"\n",
            "\n",
            "\n",
            "# Read the csv file\n",
            "dataset = pd.read_csv(path)\n",
            "\n",
            "# Review the first few entries in the dataset\n",
            "dataset.head()\n",
            "\n",
            "\"\"\"\n",
            "Explanation: Get the dataset\n",
            "Each row in the csv file is a separate review.\n",
            "The csv file has 2 columns:\n",
            "\n",
            "text (the review)\n",
            "sentiment (0 or 1 indicating a bad or good review)\n",
            "End of explanation\n",
            "\"\"\"\n",
            "\n",
            "\n",
            "# Get the reviews from the text column\n",
            "reviews = dataset['text'].tolist()\n",
            "\n",
            "\"\"\"\n",
            "Explanation: Get the reviews from the csv file\n",
            "End of explanation\n",
            "\"\"\"\n",
            "\n",
            "\n",
            "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
            "tokenizer.fit_on_texts(reviews)\n",
            "\n",
            "word_index = tokenizer.word_index\n",
            "print(len(word_index))\n",
            "print(word_index)\n",
            "\n",
            "\n",
            "\"\"\"\n",
            "Explanation: Tokenize the text\n",
            "Create the tokenizer, specify the OOV token, tokenize the text, then inspect the word index.\n",
            "End of explanation\n",
            "\"\"\"\n",
            "\n",
            "\n",
            "sequences = tokenizer.texts_to_sequences(reviews)\n",
            "padded_sequences = pad_sequences(sequences, padding='post')\n",
            "\n",
            "# What is the shape of the vector containing the padded sequences?\n",
            "# The shape shows the number of sequences and the length of each one.\n",
            "print(padded_sequences.shape)\n",
            "\n",
            "# What is the first review?\n",
            "print (reviews[0])\n",
            "\n",
            "# Show the sequence for the first review\n",
            "print(padded_sequences[0])\n",
            "\n",
            "# Try printing the review and padded sequence for other elements.\n",
            "\n",
            "\"\"\"\n",
            "Explanation: Generate sequences for the reviews\n",
            "Generate a sequence for each review. Set the max length to match the longest review. Add the padding zeros at the end of the review for reviews that are not as long as the longest one.\n",
            "End of explanation\n",
            "\"\"\"\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmoFBa5S654E"
      },
      "source": [
        "### 2. Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove comments from the codes\n",
        "import re\n",
        "\n",
        "comment_pattern = r\"(^\\s*#.*$)\"\n",
        "block_comment_pattern = r\"(\\\"\\\"\\\")(.*?)(\\\"\\\"\\\")\"\n",
        "\n",
        "train_clean = list()\n",
        "for code in train_set['content']:\n",
        "    code = re.sub(comment_pattern, \"\", code, flags=re.MULTILINE)\n",
        "    code = re.sub(block_comment_pattern, \"\", code, flags=re.DOTALL)\n",
        "    train_clean.append(code)\n",
        "\n",
        "test_clean = list()\n",
        "for code in test_set['content']:\n",
        "    code = re.sub(comment_pattern, \"\", code, flags=re.MULTILINE)\n",
        "    code = re.sub(block_comment_pattern, \"\", code, flags=re.DOTALL)\n",
        "    test_clean.append(code)"
      ],
      "metadata": {
        "id": "YlxNSx9FKj30"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_clean[0])"
      ],
      "metadata": {
        "id": "X1R7Jol_orb2",
        "outputId": "b124851d-d141-4d2c-dfed-ae275b66ff64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import numpy as np\n",
            "from tensorflow import keras\n",
            "from tensorflow.keras import layers\n",
            "\n",
            "\n",
            "\n",
            "num_classes = 10\n",
            "input_shape = (28, 28, 1)\n",
            "\n",
            "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
            "\n",
            "x_train = x_train.astype(\"float32\") / 255\n",
            "x_test = x_test.astype(\"float32\") / 255\n",
            "\n",
            "x_train = np.expand_dims(x_train, -1)\n",
            "x_test = np.expand_dims(x_test, -1)\n",
            "print(\"x_train shape:\", x_train.shape)\n",
            "print(x_train.shape[0], \"train samples\")\n",
            "print(x_test.shape[0], \"test samples\")\n",
            "\n",
            "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
            "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "model = keras.Sequential(\n",
            "    [\n",
            "        keras.Input(shape=input_shape),\n",
            "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
            "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
            "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
            "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
            "        layers.Flatten(),\n",
            "        layers.Dropout(0.5),\n",
            "        layers.Dense(num_classes, activation=\"softmax\"),\n",
            "    ]\n",
            ")\n",
            "\n",
            "model.summary()\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "batch_size = 128\n",
            "epochs = 15\n",
            "\n",
            "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
            "\n",
            "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "score = model.evaluate(x_test, y_test, verbose=0)\n",
            "print(\"Test loss:\", score[0])\n",
            "print(\"Test accuracy:\", score[1])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_clean[0])"
      ],
      "metadata": {
        "id": "wTSupshgoyi9",
        "outputId": "b2d5e2e8-533c-4f10-abfb-883121c1cfa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "import tensorflow as tf\n",
            "from tensorflow.keras.preprocessing.text import Tokenizer\n",
            "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
            "\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "path = tf.keras.utils.get_file('reviews.csv', \n",
            "                               'https://drive.google.com/uc?id=13ySLC_ue6Umt9RJYSeM2t-V0kCv-4C-P')\n",
            "print (path)\n",
            "\n",
            "\n",
            "\n",
            "dataset = pd.read_csv(path)\n",
            "\n",
            "dataset.head()\n",
            "\n",
            "\n",
            "\n",
            "reviews = dataset['text'].tolist()\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
            "tokenizer.fit_on_texts(reviews)\n",
            "\n",
            "word_index = tokenizer.word_index\n",
            "print(len(word_index))\n",
            "print(word_index)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sequences = tokenizer.texts_to_sequences(reviews)\n",
            "padded_sequences = pad_sequences(sequences, padding='post')\n",
            "\n",
            "\n",
            "print(padded_sequences.shape)\n",
            "\n",
            "print (reviews[0])\n",
            "\n",
            "print(padded_sequences[0])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide the codes into sentences\n",
        "train_sents = [sent for code in train_clean for sent in code.split('\\n') if sent != '']\n",
        "test_sents  = [sent for code in test_clean for sent in code.split('\\n') if sent != '']\n",
        "\n",
        "print(train_sents[:5])\n",
        "print(test_sents[:5])\n",
        "print(len(train_sents), len(test_sents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7Dtvv0_QbQz",
        "outputId": "3a98a024-0c83-4bde-8b0d-8d89a9e2e311"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['import numpy as np', 'from tensorflow import keras', 'from tensorflow.keras import layers', 'num_classes = 10', 'input_shape = (28, 28, 1)']\n",
            "['import tensorflow as tf', 'from tensorflow.keras.preprocessing.text import Tokenizer', 'from tensorflow.keras.preprocessing.sequence import pad_sequences', 'import numpy as np', 'import pandas as pd']\n",
            "4984055 1238709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only the sentences starting with \"import\" or \"from\"\n",
        "# because the full datasets are too big to train or test\n",
        "\n",
        "small_train = [sent for sent in train_sents if re.match(r'^(import|from)', sent)]\n",
        "small_test  = [sent for sent in test_sents if re.match(r'^(import|from)', sent)]\n",
        "\n",
        "print(len(small_train))\n",
        "print(len(small_test))"
      ],
      "metadata": {
        "id": "1Gj5o9dzuHOb",
        "outputId": "97ca0878-0001-4a43-890d-98c761c7fdea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "295359\n",
            "73939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "AuG9FBx1654E",
        "outputId": "edffa9ad-14b7-409b-91e0-8578000f5fdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['from', 'scipy.misc', 'import', 'imread', ',', 'imresize'], ['import', 'matplotlib.pyplot', 'as', 'plt'], ['from', 'gpytorch.models.deep_gps', 'import', 'DeepGPLayer', ',', 'DeepGP'], ['from', 'scipy.stats', 'import', 'dirichlet'], ['from', 'skopt.plots', 'import', 'plot_convergence']]\n",
            "[['import', 'matplotlib.pyplot', 'as', 'plt'], ['import', 'pandas', 'as', 'pd'], ['from', 'time', 'import', 'time'], ['import', 'sklearn.linear_model'], ['from', 'sklearn.model_selection', 'import', 'train_test_split']]\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the selected sentences\n",
        "tokenizer = torchtext.data.utils.get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "train_tokenized = [tokenizer(sent) for sent in small_train]\n",
        "test_tokenized  = [tokenizer(sent) for sent in small_test]\n",
        "\n",
        "print(train_tokenized[:5])\n",
        "print(test_tokenized[:5])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove \",\" from the tokens\n",
        "train_tokenized = [[token for token in sent if token != \",\"] for sent in train_tokenized]\n",
        "test_tokenized  = [[token for token in sent if token != \",\"] for sent in test_tokenized]\n",
        "\n",
        "print(train_tokenized[:5])\n",
        "print(test_tokenized[:5])"
      ],
      "metadata": {
        "id": "7r7LL7z2Hedn",
        "outputId": "0db6ca63-0b88-491b-f370-6c808f82f023",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['from', 'scipy.misc', 'import', 'imread', 'imresize'], ['import', 'matplotlib.pyplot', 'as', 'plt'], ['from', 'gpytorch.models.deep_gps', 'import', 'DeepGPLayer', 'DeepGP'], ['from', 'scipy.stats', 'import', 'dirichlet'], ['from', 'skopt.plots', 'import', 'plot_convergence']]\n",
            "[['import', 'matplotlib.pyplot', 'as', 'plt'], ['import', 'pandas', 'as', 'pd'], ['from', 'time', 'import', 'time'], ['import', 'sklearn.linear_model'], ['from', 'sklearn.model_selection', 'import', 'train_test_split']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "xYTeiVEY654F",
        "outputId": "8abafc30-a6c0-4a24-8185-708c2a13de93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38726\n",
            "['<unk>', '<eos>', 'import', 'from', 'as', 'numpy', 'np', 'plt', 'matplotlib.pyplot', 'pandas']\n"
          ]
        }
      ],
      "source": [
        "# Numericalize the train tokens\n",
        "vocab = torchtext.vocab.build_vocab_from_iterator(train_tokenized) \n",
        "vocab.insert_token('<unk>', 0)           \n",
        "vocab.insert_token('<eos>', 1)            \n",
        "vocab.set_default_index(vocab['<unk>'])   \n",
        "print(len(vocab))                         \n",
        "print(vocab.get_itos()[:10])       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAvvjYDm654G"
      },
      "source": [
        "### 3. Preparing Data Loaders  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "tags": [],
        "id": "labIREFp654G"
      },
      "outputs": [],
      "source": [
        "def get_data(dataset, vocab, batch_size):\n",
        "    data = []                                                   \n",
        "    for example in dataset:\n",
        "        #appends eos so we know it ends....so model learn how to end...                             \n",
        "        tokens = example.append('<eos>')   \n",
        "        #numericalize          \n",
        "        tokens = [vocab[token] for token in example] \n",
        "        data.extend(tokens)                                    \n",
        "    data = torch.LongTensor(data)                                 \n",
        "    num_batches = data.shape[0] // batch_size \n",
        "    data = data[:num_batches * batch_size]                       \n",
        "    data = data.view(batch_size, num_batches)          \n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "2gd12ddX654H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deaa2807-a2e9-48fe-e441-11f5bf33adc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 11226]) torch.Size([128, 2819])\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "train_data = get_data(train_tokenized, vocab, batch_size)\n",
        "valid_data = get_data(test_tokenized, vocab, batch_size)\n",
        "\n",
        "print(train_data.shape, valid_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrnY40i1654I"
      },
      "source": [
        "### 4. Modeling "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "tags": [],
        "id": "7FzsvuNi654I"
      },
      "outputs": [],
      "source": [
        "class LSTMLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
        "                \n",
        "        super().__init__()\n",
        "        self.hid_dim   = hid_dim\n",
        "        self.num_layers= num_layers\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.lstm      = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers,\n",
        "                                 dropout=dropout_rate, batch_first=True)\n",
        "        self.dropout   = nn.Dropout(dropout_rate)\n",
        "        #when you do LM, you look forward, so it does not make sense to do bidirectionality\n",
        "        self.fc        = nn.Linear(hid_dim, vocab_size)\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        #this function gonna be run in the beginning of the epoch\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
        "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
        "        \n",
        "        return hidden, cell #return as tuple\n",
        "    \n",
        "    def detach_hidden(self, hidden):\n",
        "        #this gonna run in every batch\n",
        "        hidden, cell = hidden\n",
        "        hidden = hidden.detach() #removing this hidden from gradients graph\n",
        "        cell   = cell.detach()   #removing this cell from gradients graph\n",
        "        return hidden, cell\n",
        "        \n",
        "    def forward(self, src, hidden):\n",
        "        #src: [batch size, seq len]\n",
        "        \n",
        "        #embed\n",
        "        embed = self.embedding(src)\n",
        "        #embed: [batch size, seq len, emb_dim]\n",
        "        \n",
        "        #send this to the lstm\n",
        "        #we want to put hidden here...because we want to reset hidden....\n",
        "        output, hidden = self.lstm(embed, hidden)\n",
        "        #output: [batch size, seq len, hid_dim] ==> all hidden states\n",
        "        #hidden: [num layer, batch size, hid_dim]  ===> last hidden states from each layer\n",
        "        \n",
        "        output = self.dropout(output)\n",
        "        prediction = self.fc(output)\n",
        "        #prediction: [batch size, seq len, vocab size]\n",
        "        \n",
        "        return prediction, hidden\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh2Ux_J0654J"
      },
      "source": [
        "### 5. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "O9ezL0J9654J"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocab)\n",
        "emb_dim = 1024\n",
        "hid_dim = 1024\n",
        "num_layers = 2\n",
        "dropout_rate = 0.65              \n",
        "lr = 1e-3                     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "S2yMNQxf654K",
        "outputId": "a6aa8a39-f98c-4c37-904e-52e6773cebc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 96,143,174 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {num_params:,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Ai_4kMd7654K"
      },
      "outputs": [],
      "source": [
        "def get_batch(data, seq_len, idx):\n",
        "    #this data is from get_data()\n",
        "    #train_data.shape #[batch size, number of batches....]\n",
        "    src    = data[:, idx:idx+seq_len]                   \n",
        "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
        "    return src, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "L5uENIJb654K"
      },
      "outputs": [],
      "source": [
        "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    # drop all batches that are not a multiple of seq_len\n",
        "    num_batches = data.shape[-1]\n",
        "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
        "    num_batches = data.shape[-1]\n",
        "\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "    \n",
        "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
        "        optimizer.zero_grad()\n",
        "        hidden = model.detach_hidden(hidden)\n",
        "\n",
        "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
        "        src, target = src.to(device), target.to(device)\n",
        "        batch_size = src.shape[0]\n",
        "        prediction, hidden = model(src, hidden)               \n",
        "\n",
        "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
        "        target = target.reshape(-1)\n",
        "        loss = criterion(prediction, target)\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  #prevents gradient explosion - clip is basically the threshold.....\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "GQeIa7YG654L"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    model.eval()\n",
        "    num_batches = data.shape[-1]\n",
        "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
        "    num_batches = data.shape[-1]\n",
        "\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in range(0, num_batches - 1, seq_len):\n",
        "            hidden = model.detach_hidden(hidden)\n",
        "            src, target = get_batch(data, seq_len, idx)\n",
        "            src, target = src.to(device), target.to(device)\n",
        "            batch_size= src.shape[0]\n",
        "\n",
        "            prediction, hidden = model(src, hidden)\n",
        "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
        "            target = target.reshape(-1)\n",
        "\n",
        "            loss = criterion(prediction, target)\n",
        "            epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the folder to save models\n",
        "import os\n",
        "from os import path\n",
        "if path.exists('./models') == False:\n",
        "  os.mkdir('./models')"
      ],
      "metadata": {
        "id": "tehRa9L6bz0D"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_V57INO654L",
        "outputId": "541efbe7-2fa7-4898-9036-cc569224c943",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "\tTrain Perplexity: 19.139\n",
            "\tValid Perplexity: 10.892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2\n",
            "\tTrain Perplexity: 9.673\n",
            "\tValid Perplexity: 8.919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3\n",
            "\tTrain Perplexity: 8.164\n",
            "\tValid Perplexity: 8.086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4\n",
            "\tTrain Perplexity: 7.336\n",
            "\tValid Perplexity: 7.596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5\n",
            "\tTrain Perplexity: 6.796\n",
            "\tValid Perplexity: 7.317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6\n",
            "\tTrain Perplexity: 6.412\n",
            "\tValid Perplexity: 7.126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  81%|████████▏ | 182/224 [02:21<00:32,  1.28it/s]"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "n_epochs = 50\n",
        "seq_len  = 50\n",
        "clip     = 0.25\n",
        "\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, train_data, optimizer, criterion, \n",
        "                batch_size, seq_len, clip, device)\n",
        "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
        "                seq_len, device)\n",
        "\n",
        "    lr_scheduler.step(valid_loss)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'models/lstm_lm.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1}')\n",
        "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
        "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMUPw3lX654L"
      },
      "source": [
        "### 6. Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34HmlP9u654M"
      },
      "source": [
        "### 7. Real-world inference"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9 (default, Oct 26 2021, 07:25:54) \n[Clang 13.0.0 (clang-1300.0.29.30)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}