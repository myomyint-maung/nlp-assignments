{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19 Jan - GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the training corpus\n",
    "#I use the Inaugural corpus from NLTK\n",
    "import nltk\n",
    "from nltk.corpus import inaugural\n",
    "corpus = nltk.corpus.inaugural.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the', 'House', 'of', 'Representatives', ':'], ['Among', 'the', 'vicissitudes', 'incident', 'to', 'life', 'no', 'event', 'could', 'have', 'filled', 'me', 'with', 'greater', 'anxieties', 'than', 'that', 'of', 'which', 'the', 'notification', 'was', 'transmitted', 'by', 'your', 'order', ',', 'and', 'received', 'on', 'the', '14th', 'day', 'of', 'the', 'present', 'month', '.'], ['On', 'the', 'one', 'hand', ',', 'I', 'was', 'summoned', 'by', 'my', 'Country', ',', 'whose', 'voice', 'I', 'can', 'never', 'hear', 'but', 'with', 'veneration', 'and', 'love', ',', 'from', 'a', 'retreat', 'which', 'I', 'had', 'chosen', 'with', 'the', 'fondest', 'predilection', ',', 'and', ',', 'in', 'my', 'flattering', 'hopes', ',', 'with', 'an', 'immutable', 'decision', ',', 'as', 'the', 'asylum', 'of', 'my', 'declining', 'years', '--', 'a', 'retreat', 'which', 'was', 'rendered', 'every', 'day', 'more', 'necessary', 'as', 'well', 'as', 'more', 'dear', 'to', 'me', 'by', 'the', 'addition', 'of', 'habit', 'to', 'inclination', ',', 'and', 'of', 'frequent', 'interruptions', 'in', 'my', 'health', 'to', 'the', 'gradual', 'waste', 'committed', 'on', 'it', 'by', 'time', '.']]\n"
     ]
    }
   ],
   "source": [
    "#the corpus is already tokenized\n",
    "print(corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fellow', '-', 'citizens', 'of', 'the', 'senate', 'and', 'of', 'the', 'house', 'of', 'representatives', ':'], ['among', 'the', 'vicissitudes', 'incident', 'to', 'life', 'no', 'event', 'could', 'have', 'filled', 'me', 'with', 'greater', 'anxieties', 'than', 'that', 'of', 'which', 'the', 'notification', 'was', 'transmitted', 'by', 'your', 'order', ',', 'and', 'received', 'on', 'the', '14th', 'day', 'of', 'the', 'present', 'month', '.'], ['on', 'the', 'one', 'hand', ',', 'i', 'was', 'summoned', 'by', 'my', 'country', ',', 'whose', 'voice', 'i', 'can', 'never', 'hear', 'but', 'with', 'veneration', 'and', 'love', ',', 'from', 'a', 'retreat', 'which', 'i', 'had', 'chosen', 'with', 'the', 'fondest', 'predilection', ',', 'and', ',', 'in', 'my', 'flattering', 'hopes', ',', 'with', 'an', 'immutable', 'decision', ',', 'as', 'the', 'asylum', 'of', 'my', 'declining', 'years', '--', 'a', 'retreat', 'which', 'was', 'rendered', 'every', 'day', 'more', 'necessary', 'as', 'well', 'as', 'more', 'dear', 'to', 'me', 'by', 'the', 'addition', 'of', 'habit', 'to', 'inclination', ',', 'and', 'of', 'frequent', 'interruptions', 'in', 'my', 'health', 'to', 'the', 'gradual', 'waste', 'committed', 'on', 'it', 'by', 'time', '.']]\n"
     ]
    }
   ],
   "source": [
    "#convert the words in the corpus into lower case\n",
    "corpus_tokenized = [[]] * len(corpus)\n",
    "for i in range(len(corpus)):\n",
    "    corpus_tokenized[i] = [word.lower() for word in corpus[i]]\n",
    "\n",
    "print(corpus_tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fellow', '-', 'citizens', 'senate', 'house', 'representatives', ':'], ['vicissitudes', 'incident', 'life', 'event', 'filled', 'greater', 'anxieties', 'notification', 'transmitted', 'order', ',', 'received', '14th', 'day', 'present', 'month', '.'], ['hand', ',', 'summoned', 'country', ',', 'voice', 'hear', 'veneration', 'love', ',', 'retreat', 'chosen', 'fondest', 'predilection', ',', ',', 'flattering', 'hopes', ',', 'immutable', 'decision', ',', 'asylum', 'declining', 'years', '--', 'retreat', 'rendered', 'day', 'necessary', 'dear', 'addition', 'habit', 'inclination', ',', 'frequent', 'interruptions', 'health', 'gradual', 'waste', 'committed', 'time', '.']]\n"
     ]
    }
   ],
   "source": [
    "#remove stop words with SpaCy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "for sentence in corpus_tokenized:\n",
    "    for word in sentence[:]:\n",
    "        if word in stopwords:\n",
    "            sentence.remove(word)\n",
    "\n",
    "print(corpus_tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fellow', 'citizens', 'senate', 'house', 'representatives'], ['vicissitudes', 'incident', 'life', 'event', 'filled', 'greater', 'anxieties', 'notification', 'transmitted', 'order', 'received', '14th', 'day', 'present', 'month'], ['hand', 'summoned', 'country', 'voice', 'hear', 'veneration', 'love', 'retreat', 'chosen', 'fondest', 'predilection', 'flattering', 'hopes', 'immutable', 'decision', 'asylum', 'declining', 'years', '--', 'retreat', 'rendered', 'day', 'necessary', 'dear', 'addition', 'habit', 'inclination', 'frequent', 'interruptions', 'health', 'gradual', 'waste', 'committed', 'time']]\n"
     ]
    }
   ],
   "source": [
    "#remove punctutations with String\n",
    "import string\n",
    "punctutations = string.punctuation\n",
    "\n",
    "for sentence in corpus_tokenized:\n",
    "    for word in sentence[:]:\n",
    "        if word in punctutations:\n",
    "            sentence.remove(word)\n",
    "\n",
    "print(corpus_tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fellow', 'citizens', 'senate', 'house', 'representatives'], ['vicissitudes', 'incident', 'life', 'event', 'filled', 'greater', 'anxieties', 'notification', 'transmitted', 'order', 'received', '14th', 'day', 'present', 'month'], ['hand', 'summoned', 'country', 'voice', 'hear', 'veneration', 'love', 'retreat', 'chosen', 'fondest', 'predilection', 'flattering', 'hopes', 'immutable', 'decision', 'asylum', 'declining', 'years', 'retreat', 'rendered', 'day', 'necessary', 'dear', 'addition', 'habit', 'inclination', 'frequent', 'interruptions', 'health', 'gradual', 'waste', 'committed', 'time']]\n"
     ]
    }
   ],
   "source": [
    "#remove '--'\n",
    "for sentence in corpus_tokenized:\n",
    "    for word in sentence[:]:\n",
    "        if word == '--':\n",
    "            sentence.remove(word)\n",
    "\n",
    "print(corpus_tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get unique words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabs = list(set(flatten(corpus_tokenized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numericalize the vocabs\n",
    "word2index = {w: i for i, w in enumerate(vocabs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append <UNK>\n",
    "vocabs.append('<UNK>')\n",
    "word2index['<UNK>'] = len(word2index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GloVe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Co-occurence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count the frequency of each word\n",
    "from collections import Counter\n",
    "\n",
    "X_i = Counter(flatten(corpus_tokenized))\n",
    "\n",
    "X_i['fellow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate skipgrams with a generic window size\n",
    "def generate_skip_gram(window_size): \n",
    "    skip_grams = []\n",
    "    for sentence in corpus_tokenized:\n",
    "        #I include the first and last words\n",
    "        #so that every word could be a center word\n",
    "        for i in range(len(sentence)):\n",
    "            center = sentence[i]\n",
    "            context = []\n",
    "            for j in range(window_size):\n",
    "                if (i - window_size + j) >= 0:\n",
    "                    context.append(sentence[i - window_size + j])\n",
    "            for k in range(1, window_size + 1):\n",
    "                if (i + k) < len(sentence):\n",
    "                    context.append(sentence[i + k])\n",
    "            for w in context:\n",
    "                skip_grams.append((center, w))\n",
    "        \n",
    "    return skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fellow', 'citizens'),\n",
       " ('fellow', 'senate'),\n",
       " ('citizens', 'fellow'),\n",
       " ('citizens', 'senate'),\n",
       " ('citizens', 'house')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare skipgrams with window size of 2\n",
    "skip_grams = generate_skip_gram(2)\n",
    "\n",
    "skip_grams[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count co-occurences in the skipgrams\n",
    "X_ik_skipgram = Counter(skip_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(X_ik_skipgram[('fellow', 'citizens')])\n",
    "print(X_ik_skipgram[('fellow', 'communists')])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Weighting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the weighting function\n",
    "def weighting(w_i, w_j, X_ik):\n",
    "    \n",
    "    x_ij = X_ik[(w_i, w_j)]\n",
    "\n",
    "    #label smoothing if there is no co-occurence (i.e., x_ij is 0)\n",
    "    if x_ij == 0:\n",
    "        x_ij = 1\n",
    "        \n",
    "    #maximum co-occurrences is 100 according to the paper\n",
    "    x_max = 100\n",
    "    alpha = 0.75\n",
    "    \n",
    "    #if the co-occurrences does not exceed x_max, scale it down based on some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij/x_max) ** alpha\n",
    "    else:\n",
    "        result = 1 #the maximum probability\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#test the weighting function\n",
    "w_i  = 'fellow'\n",
    "w_j  = 'citizens'\n",
    "\n",
    "print(weighting(w_i, w_j, X_ik_skipgram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03162277660168379\n",
      "0.03162277660168379\n"
     ]
    }
   ],
   "source": [
    "w_i  = 'fellow'\n",
    "w_j  = 'communists'\n",
    "\n",
    "print(weighting(w_i, w_j, X_ik_skipgram))\n",
    "print((1 / 100) ** 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply this weighting to all possible pairs\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {} #for keeping the co-occurrences\n",
    "weighting_dic = {} #for keeping all the probabilities after passing through the weighting function\n",
    "\n",
    "for bigram in combinations_with_replacement(vocabs, 2):  #we need to also think its reverse\n",
    "    #if this bigram exists in X_ik_skipgrams\n",
    "    #we gonna add this to our co-occurence matrix\n",
    "    if X_ik_skipgram.get(bigram) is not None:\n",
    "        cooc = X_ik_skipgram[bigram]  #get the co-occurrence\n",
    "        X_ik[bigram] = cooc + 1 #this is again basically label smoothing\n",
    "        X_ik[(bigram[1], bigram[0])] = cooc + 1  #trick to get all pairs\n",
    "    else: #otherwise, put 0\n",
    "        X_ik[bigram] = 0\n",
    "        X_ik[(bigram[1], bigram[0])] = 0\n",
    "\n",
    "    #apply the weighting function using this co-occurrence matrix thingy    \n",
    "    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "0.10573712634405642\n",
      "0.10573712634405642\n",
      "0.10573712634405642\n"
     ]
    }
   ],
   "source": [
    "#test the weighting function\n",
    "print(X_ik_skipgram[('senate', 'house')])\n",
    "print(X_ik_skipgram[('house', 'senate')])\n",
    "\n",
    "print(X_ik[('senate', 'house')])\n",
    "print(X_ik[('house', 'senate')])\n",
    "\n",
    "print(weighting_dic[('senate', 'house')])\n",
    "print(weighting_dic[('house', 'senate')])\n",
    "\n",
    "print((5 / 100) ** 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0.03162277660168379\n",
      "0.03162277660168379\n"
     ]
    }
   ],
   "source": [
    "print(X_ik_skipgram[('communists', 'communists')])\n",
    "print(X_ik[('communists', 'communists')])\n",
    "print(weighting_dic[('communists', 'communists')])\n",
    "print((1 / 100) ** 0.75)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random batch for GloVe with generic batch size, corpus and skipgrams\n",
    "import math\n",
    "\n",
    "def random_batch_glove(batch_size, corpus, skip_grams, X_ik, weighting_dic):\n",
    "    \n",
    "    #change words in the skipgrams to idices\n",
    "    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    \n",
    "    #randomly pick \"batch_size\" indices\n",
    "    number_of_choices = len(skip_grams_id)\n",
    "    random_index = np.random.choice(number_of_choices, batch_size, replace=False) #no repeating indexes among these random indexes\n",
    "    \n",
    "    random_inputs = [] #xi, wi (in batches)\n",
    "    random_labels = [] #xj, wj (in batches)\n",
    "    random_coocs  = [] #Xij (in batches)\n",
    "    random_weightings = [] #weighting_dic(Xij) (in batches)\n",
    "    #for each of the sample in these indexes\n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams_id[i][0]])\n",
    "        random_labels.append([skip_grams_id[i][1]])\n",
    "        \n",
    "        #get cooc\n",
    "        #first check whether it exists...\n",
    "        pair = skip_grams[i]\n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1 #label smoothing\n",
    "            \n",
    "        random_coocs.append([math.log(cooc)])\n",
    "        #log according to the cost function equation\n",
    "        #bracket because neural network requires size ( , 1)\n",
    "        \n",
    "        #get weighting\n",
    "        weighting = weighting_dic[pair]\n",
    "        random_weightings.append(weighting)\n",
    "\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[4651],\n",
       "        [2868]]),\n",
       " array([[7867],\n",
       "        [3419]]),\n",
       " array([[0.69314718],\n",
       "        [3.76120012]]),\n",
       " array([0.05318296, 0.53100834]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the method\n",
    "batch_size = 2\n",
    "inputs, targets, coocs, weightings = random_batch_glove(batch_size, corpus_tokenized, skip_grams, X_ik, weighting_dic)\n",
    "\n",
    "inputs, targets, coocs, weightings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size,embed_size):\n",
    "        super(GloVe,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, embed_size) # center embedding\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size) # context embedding\n",
    "        \n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "        \n",
    "    def forward(self, center_words, context_words, coocs, weightings):\n",
    "        center_embeds = self.embedding_v(center_words) # [batch_size, 1, emb_size]\n",
    "        context_embeds = self.embedding_u(context_words) # [batch_size, 1, emb_size]\n",
    "        \n",
    "        center_bias = self.v_bias(center_words).squeeze(1)\n",
    "        context_bias = self.u_bias(context_words).squeeze(1)\n",
    "        \n",
    "        inner_product = context_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "        \n",
    "        #note that coocs is already log\n",
    "        loss = weightings * torch.pow(inner_product + center_bias + context_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set training parameters\n",
    "batch_size  = 10\n",
    "vocab_size  = len(vocabs)\n",
    "emb_size    = 50\n",
    "model       = GloVe(vocab_size, emb_size)\n",
    "optimizer   = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate epoch time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time // 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | cost: 188.933212 | time: 0m 9s\n",
      "Epoch: 200 | cost: 295.503082 | time: 0m 19s\n",
      "Epoch: 300 | cost: 203.059021 | time: 0m 29s\n",
      "Epoch: 400 | cost: 431.108551 | time: 0m 38s\n",
      "Epoch: 500 | cost: 476.008148 | time: 0m 47s\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "import time\n",
    "\n",
    "num_epochs = 500\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    inputs, targets, coocs, weightings = random_batch_glove(batch_size, corpus_tokenized, skip_grams, X_ik, weighting_dic)\n",
    "    input_batch     = torch.LongTensor(inputs)\n",
    "    target_batch    = torch.LongTensor(targets)\n",
    "    cooc_batch      = torch.FloatTensor(coocs)\n",
    "    weighting_batch = torch.FloatTensor(weightings)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(input_batch, target_batch, cooc_batch, weighting_batch)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        end = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6. Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the GloVe model with pickle\n",
    "import pickle\n",
    "\n",
    "pickle.dump(model, open('GloVe.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random batch for CBOW model with default window size and batch size of 1 each\n",
    "def random_batch_cbow(window_size=1, batch_size=1): \n",
    "    cbow = []\n",
    "    for sentence in corpus_tokenized:\n",
    "        #I include the first and last words\n",
    "        #so that every word could be a center word\n",
    "        for i in range(len(sentence)):\n",
    "            center = word2index[sentence[i]]\n",
    "            context = []\n",
    "            context_idx = []\n",
    "            #group the indices of the context words\n",
    "            for j in range(window_size):\n",
    "                context_idx.append(i - window_size + j)\n",
    "            for k in range(1, window_size + 1):\n",
    "                context_idx.append(i + k)\n",
    "            #append the context words based on their indices\n",
    "            #append <UNK> if there is no word at an index\n",
    "            for idx in context_idx:\n",
    "                if idx < 0:\n",
    "                    context.append(word2index['<UNK>'])\n",
    "                elif idx >= len(sentence):\n",
    "                    context.append(word2index['<UNK>'])\n",
    "                else:\n",
    "                    context.append(word2index[sentence[idx]])\n",
    "            cbow.append([context, center])\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(cbow)), batch_size, replace=False)\n",
    "    for i in random_index:\n",
    "        random_inputs.append(cbow[i][0])\n",
    "        random_labels.append([cbow[i][1]])\n",
    "            \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [[4210 3418 8289 6532]\n",
      " [8140 1099 5047 2694]\n",
      " [3691 5801 2886 6777]\n",
      " [9019 2365 4130 6921]\n",
      " [9019 6789 6584 2886]\n",
      " [4372  865 6559 8434]\n",
      " [3845 8316 8807 1210]\n",
      " [7217 1296 5234 5276]\n",
      " [9019 9019  425 4281]\n",
      " [6251 2233 9019 9019]]\n",
      "Target:  [[4749]\n",
      " [2548]\n",
      " [4988]\n",
      " [3097]\n",
      " [7087]\n",
      " [4742]\n",
      " [7149]\n",
      " [8181]\n",
      " [3418]\n",
      " [8904]]\n"
     ]
    }
   ],
   "source": [
    "#test the CBOW method\n",
    "input_batch, target_batch = random_batch_cbow(2, 10)\n",
    "\n",
    "print(\"Input: \", input_batch)\n",
    "print(\"Target: \", target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CBOW model\n",
    "class CBOW(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(CBOW,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, emb_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, emb_size)\n",
    "    \n",
    "    def forward(self, context_words, center_words, all_vocabs):\n",
    "        center_embeds = self.embedding_v(center_words)  #[batch_size, 1, emb_size]\n",
    "        context_embeds = self.embedding_u(context_words)  #[batch_size, window_size, emb_size]\n",
    "        all_embeds    = self.embedding_v(all_vocabs)    #[batch_size, vocab_size, emb_size]\n",
    "        \n",
    "        scores      = center_embeds.bmm(context_embeds.transpose(1, 2)).squeeze(2)\n",
    "\n",
    "        norm_scores = all_embeds.bmm(context_embeds.transpose(1, 2)).squeeze(2)\n",
    "\n",
    "        nll = -torch.mean(torch.log(torch.exp(scores)/torch.sum(torch.exp(norm_scores), 1).unsqueeze(1)))\n",
    "            \n",
    "        return nll # negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | cost: 24.164333 | time: 0m 36s\n",
      "Epoch: 200 | cost: 25.088284 | time: 1m 13s\n",
      "Epoch: 300 | cost: 22.174625 | time: 1m 51s\n",
      "Epoch: 400 | cost: 22.098713 | time: 2m 29s\n",
      "Epoch: 500 | cost: 21.277281 | time: 3m 6s\n"
     ]
    }
   ],
   "source": [
    "#set training parameters\n",
    "window_size = 2\n",
    "batch_size = 10\n",
    "vocab_size = len(vocabs)\n",
    "emb_size = 50\n",
    "model = CBOW(vocab_size, emb_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#convert all vocabs to tensors\n",
    "def prepare_sequence(vocabs, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], vocabs))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, vocab_size)\n",
    "\n",
    "#calculate epoch time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time // 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "#train the model\n",
    "import time\n",
    "\n",
    "num_epochs = 500\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    input_batch, target_batch = random_batch_cbow(window_size, batch_size)\n",
    "    input_batch  = torch.LongTensor(input_batch)  #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch) #[batch_size, 1]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(input_batch, target_batch, all_vocabs)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        end = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the CBOW model\n",
    "pickle.dump(model, open('CBOW.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random batch for skip-gram model with default window size and batch size of 1 each\n",
    "def random_batch_skip_gram(window_size=1, batch_size=1): \n",
    "    skip_grams = []\n",
    "    for sentence in corpus_tokenized:\n",
    "        #I include the first and last words\n",
    "        #so that every word could be a center word\n",
    "        for i in range(len(sentence)):\n",
    "            center = word2index[sentence[i]]\n",
    "            context = []\n",
    "            for j in range(window_size):\n",
    "                if (i - window_size + j) >= 0:\n",
    "                    context.append(word2index[sentence[i - window_size + j]])\n",
    "            for k in range(1, window_size + 1):\n",
    "                if (i + k) < len(sentence):\n",
    "                    context.append(word2index[sentence[i + k]])\n",
    "            for w in context:\n",
    "                skip_grams.append([center, w])\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False)\n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams[i][0]])\n",
    "        random_labels.append([skip_grams[i][1]])\n",
    "            \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [[   5]\n",
      " [ 301]\n",
      " [8579]\n",
      " [6921]\n",
      " [2760]\n",
      " [1147]\n",
      " [6559]\n",
      " [6218]\n",
      " [3865]\n",
      " [2070]]\n",
      "Target:  [[2430]\n",
      " [4110]\n",
      " [6971]\n",
      " [2886]\n",
      " [ 890]\n",
      " [ 497]\n",
      " [6330]\n",
      " [1788]\n",
      " [4703]\n",
      " [7538]]\n"
     ]
    }
   ],
   "source": [
    "#test the skip-gram method\n",
    "input_batch, target_batch = random_batch_skip_gram(2, 10)\n",
    "\n",
    "print(\"Input: \", input_batch)\n",
    "print(\"Target: \", target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip-gram model\n",
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(Skipgram,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, emb_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, emb_size)\n",
    "    \n",
    "    def forward(self, center_words, context_words, all_vocabs):\n",
    "        center_embeds = self.embedding_v(center_words)  #[batch_size, 1, emb_size]\n",
    "        context_embeds = self.embedding_u(context_words)  #[batch_size, 1, emb_size]\n",
    "        all_embeds    = self.embedding_u(all_vocabs)    #[batch_size, vocab_size, emb_size]\n",
    "        \n",
    "        scores      = context_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "\n",
    "        norm_scores = all_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, vocab_size, emb_size] @ [batch_size, emb_size, 1] = [batch_size, vocab_size, 1] = [batch_size, vocab_size]\n",
    "\n",
    "        nll = -torch.mean(torch.log(torch.exp(scores)/torch.sum(torch.exp(norm_scores), 1).unsqueeze(1))) # log-softmax\n",
    "        # scalar (loss must be scalar)    \n",
    "            \n",
    "        return nll # negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | cost: 25.881420 | time: 0m 48s\n",
      "Epoch: 200 | cost: 26.217566 | time: 1m 37s\n",
      "Epoch: 300 | cost: 23.113102 | time: 2m 26s\n",
      "Epoch: 400 | cost: 24.003254 | time: 3m 14s\n",
      "Epoch: 500 | cost: 29.626434 | time: 4m 4s\n"
     ]
    }
   ],
   "source": [
    "#set training parameters\n",
    "window_size = 2\n",
    "batch_size = 10\n",
    "vocab_size = len(vocabs)\n",
    "emb_size = 50\n",
    "model = Skipgram(vocab_size, emb_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#convert all vocabs to tensors\n",
    "def prepare_sequence(vocabs, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], vocabs))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, vocab_size)\n",
    "\n",
    "#calculate epoch time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time // 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "#train the model\n",
    "import time\n",
    "\n",
    "num_epochs = 500\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    input_batch, target_batch = random_batch_skip_gram(window_size, batch_size)\n",
    "    input_batch  = torch.LongTensor(input_batch)  #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch) #[batch_size, 1]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(input_batch, target_batch, all_vocabs)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        end = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the skip-gram model\n",
    "pickle.dump(model, open('Skipgram.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Skip-gram with Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count all the occurrences of vocabs\n",
    "from collections import Counter\n",
    "word_count = Counter(flatten(corpus_tokenized))\n",
    "\n",
    "#count the number of total words\n",
    "num_total_words = sum([c for w, c in word_count.items()])\n",
    "\n",
    "#create the scaled-up unigram distribution table for vocabs\n",
    "z = 0.001 #the scaler\n",
    "unigram_table = []\n",
    "for v in vocabs:\n",
    "    unigram_table.extend([v] * int(((word_count[v]/num_total_words)**0.75)/z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert word indices to tensors\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "#generate random negative samples\n",
    "import random\n",
    "\n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.size(0)\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):\n",
    "        nsample = []\n",
    "        target_index = targets[i].item()\n",
    "        while len(nsample) < k:\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).view(1, -1))\n",
    "    \n",
    "    return torch.cat(neg_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5630, 5219, 2602, 8259, 2688],\n",
       "        [6113,  703, 4486, 1417, 2741],\n",
       "        [4723, 4997, 8459, 8681, 5108],\n",
       "        [6701, 5953, 1590,  497,  925],\n",
       "        [1211, 4928,  782, 3051, 8198],\n",
       "        [2287, 2855, 2776, 5155, 6376],\n",
       "        [5710,  603, 5553, 7308, 2859],\n",
       "        [2013, 8716, 4389, 3051, 2741],\n",
       "        [4052, 5832,  938, 5106, 6535],\n",
       "        [7618, 2631, 4742, 1485, 3807]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the negative sampling method\n",
    "input_batch, target_batch = random_batch_skip_gram(2, 10)\n",
    "input_batch = torch.LongTensor(input_batch)\n",
    "target_batch = torch.LongTensor(target_batch)\n",
    "\n",
    "num_neg = 5 #number of negative samples for each target word\n",
    "\n",
    "neg_samples = negative_sampling(target_batch, unigram_table, num_neg)\n",
    "\n",
    "neg_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip-gram with negative sampling model\n",
    "class SkipgramNegSampling(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(SkipgramNegSampling, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, emb_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, emb_size)\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "                    \n",
    "    def forward(self, center_words, context_words, neg_samples):\n",
    "        center_embeds  = self.embedding_v(center_words) # [batch_size, 1, emb_size]\n",
    "        context_embeds = self.embedding_u(context_words) # [batch_size, 1, emb_size]\n",
    "        neg_embeds     = self.embedding_u(neg_samples) # [batch_size, num_neg, emb_size]\n",
    "        \n",
    "        positive_score = context_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "        \n",
    "        negative_score = -neg_embeds.bmm(center_embeds.transpose(1, 2))\n",
    "        #[batch_size, k, emb_size] @ [batch_size, emb_size, 1] = [batch_size, num_neg, 1]\n",
    "        \n",
    "        loss = -torch.mean(self.logsigmoid(positive_score) + torch.sum(self.logsigmoid(negative_score), 1))\n",
    "                \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | cost: 31.318594 | time: 0m 45s\n",
      "Epoch: 200 | cost: 22.963648 | time: 1m 32s\n",
      "Epoch: 300 | cost: 27.809229 | time: 2m 21s\n",
      "Epoch: 400 | cost: 37.649330 | time: 3m 12s\n",
      "Epoch: 500 | cost: 34.258484 | time: 3m 59s\n"
     ]
    }
   ],
   "source": [
    "#set parameters\n",
    "window_size = 2\n",
    "batch_size  = 10\n",
    "vocab_size  = len(vocabs)\n",
    "emb_size    = 50\n",
    "model       = SkipgramNegSampling(vocab_size, emb_size)\n",
    "num_neg     = 10\n",
    "optimizer   = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#calculate epoch time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time // 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "#train the model\n",
    "import time\n",
    "\n",
    "num_epochs = 500\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):    \n",
    "    input_batch, target_batch = random_batch_skip_gram(window_size, batch_size)\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    target_batch = torch.LongTensor(target_batch)\n",
    "    neg_samples = negative_sampling(target_batch, unigram_table, num_neg)\n",
    "    \n",
    "    optimizer.zero_grad()   \n",
    "    loss = model(input_batch, target_batch, neg_samples)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        end = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the skip-gram with negative sampling model\n",
    "pickle.dump(model, open('SkipgramNegSampling.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. Preparing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[': capital-common-countries',\n",
       " 'Athens Greece Baghdad Iraq',\n",
       " 'Athens Greece Bangkok Thailand',\n",
       " 'Athens Greece Beijing China',\n",
       " 'Athens Greece Berlin Germany']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the dataset for testing\n",
    "file_path = \"data/questions-words.txt\"\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    contents = f.read()\n",
    "    data = contents.split('\\n')\n",
    "\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : capital-common-countries\n",
      "507 : capital-world\n",
      "5032 : currency\n",
      "5899 : city-in-state\n",
      "8367 : family\n",
      "8874 : gram1-adjective-to-adverb\n",
      "9867 : gram2-opposite\n",
      "10680 : gram3-comparative\n",
      "12013 : gram4-superlative\n",
      "13136 : gram5-present-participle\n",
      "14193 : gram6-nationality-adjective\n",
      "15793 : gram7-past-tense\n",
      "17354 : gram8-plural\n",
      "18687 : gram9-plural-verbs\n"
     ]
    }
   ],
   "source": [
    "#explore the dataset\n",
    "for idx, sent in enumerate(data):\n",
    "    if sent[0] == ':':\n",
    "        print(idx, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['boy', 'girl', 'brother', 'sister'], ['boy', 'girl', 'brothers', 'sisters'], ['boy', 'girl', 'dad', 'mom'], ['boy', 'girl', 'father', 'mother'], ['boy', 'girl', 'grandfather', 'grandmother']]\n",
      "[['banana', 'bananas', 'bird', 'birds'], ['banana', 'bananas', 'bottle', 'bottles'], ['banana', 'bananas', 'building', 'buildings'], ['banana', 'bananas', 'car', 'cars'], ['banana', 'bananas', 'cat', 'cats']]\n"
     ]
    }
   ],
   "source": [
    "#create the corpora for testing\n",
    "family = data[8368:8874]\n",
    "family_tokenized = [sent.split(' ') for sent in family]\n",
    "print(family_tokenized[:5])\n",
    "\n",
    "plural = data[17355:18687]\n",
    "plural_tokenized = [sent.split(' ') for sent in plural]\n",
    "print(plural_tokenized[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get unique words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "family_vocabs  = list(set(flatten(family_tokenized)))\n",
    "plural_vocabs  = list(set(flatten(plural_tokenized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numericalize the vocabs\n",
    "family_word2index = {w: i for i, w in enumerate(family_vocabs)}\n",
    "plural_word2index = {w: i for i, w in enumerate(plural_vocabs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append <UNK>\n",
    "family_vocabs.append('<UNK>')\n",
    "family_word2index['<UNK>'] = len(family_word2index)\n",
    "\n",
    "plural_vocabs.append('<UNK>')\n",
    "plural_word2index['<UNK>'] = len(plural_word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare index2word\n",
    "family_index2word = {i:w for w, i in family_word2index.items()}\n",
    "plural_index2word = {i:w for w, i in plural_word2index.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. Syntactic Test\n",
    "\n",
    "##### The 'plural' corpus will be used for syntactic analogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embedding\n",
    "def get_embed(word):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except:\n",
    "        index = word2index['<UNK>']\n",
    "    \n",
    "    word = torch.LongTensor([index])\n",
    "\n",
    "    center_embed  = model.embedding_v(word)\n",
    "    context_embed = model.embedding_u(word)\n",
    "    \n",
    "    embed = (center_embed + context_embed) / 2\n",
    "    \n",
    "    return  embed[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare word2index and index2word\n",
    "word2index = plural_word2index\n",
    "index2word = {i:w for w, i in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'GloVe' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [38], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#get embeddings for GloVe\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(\u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mGloVe.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m      3\u001b[0m GloVe_embeds \u001b[39m=\u001b[39m {}\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(word2index)):\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't get attribute 'GloVe' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "#get embeddings for GloVe\n",
    "model = pickle.load(open('GloVe.pkl', 'rb'))\n",
    "GloVe_embeds = {}\n",
    "\n",
    "for i in range(len(word2index)):\n",
    "    word = index2word[i]\n",
    "    embed = get_embed(word)\n",
    "    GloVe_embeds.update({word: embed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings for CBOW\n",
    "model = pickle.load(open('CBOW.pkl', 'rb'))\n",
    "CBOW_embeds = {}\n",
    "\n",
    "for i in range(len(word2index)):\n",
    "    word = index2word[i]\n",
    "    embed = get_embed(word)\n",
    "    CBOW_embeds.update({word: embed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings for Skipgram\n",
    "model = pickle.load(open('Skipgram.pkl', 'rb'))\n",
    "Skipgram_embeds = {}\n",
    "\n",
    "for i in range(len(word2index)):\n",
    "    word = index2word[i]\n",
    "    embed = get_embed(word)\n",
    "    Skipgram_embeds.update({word: embed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings for SkipgramNegSampling\n",
    "model = pickle.load(open('SkipgramNegSampling.pkl', 'rb'))\n",
    "SkipgramNegSampling_embeds = {}\n",
    "\n",
    "for i in range(len(word2index)):\n",
    "    word = index2word[i]\n",
    "    embed = get_embed(word)\n",
    "    SkipgramNegSampling_embeds.update({word: embed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine similarity function\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analogy function\n",
    "def analogy(a, b, c, embeds, vocabs):\n",
    "    d_vector = embeds[c] - embeds[a] + embeds[b]\n",
    "\n",
    "    similarity = -1\n",
    "    for vocab in vocabs:\n",
    "        if vocab not in [a, b, c]:\n",
    "            if cos_sim(d_vector, embeds[vocab]) > similarity:\n",
    "                similarity = cos_sim(d_vector, embeds[vocab])\n",
    "                d = (vocab, similarity)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy('car', 'cars', 'lion', GloVe_embeds, plural_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy function\n",
    "def accuracy(label, pred):\n",
    "    if pred == label:\n",
    "        True\n",
    "    else:\n",
    "        False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the syntactic accuracies of model embeddings\n",
    "models = ['GloVe', 'CBOW', 'Skipgram', 'SkipgramNegSampling']\n",
    "embeds = [GloVe_embeds, CBOW_embeds, Skipgram_embeds, SkipgramNegSampling_embeds]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    embed = embeds[i]\n",
    "    accuracy_count = 0\n",
    "    for sent in plural_tokenized:\n",
    "        label = sent[-1]\n",
    "        a, b, c = sent[:-1]\n",
    "        pred = analogy(a, b, c, embed, plural_vocabs)[0]\n",
    "        if accuracy(label, pred) is True:\n",
    "            accuracy_count += 1\n",
    "    \n",
    "    print(f'The accuracy of {model} is {accuracy_count/len(plural_tokenized)}%.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Semantic Test\n",
    "\n",
    "The 'family' corpus will be used for semantic analogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare word2index and index2word\n",
    "word2index = family_word2index\n",
    "index2word = {i:w for w, i in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings for GloVe\n",
    "model = pickle.load(open('GloVe.pkl', 'rb'))\n",
    "GloVe_embeds = {}\n",
    "\n",
    "for i in range(len(word2index)):\n",
    "    word = index2word[i]\n",
    "    embed = get_embed(word)\n",
    "    GloVe_embeds.update({word: embed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings for CBOW\n",
    "model = pickle.load(open('CBOW.pkl', 'rb'))\n",
    "CBOW_embeds = {}\n",
    "\n",
    "for i in range(len(word2index)):\n",
    "    word = index2word[i]\n",
    "    embed = get_embed(word)\n",
    "    CBOW_embeds.update({word: embed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings for Skipgram\n",
    "model = pickle.load(open('Skipgram.pkl', 'rb'))\n",
    "Skipgram_embeds = {}\n",
    "\n",
    "for i in range(len(word2index)):\n",
    "    word = index2word[i]\n",
    "    embed = get_embed(word)\n",
    "    Skipgram_embeds.update({word: embed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings for SkipgramNegSampling\n",
    "model = pickle.load(open('SkipgramNegSampling.pkl', 'rb'))\n",
    "SkipgramNegSampling_embeds = {}\n",
    "\n",
    "for i in range(len(word2index)):\n",
    "    word = index2word[i]\n",
    "    embed = get_embed(word)\n",
    "    SkipgramNegSampling_embeds.update({word: embed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the semantic accuracies of model embeddings\n",
    "models = ['GloVe', 'CBOW', 'Skipgram', 'SkipgramNegSampling']\n",
    "embeds = [GloVe_embeds, CBOW_embeds, Skipgram_embeds, SkipgramNegSampling_embeds]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    embed = embeds[i]\n",
    "    accuracy_count = 0\n",
    "    for sent in family_tokenized:\n",
    "        label = sent[-1]\n",
    "        a, b, c = sent[:-1]\n",
    "        pred = analogy(a, b, c, embed, family_vocabs)[0]\n",
    "        if accuracy(label, pred) is True:\n",
    "            accuracy_count += 1\n",
    "    \n",
    "    print(f'The accuracy of {model} is {accuracy_count/len(plural_tokenized)}%.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4. Findings and Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The 4 embedding models were trained with the following parameters:**\n",
    "- window size = 2\n",
    "- batch size = 10\n",
    "- embedding size = 50\n",
    "- number of negative samples = 10\n",
    "- optimizer = Adam\n",
    "- learning rate = 0.001\n",
    "\n",
    "**Then, the embeddings were tested on syntactic and semantic analogies. The test results are as follows:**\n",
    "\n",
    "| Model | Syntactic Accuracy | Semantic Accuracy  |\n",
    "| --- | --- | ---  |\n",
    "| GloVe | 0.0 | 0.0  |\n",
    "| CBOW | 0.0 | 0.0  |\n",
    "| Skip-gram | 0.0 | 0.0  |\n",
    "| Skip-gram (Neg) | 0.0 | 0.0  |\n",
    "\n",
    "**All models achieved 0% accuracy. It may be due to the limited size of the corpus.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "import pandas as pd\n",
    "file_path = \"data/wordsim_similarity_goldstandard.txt\"\n",
    "df = pd.read_table(file_path, header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract features and labels\n",
    "x1 = df.iloc[:, 0]\n",
    "x2 = df.iloc[:, 1]\n",
    "y  = df.iloc[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recall the Inaugural corpus\n",
    "print(corpus_tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare word2index\n",
    "vocabs = list(set(flatten(corpus_tokenized)))\n",
    "word2index = {w: i for i, w in enumerate(vocabs)}\n",
    "\n",
    "vocabs.append('<UNK>')\n",
    "word2index['<UNK>'] = len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embedding\n",
    "def get_embed(word):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except:\n",
    "        index = word2index['<UNK>']\n",
    "    \n",
    "    word = torch.LongTensor([index])\n",
    "\n",
    "    center_embed  = model.embedding_v(word)\n",
    "    context_embed = model.embedding_u(word)\n",
    "    \n",
    "    embed = (center_embed + context_embed) / 2\n",
    "    \n",
    "    return  embed[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate Spearman's correlation\n",
    "from scipy import stats\n",
    "\n",
    "def correlation(x1, x2):\n",
    "    x1_embed = get_embed(x1)\n",
    "    x2_embed = get_embed(x2)\n",
    "    return stats.spearmanr(x1_embed, x2_embed)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "model = pickle.load(open('GloVe.pkl', 'rb'))\n",
    "\n",
    "print(correlation('tiger', 'cat'))\n",
    "print(correlation('tiger', 'tiger'))\n",
    "print(correlation('plane', 'car'))\n",
    "print(correlation('train', 'car'))\n",
    "print(correlation('television', 'radio'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open('CBOW.pkl', 'rb'))\n",
    "\n",
    "print(correlation('tiger', 'cat'))\n",
    "print(correlation('tiger', 'tiger'))\n",
    "print(correlation('plane', 'car'))\n",
    "print(correlation('train', 'car'))\n",
    "print(correlation('television', 'radio'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcuate word similarity correlations of model embeddings\n",
    "models = ['GloVe.pkl', 'CBOW.pkl', 'Skipgram.pkl', 'SkipgramNegSampling.pkl']\n",
    "yhats = [[]] * len(models)\n",
    "\n",
    "for m in range(len(models)):\n",
    "    model = pickle.load(open(models[m], 'rb'))\n",
    "\n",
    "    yhat_list = []\n",
    "    for i in range(len(y)):\n",
    "        yhat_list.append(correlation(x1[i], x2[i])*10)\n",
    "\n",
    "    yhats[m] = yhat_list\n",
    "    \n",
    "    print(yhats[m][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate word similarity correlations of model embeddings\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "models = ['GloVe', 'CBOW', 'Skipgram', 'SkipgramNegSampling']\n",
    "\n",
    "for m in range(len(models)):\n",
    "    yhat = yhats[m]\n",
    "    loss = mse(y, yhat)\n",
    "    print(f'MSE of the word similarity correlations of {models[m]} is {loss:.2f}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Although CBOW gets the lowest error, all models perform more or less the same in terms of word similarity based on the WS353 dataset. However, as many words from the dataset are unknown in the corpus, the performance scores are not reliable.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7ca07084f99cae884d00a2401f5a915152405bd446c47609c53897335e04337"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
