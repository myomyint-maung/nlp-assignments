{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19 Jan - GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the training corpus\n",
    "#I use the Inaugural corpus from NLTK\n",
    "import nltk\n",
    "from nltk.corpus import inaugural\n",
    "corpus = nltk.corpus.inaugural.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the', 'House', 'of', 'Representatives', ':'], ['Among', 'the', 'vicissitudes', 'incident', 'to', 'life', 'no', 'event', 'could', 'have', 'filled', 'me', 'with', 'greater', 'anxieties', 'than', 'that', 'of', 'which', 'the', 'notification', 'was', 'transmitted', 'by', 'your', 'order', ',', 'and', 'received', 'on', 'the', '14th', 'day', 'of', 'the', 'present', 'month', '.'], ['On', 'the', 'one', 'hand', ',', 'I', 'was', 'summoned', 'by', 'my', 'Country', ',', 'whose', 'voice', 'I', 'can', 'never', 'hear', 'but', 'with', 'veneration', 'and', 'love', ',', 'from', 'a', 'retreat', 'which', 'I', 'had', 'chosen', 'with', 'the', 'fondest', 'predilection', ',', 'and', ',', 'in', 'my', 'flattering', 'hopes', ',', 'with', 'an', 'immutable', 'decision', ',', 'as', 'the', 'asylum', 'of', 'my', 'declining', 'years', '--', 'a', 'retreat', 'which', 'was', 'rendered', 'every', 'day', 'more', 'necessary', 'as', 'well', 'as', 'more', 'dear', 'to', 'me', 'by', 'the', 'addition', 'of', 'habit', 'to', 'inclination', ',', 'and', 'of', 'frequent', 'interruptions', 'in', 'my', 'health', 'to', 'the', 'gradual', 'waste', 'committed', 'on', 'it', 'by', 'time', '.']]\n"
     ]
    }
   ],
   "source": [
    "#the corpus is already tokenized\n",
    "print(corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fellow', '-', 'citizens', 'of', 'the', 'senate', 'and', 'of', 'the', 'house', 'of', 'representatives', ':'], ['among', 'the', 'vicissitudes', 'incident', 'to', 'life', 'no', 'event', 'could', 'have', 'filled', 'me', 'with', 'greater', 'anxieties', 'than', 'that', 'of', 'which', 'the', 'notification', 'was', 'transmitted', 'by', 'your', 'order', ',', 'and', 'received', 'on', 'the', '14th', 'day', 'of', 'the', 'present', 'month', '.'], ['on', 'the', 'one', 'hand', ',', 'i', 'was', 'summoned', 'by', 'my', 'country', ',', 'whose', 'voice', 'i', 'can', 'never', 'hear', 'but', 'with', 'veneration', 'and', 'love', ',', 'from', 'a', 'retreat', 'which', 'i', 'had', 'chosen', 'with', 'the', 'fondest', 'predilection', ',', 'and', ',', 'in', 'my', 'flattering', 'hopes', ',', 'with', 'an', 'immutable', 'decision', ',', 'as', 'the', 'asylum', 'of', 'my', 'declining', 'years', '--', 'a', 'retreat', 'which', 'was', 'rendered', 'every', 'day', 'more', 'necessary', 'as', 'well', 'as', 'more', 'dear', 'to', 'me', 'by', 'the', 'addition', 'of', 'habit', 'to', 'inclination', ',', 'and', 'of', 'frequent', 'interruptions', 'in', 'my', 'health', 'to', 'the', 'gradual', 'waste', 'committed', 'on', 'it', 'by', 'time', '.']]\n"
     ]
    }
   ],
   "source": [
    "#convert the words in the corpus into lower case\n",
    "corpus_tokenized = [[]] * len(corpus)\n",
    "for i in range(len(corpus)):\n",
    "    corpus_tokenized[i] = [word.lower() for word in corpus[i]]\n",
    "\n",
    "print(corpus_tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fellow', '-', 'citizens', 'senate', 'house', 'representatives', ':'], ['vicissitudes', 'incident', 'life', 'event', 'filled', 'greater', 'anxieties', 'notification', 'transmitted', 'order', ',', 'received', '14th', 'day', 'present', 'month', '.'], ['hand', ',', 'summoned', 'country', ',', 'voice', 'hear', 'veneration', 'love', ',', 'retreat', 'chosen', 'fondest', 'predilection', ',', ',', 'flattering', 'hopes', ',', 'immutable', 'decision', ',', 'asylum', 'declining', 'years', '--', 'retreat', 'rendered', 'day', 'necessary', 'dear', 'addition', 'habit', 'inclination', ',', 'frequent', 'interruptions', 'health', 'gradual', 'waste', 'committed', 'time', '.']]\n"
     ]
    }
   ],
   "source": [
    "#remove stop words with SpaCy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "for sentence in corpus_tokenized:\n",
    "    for word in sentence[:]:\n",
    "        if word in stopwords:\n",
    "            sentence.remove(word)\n",
    "\n",
    "print(corpus_tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fellow', 'citizens', 'senate', 'house', 'representatives'], ['vicissitudes', 'incident', 'life', 'event', 'filled', 'greater', 'anxieties', 'notification', 'transmitted', 'order', 'received', '14th', 'day', 'present', 'month'], ['hand', 'summoned', 'country', 'voice', 'hear', 'veneration', 'love', 'retreat', 'chosen', 'fondest', 'predilection', 'flattering', 'hopes', 'immutable', 'decision', 'asylum', 'declining', 'years', '--', 'retreat', 'rendered', 'day', 'necessary', 'dear', 'addition', 'habit', 'inclination', 'frequent', 'interruptions', 'health', 'gradual', 'waste', 'committed', 'time']]\n"
     ]
    }
   ],
   "source": [
    "#remove punctutations with String\n",
    "import string\n",
    "punctutations = string.punctuation\n",
    "\n",
    "for sentence in corpus_tokenized:\n",
    "    for word in sentence[:]:\n",
    "        if word in punctutations:\n",
    "            sentence.remove(word)\n",
    "\n",
    "print(corpus_tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fellow', 'citizens', 'senate', 'house', 'representatives'], ['vicissitudes', 'incident', 'life', 'event', 'filled', 'greater', 'anxieties', 'notification', 'transmitted', 'order', 'received', '14th', 'day', 'present', 'month'], ['hand', 'summoned', 'country', 'voice', 'hear', 'veneration', 'love', 'retreat', 'chosen', 'fondest', 'predilection', 'flattering', 'hopes', 'immutable', 'decision', 'asylum', 'declining', 'years', 'retreat', 'rendered', 'day', 'necessary', 'dear', 'addition', 'habit', 'inclination', 'frequent', 'interruptions', 'health', 'gradual', 'waste', 'committed', 'time']]\n"
     ]
    }
   ],
   "source": [
    "#remove '--'\n",
    "for sentence in corpus_tokenized:\n",
    "    for word in sentence[:]:\n",
    "        if word == '--':\n",
    "            sentence.remove(word)\n",
    "\n",
    "print(corpus_tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get unique words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabs = list(set(flatten(corpus_tokenized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numericalize the vocabs\n",
    "word2index = {w: i for i, w in enumerate(vocabs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append <UNK>\n",
    "vocabs.append('<UNK>')\n",
    "word2index['<UNK>'] = len(word2index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GloVe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Co-occurence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count the frequency of each word\n",
    "from collections import Counter\n",
    "\n",
    "X_i = Counter(flatten(corpus_tokenized))\n",
    "\n",
    "X_i['fellow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate skipgrams with a generic window size\n",
    "def generate_skip_gram(window_size): \n",
    "    skip_grams = []\n",
    "    for sentence in corpus_tokenized:\n",
    "        #I include the first and last words\n",
    "        #so that every word could be a center word\n",
    "        for i in range(len(sentence)):\n",
    "            center = sentence[i]\n",
    "            context = []\n",
    "            for j in range(window_size):\n",
    "                if (i - window_size + j) >= 0:\n",
    "                    context.append(sentence[i - window_size + j])\n",
    "            for k in range(1, window_size + 1):\n",
    "                if (i + k) < len(sentence):\n",
    "                    context.append(sentence[i + k])\n",
    "            for w in context:\n",
    "                skip_grams.append((center, w))\n",
    "        \n",
    "    return skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fellow', 'citizens'),\n",
       " ('fellow', 'senate'),\n",
       " ('citizens', 'fellow'),\n",
       " ('citizens', 'senate'),\n",
       " ('citizens', 'house')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare skipgrams with window size of 2\n",
    "skip_grams = generate_skip_gram(2)\n",
    "\n",
    "skip_grams[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('fellow', 'citizens'): 117,\n",
       "         ('fellow', 'senate'): 1,\n",
       "         ('citizens', 'fellow'): 117,\n",
       "         ('citizens', 'senate'): 1,\n",
       "         ('citizens', 'house'): 1,\n",
       "         ('senate', 'fellow'): 1,\n",
       "         ('senate', 'citizens'): 1,\n",
       "         ('senate', 'house'): 4,\n",
       "         ('senate', 'representatives'): 1,\n",
       "         ('house', 'citizens'): 1,\n",
       "         ('house', 'senate'): 4,\n",
       "         ('house', 'representatives'): 4,\n",
       "         ('representatives', 'senate'): 1,\n",
       "         ('representatives', 'house'): 4,\n",
       "         ('vicissitudes', 'incident'): 1,\n",
       "         ('vicissitudes', 'life'): 1,\n",
       "         ('incident', 'vicissitudes'): 1,\n",
       "         ('incident', 'life'): 1,\n",
       "         ('incident', 'event'): 1,\n",
       "         ('life', 'vicissitudes'): 1,\n",
       "         ('life', 'incident'): 1,\n",
       "         ('life', 'event'): 1,\n",
       "         ('life', 'filled'): 1,\n",
       "         ('event', 'incident'): 1,\n",
       "         ('event', 'life'): 1,\n",
       "         ('event', 'filled'): 1,\n",
       "         ('event', 'greater'): 1,\n",
       "         ('filled', 'life'): 1,\n",
       "         ('filled', 'event'): 1,\n",
       "         ('filled', 'greater'): 1,\n",
       "         ('filled', 'anxieties'): 1,\n",
       "         ('greater', 'event'): 1,\n",
       "         ('greater', 'filled'): 1,\n",
       "         ('greater', 'anxieties'): 1,\n",
       "         ('greater', 'notification'): 1,\n",
       "         ('anxieties', 'filled'): 1,\n",
       "         ('anxieties', 'greater'): 1,\n",
       "         ('anxieties', 'notification'): 1,\n",
       "         ('anxieties', 'transmitted'): 1,\n",
       "         ('notification', 'greater'): 1,\n",
       "         ('notification', 'anxieties'): 1,\n",
       "         ('notification', 'transmitted'): 1,\n",
       "         ('notification', 'order'): 1,\n",
       "         ('transmitted', 'anxieties'): 1,\n",
       "         ('transmitted', 'notification'): 1,\n",
       "         ('transmitted', 'order'): 1,\n",
       "         ('transmitted', 'received'): 1,\n",
       "         ('order', 'notification'): 1,\n",
       "         ('order', 'transmitted'): 1,\n",
       "         ('order', 'received'): 1,\n",
       "         ('order', '14th'): 1,\n",
       "         ('received', 'transmitted'): 1,\n",
       "         ('received', 'order'): 1,\n",
       "         ('received', '14th'): 1,\n",
       "         ('received', 'day'): 1,\n",
       "         ('14th', 'order'): 1,\n",
       "         ('14th', 'received'): 1,\n",
       "         ('14th', 'day'): 1,\n",
       "         ('14th', 'present'): 1,\n",
       "         ('day', 'received'): 1,\n",
       "         ('day', '14th'): 1,\n",
       "         ('day', 'present'): 7,\n",
       "         ('day', 'month'): 1,\n",
       "         ('present', '14th'): 1,\n",
       "         ('present', 'day'): 7,\n",
       "         ('present', 'month'): 1,\n",
       "         ('month', 'day'): 1,\n",
       "         ('month', 'present'): 1,\n",
       "         ('hand', 'summoned'): 1,\n",
       "         ('hand', 'country'): 1,\n",
       "         ('summoned', 'hand'): 1,\n",
       "         ('summoned', 'country'): 1,\n",
       "         ('summoned', 'voice'): 1,\n",
       "         ('country', 'hand'): 1,\n",
       "         ('country', 'summoned'): 1,\n",
       "         ('country', 'voice'): 4,\n",
       "         ('country', 'hear'): 1,\n",
       "         ('voice', 'summoned'): 1,\n",
       "         ('voice', 'country'): 4,\n",
       "         ('voice', 'hear'): 1,\n",
       "         ('voice', 'veneration'): 1,\n",
       "         ('hear', 'country'): 1,\n",
       "         ('hear', 'voice'): 1,\n",
       "         ('hear', 'veneration'): 1,\n",
       "         ('hear', 'love'): 1,\n",
       "         ('veneration', 'voice'): 1,\n",
       "         ('veneration', 'hear'): 1,\n",
       "         ('veneration', 'love'): 2,\n",
       "         ('veneration', 'retreat'): 1,\n",
       "         ('love', 'hear'): 1,\n",
       "         ('love', 'veneration'): 2,\n",
       "         ('love', 'retreat'): 1,\n",
       "         ('love', 'chosen'): 1,\n",
       "         ('retreat', 'veneration'): 1,\n",
       "         ('retreat', 'love'): 1,\n",
       "         ('retreat', 'chosen'): 1,\n",
       "         ('retreat', 'fondest'): 1,\n",
       "         ('chosen', 'love'): 1,\n",
       "         ('chosen', 'retreat'): 1,\n",
       "         ('chosen', 'fondest'): 1,\n",
       "         ('chosen', 'predilection'): 1,\n",
       "         ('fondest', 'retreat'): 1,\n",
       "         ('fondest', 'chosen'): 1,\n",
       "         ('fondest', 'predilection'): 1,\n",
       "         ('fondest', 'flattering'): 1,\n",
       "         ('predilection', 'chosen'): 1,\n",
       "         ('predilection', 'fondest'): 1,\n",
       "         ('predilection', 'flattering'): 1,\n",
       "         ('predilection', 'hopes'): 1,\n",
       "         ('flattering', 'fondest'): 1,\n",
       "         ('flattering', 'predilection'): 1,\n",
       "         ('flattering', 'hopes'): 1,\n",
       "         ('flattering', 'immutable'): 1,\n",
       "         ('hopes', 'predilection'): 1,\n",
       "         ('hopes', 'flattering'): 1,\n",
       "         ('hopes', 'immutable'): 1,\n",
       "         ('hopes', 'decision'): 1,\n",
       "         ('immutable', 'flattering'): 1,\n",
       "         ('immutable', 'hopes'): 1,\n",
       "         ('immutable', 'decision'): 1,\n",
       "         ('immutable', 'asylum'): 1,\n",
       "         ('decision', 'hopes'): 1,\n",
       "         ('decision', 'immutable'): 1,\n",
       "         ('decision', 'asylum'): 1,\n",
       "         ('decision', 'declining'): 1,\n",
       "         ('asylum', 'immutable'): 1,\n",
       "         ('asylum', 'decision'): 1,\n",
       "         ('asylum', 'declining'): 1,\n",
       "         ('asylum', 'years'): 1,\n",
       "         ('declining', 'decision'): 1,\n",
       "         ('declining', 'asylum'): 1,\n",
       "         ('declining', 'years'): 2,\n",
       "         ('declining', 'retreat'): 1,\n",
       "         ('years', 'asylum'): 1,\n",
       "         ('years', 'declining'): 2,\n",
       "         ('years', 'retreat'): 1,\n",
       "         ('years', 'rendered'): 1,\n",
       "         ('retreat', 'declining'): 1,\n",
       "         ('retreat', 'years'): 1,\n",
       "         ('retreat', 'rendered'): 1,\n",
       "         ('retreat', 'day'): 1,\n",
       "         ('rendered', 'years'): 1,\n",
       "         ('rendered', 'retreat'): 1,\n",
       "         ('rendered', 'day'): 1,\n",
       "         ('rendered', 'necessary'): 1,\n",
       "         ('day', 'retreat'): 1,\n",
       "         ('day', 'rendered'): 1,\n",
       "         ('day', 'necessary'): 1,\n",
       "         ('day', 'dear'): 1,\n",
       "         ('necessary', 'rendered'): 1,\n",
       "         ('necessary', 'day'): 1,\n",
       "         ('necessary', 'dear'): 1,\n",
       "         ('necessary', 'addition'): 1,\n",
       "         ('dear', 'day'): 1,\n",
       "         ('dear', 'necessary'): 1,\n",
       "         ('dear', 'addition'): 1,\n",
       "         ('dear', 'habit'): 1,\n",
       "         ('addition', 'necessary'): 1,\n",
       "         ('addition', 'dear'): 1,\n",
       "         ('addition', 'habit'): 1,\n",
       "         ('addition', 'inclination'): 1,\n",
       "         ('habit', 'dear'): 1,\n",
       "         ('habit', 'addition'): 1,\n",
       "         ('habit', 'inclination'): 1,\n",
       "         ('habit', 'frequent'): 1,\n",
       "         ('inclination', 'addition'): 1,\n",
       "         ('inclination', 'habit'): 1,\n",
       "         ('inclination', 'frequent'): 1,\n",
       "         ('inclination', 'interruptions'): 1,\n",
       "         ('frequent', 'habit'): 1,\n",
       "         ('frequent', 'inclination'): 1,\n",
       "         ('frequent', 'interruptions'): 1,\n",
       "         ('frequent', 'health'): 1,\n",
       "         ('interruptions', 'inclination'): 1,\n",
       "         ('interruptions', 'frequent'): 1,\n",
       "         ('interruptions', 'health'): 1,\n",
       "         ('interruptions', 'gradual'): 1,\n",
       "         ('health', 'frequent'): 1,\n",
       "         ('health', 'interruptions'): 1,\n",
       "         ('health', 'gradual'): 1,\n",
       "         ('health', 'waste'): 1,\n",
       "         ('gradual', 'interruptions'): 1,\n",
       "         ('gradual', 'health'): 1,\n",
       "         ('gradual', 'waste'): 1,\n",
       "         ('gradual', 'committed'): 1,\n",
       "         ('waste', 'health'): 1,\n",
       "         ('waste', 'gradual'): 1,\n",
       "         ('waste', 'committed'): 1,\n",
       "         ('waste', 'time'): 2,\n",
       "         ('committed', 'gradual'): 1,\n",
       "         ('committed', 'waste'): 1,\n",
       "         ('committed', 'time'): 1,\n",
       "         ('time', 'waste'): 2,\n",
       "         ('time', 'committed'): 1,\n",
       "         ('hand', 'magnitude'): 1,\n",
       "         ('hand', 'difficulty'): 1,\n",
       "         ('magnitude', 'hand'): 1,\n",
       "         ('magnitude', 'difficulty'): 1,\n",
       "         ('magnitude', 'trust'): 1,\n",
       "         ('difficulty', 'hand'): 1,\n",
       "         ('difficulty', 'magnitude'): 1,\n",
       "         ('difficulty', 'trust'): 1,\n",
       "         ('difficulty', 'voice'): 1,\n",
       "         ('trust', 'magnitude'): 1,\n",
       "         ('trust', 'difficulty'): 1,\n",
       "         ('trust', 'voice'): 1,\n",
       "         ('trust', 'country'): 2,\n",
       "         ('voice', 'difficulty'): 1,\n",
       "         ('voice', 'trust'): 1,\n",
       "         ('voice', 'called'): 4,\n",
       "         ('country', 'trust'): 2,\n",
       "         ('country', 'called'): 3,\n",
       "         ('country', 'sufficient'): 1,\n",
       "         ('called', 'voice'): 4,\n",
       "         ('called', 'country'): 3,\n",
       "         ('called', 'sufficient'): 2,\n",
       "         ('called', 'awaken'): 1,\n",
       "         ('sufficient', 'country'): 1,\n",
       "         ('sufficient', 'called'): 2,\n",
       "         ('sufficient', 'awaken'): 1,\n",
       "         ('sufficient', 'wisest'): 1,\n",
       "         ('awaken', 'called'): 1,\n",
       "         ('awaken', 'sufficient'): 1,\n",
       "         ('awaken', 'wisest'): 1,\n",
       "         ('awaken', 'experienced'): 1,\n",
       "         ('wisest', 'sufficient'): 1,\n",
       "         ('wisest', 'awaken'): 1,\n",
       "         ('wisest', 'experienced'): 1,\n",
       "         ('wisest', 'citizens'): 1,\n",
       "         ('experienced', 'awaken'): 1,\n",
       "         ('experienced', 'wisest'): 1,\n",
       "         ('experienced', 'citizens'): 1,\n",
       "         ('experienced', 'distrustful'): 1,\n",
       "         ('citizens', 'wisest'): 1,\n",
       "         ('citizens', 'experienced'): 1,\n",
       "         ('citizens', 'distrustful'): 1,\n",
       "         ('citizens', 'scrutiny'): 1,\n",
       "         ('distrustful', 'experienced'): 1,\n",
       "         ('distrustful', 'citizens'): 1,\n",
       "         ('distrustful', 'scrutiny'): 1,\n",
       "         ('distrustful', 'qualifications'): 1,\n",
       "         ('scrutiny', 'citizens'): 1,\n",
       "         ('scrutiny', 'distrustful'): 1,\n",
       "         ('scrutiny', 'qualifications'): 1,\n",
       "         ('scrutiny', 'overwhelm'): 1,\n",
       "         ('qualifications', 'distrustful'): 1,\n",
       "         ('qualifications', 'scrutiny'): 1,\n",
       "         ('qualifications', 'overwhelm'): 1,\n",
       "         ('qualifications', 'despondence'): 1,\n",
       "         ('overwhelm', 'scrutiny'): 1,\n",
       "         ('overwhelm', 'qualifications'): 1,\n",
       "         ('overwhelm', 'despondence'): 1,\n",
       "         ('overwhelm', 'inheriting'): 1,\n",
       "         ('despondence', 'qualifications'): 1,\n",
       "         ('despondence', 'overwhelm'): 1,\n",
       "         ('despondence', 'inheriting'): 1,\n",
       "         ('despondence', 'inferior'): 1,\n",
       "         ('inheriting', 'overwhelm'): 1,\n",
       "         ('inheriting', 'despondence'): 1,\n",
       "         ('inheriting', 'inferior'): 1,\n",
       "         ('inheriting', 'endowments'): 1,\n",
       "         ('inferior', 'despondence'): 1,\n",
       "         ('inferior', 'inheriting'): 1,\n",
       "         ('inferior', 'endowments'): 1,\n",
       "         ('inferior', 'nature'): 1,\n",
       "         ('endowments', 'inheriting'): 1,\n",
       "         ('endowments', 'inferior'): 1,\n",
       "         ('endowments', 'nature'): 1,\n",
       "         ('endowments', 'unpracticed'): 1,\n",
       "         ('nature', 'inferior'): 1,\n",
       "         ('nature', 'endowments'): 1,\n",
       "         ('nature', 'unpracticed'): 1,\n",
       "         ('nature', 'duties'): 3,\n",
       "         ('unpracticed', 'endowments'): 1,\n",
       "         ('unpracticed', 'nature'): 1,\n",
       "         ('unpracticed', 'duties'): 1,\n",
       "         ('unpracticed', 'civil'): 1,\n",
       "         ('duties', 'nature'): 3,\n",
       "         ('duties', 'unpracticed'): 1,\n",
       "         ('duties', 'civil'): 1,\n",
       "         ('duties', 'administration'): 3,\n",
       "         ('civil', 'unpracticed'): 1,\n",
       "         ('civil', 'duties'): 1,\n",
       "         ('civil', 'administration'): 1,\n",
       "         ('civil', 'ought'): 1,\n",
       "         ('administration', 'duties'): 3,\n",
       "         ('administration', 'civil'): 1,\n",
       "         ('administration', 'ought'): 2,\n",
       "         ('administration', 'peculiarly'): 1,\n",
       "         ('ought', 'civil'): 1,\n",
       "         ('ought', 'administration'): 2,\n",
       "         ('ought', 'peculiarly'): 1,\n",
       "         ('ought', 'conscious'): 1,\n",
       "         ('peculiarly', 'administration'): 1,\n",
       "         ('peculiarly', 'ought'): 1,\n",
       "         ('peculiarly', 'conscious'): 1,\n",
       "         ('peculiarly', 'deficiencies'): 1,\n",
       "         ('conscious', 'ought'): 1,\n",
       "         ('conscious', 'peculiarly'): 1,\n",
       "         ('conscious', 'deficiencies'): 1,\n",
       "         ('deficiencies', 'peculiarly'): 1,\n",
       "         ('deficiencies', 'conscious'): 1,\n",
       "         ('conflict', 'emotions'): 1,\n",
       "         ('conflict', 'dare'): 1,\n",
       "         ('emotions', 'conflict'): 1,\n",
       "         ('emotions', 'dare'): 1,\n",
       "         ('emotions', 'aver'): 1,\n",
       "         ('dare', 'conflict'): 1,\n",
       "         ('dare', 'emotions'): 1,\n",
       "         ('dare', 'aver'): 1,\n",
       "         ('dare', 'faithful'): 1,\n",
       "         ('aver', 'emotions'): 1,\n",
       "         ('aver', 'dare'): 1,\n",
       "         ('aver', 'faithful'): 1,\n",
       "         ('aver', 'study'): 1,\n",
       "         ('faithful', 'dare'): 1,\n",
       "         ('faithful', 'aver'): 1,\n",
       "         ('faithful', 'study'): 1,\n",
       "         ('faithful', 'collect'): 1,\n",
       "         ('study', 'aver'): 1,\n",
       "         ('study', 'faithful'): 1,\n",
       "         ('study', 'collect'): 1,\n",
       "         ('study', 'duty'): 1,\n",
       "         ('collect', 'faithful'): 1,\n",
       "         ('collect', 'study'): 1,\n",
       "         ('collect', 'duty'): 1,\n",
       "         ('collect', 'appreciation'): 1,\n",
       "         ('duty', 'study'): 1,\n",
       "         ('duty', 'collect'): 1,\n",
       "         ('duty', 'appreciation'): 1,\n",
       "         ('duty', 'circumstance'): 1,\n",
       "         ('appreciation', 'collect'): 1,\n",
       "         ('appreciation', 'duty'): 1,\n",
       "         ('appreciation', 'circumstance'): 1,\n",
       "         ('appreciation', 'affected'): 1,\n",
       "         ('circumstance', 'duty'): 1,\n",
       "         ('circumstance', 'appreciation'): 1,\n",
       "         ('circumstance', 'affected'): 1,\n",
       "         ('affected', 'appreciation'): 1,\n",
       "         ('affected', 'circumstance'): 1,\n",
       "         ('dare', 'hope'): 1,\n",
       "         ('dare', 'executing'): 1,\n",
       "         ('hope', 'dare'): 1,\n",
       "         ('hope', 'executing'): 1,\n",
       "         ('hope', 'task'): 1,\n",
       "         ('executing', 'dare'): 1,\n",
       "         ('executing', 'hope'): 1,\n",
       "         ('executing', 'task'): 1,\n",
       "         ('executing', 'swayed'): 1,\n",
       "         ('task', 'hope'): 1,\n",
       "         ('task', 'executing'): 1,\n",
       "         ('task', 'swayed'): 1,\n",
       "         ('task', 'grateful'): 2,\n",
       "         ('swayed', 'executing'): 1,\n",
       "         ('swayed', 'task'): 1,\n",
       "         ('swayed', 'grateful'): 1,\n",
       "         ('swayed', 'remembrance'): 1,\n",
       "         ('grateful', 'task'): 2,\n",
       "         ('grateful', 'swayed'): 1,\n",
       "         ('grateful', 'remembrance'): 1,\n",
       "         ('grateful', 'instances'): 1,\n",
       "         ('remembrance', 'swayed'): 1,\n",
       "         ('remembrance', 'grateful'): 1,\n",
       "         ('remembrance', 'instances'): 1,\n",
       "         ('remembrance', 'affectionate'): 1,\n",
       "         ('instances', 'grateful'): 1,\n",
       "         ('instances', 'remembrance'): 1,\n",
       "         ('instances', 'affectionate'): 1,\n",
       "         ('instances', 'sensibility'): 1,\n",
       "         ('affectionate', 'remembrance'): 1,\n",
       "         ('affectionate', 'instances'): 1,\n",
       "         ('affectionate', 'sensibility'): 1,\n",
       "         ('affectionate', 'transcendent'): 1,\n",
       "         ('sensibility', 'instances'): 1,\n",
       "         ('sensibility', 'affectionate'): 1,\n",
       "         ('sensibility', 'transcendent'): 1,\n",
       "         ('sensibility', 'proof'): 1,\n",
       "         ('transcendent', 'affectionate'): 1,\n",
       "         ('transcendent', 'sensibility'): 1,\n",
       "         ('transcendent', 'proof'): 1,\n",
       "         ('transcendent', 'confidence'): 1,\n",
       "         ('proof', 'sensibility'): 1,\n",
       "         ('proof', 'transcendent'): 1,\n",
       "         ('proof', 'confidence'): 3,\n",
       "         ('proof', 'fellow'): 4,\n",
       "         ('confidence', 'transcendent'): 1,\n",
       "         ('confidence', 'proof'): 3,\n",
       "         ('confidence', 'fellow'): 4,\n",
       "         ('confidence', 'citizens'): 5,\n",
       "         ('fellow', 'proof'): 4,\n",
       "         ('fellow', 'confidence'): 4,\n",
       "         ('fellow', 'little'): 1,\n",
       "         ('citizens', 'confidence'): 5,\n",
       "         ('citizens', 'little'): 1,\n",
       "         ('citizens', 'consulted'): 1,\n",
       "         ('little', 'fellow'): 1,\n",
       "         ('little', 'citizens'): 1,\n",
       "         ('little', 'consulted'): 1,\n",
       "         ('little', 'incapacity'): 1,\n",
       "         ('consulted', 'citizens'): 1,\n",
       "         ('consulted', 'little'): 1,\n",
       "         ('consulted', 'incapacity'): 1,\n",
       "         ('consulted', 'disinclination'): 1,\n",
       "         ('incapacity', 'little'): 1,\n",
       "         ('incapacity', 'consulted'): 1,\n",
       "         ('incapacity', 'disinclination'): 1,\n",
       "         ('incapacity', 'weighty'): 1,\n",
       "         ('disinclination', 'consulted'): 1,\n",
       "         ('disinclination', 'incapacity'): 1,\n",
       "         ('disinclination', 'weighty'): 1,\n",
       "         ('disinclination', 'untried'): 1,\n",
       "         ('weighty', 'incapacity'): 1,\n",
       "         ('weighty', 'disinclination'): 1,\n",
       "         ('weighty', 'untried'): 1,\n",
       "         ('weighty', 'cares'): 1,\n",
       "         ('untried', 'disinclination'): 1,\n",
       "         ('untried', 'weighty'): 1,\n",
       "         ('untried', 'cares'): 1,\n",
       "         ('untried', 'error'): 1,\n",
       "         ('cares', 'weighty'): 1,\n",
       "         ('cares', 'untried'): 1,\n",
       "         ('cares', 'error'): 1,\n",
       "         ('cares', 'palliated'): 1,\n",
       "         ('error', 'untried'): 1,\n",
       "         ('error', 'cares'): 1,\n",
       "         ('error', 'palliated'): 1,\n",
       "         ('error', 'motives'): 1,\n",
       "         ('palliated', 'cares'): 1,\n",
       "         ('palliated', 'error'): 1,\n",
       "         ('palliated', 'motives'): 1,\n",
       "         ('palliated', 'mislead'): 1,\n",
       "         ('motives', 'error'): 1,\n",
       "         ('motives', 'palliated'): 1,\n",
       "         ('motives', 'mislead'): 1,\n",
       "         ('motives', 'consequences'): 1,\n",
       "         ('mislead', 'palliated'): 1,\n",
       "         ('mislead', 'motives'): 1,\n",
       "         ('mislead', 'consequences'): 1,\n",
       "         ('mislead', 'judged'): 1,\n",
       "         ('consequences', 'motives'): 1,\n",
       "         ('consequences', 'mislead'): 1,\n",
       "         ('consequences', 'judged'): 1,\n",
       "         ('consequences', 'country'): 1,\n",
       "         ('judged', 'mislead'): 1,\n",
       "         ('judged', 'consequences'): 1,\n",
       "         ('judged', 'country'): 1,\n",
       "         ('judged', 'share'): 1,\n",
       "         ('country', 'consequences'): 1,\n",
       "         ('country', 'judged'): 1,\n",
       "         ('country', 'share'): 3,\n",
       "         ('country', 'partiality'): 1,\n",
       "         ('share', 'judged'): 1,\n",
       "         ('share', 'country'): 3,\n",
       "         ('share', 'partiality'): 1,\n",
       "         ('share', 'originated'): 1,\n",
       "         ('partiality', 'country'): 1,\n",
       "         ('partiality', 'share'): 1,\n",
       "         ('partiality', 'originated'): 1,\n",
       "         ('originated', 'share'): 1,\n",
       "         ('originated', 'partiality'): 1,\n",
       "         ('impressions', 'obedience'): 1,\n",
       "         ('impressions', 'public'): 1,\n",
       "         ('obedience', 'impressions'): 1,\n",
       "         ('obedience', 'public'): 1,\n",
       "         ('obedience', 'summons'): 1,\n",
       "         ('public', 'impressions'): 1,\n",
       "         ('public', 'obedience'): 1,\n",
       "         ('public', 'summons'): 1,\n",
       "         ('public', 'repaired'): 1,\n",
       "         ('summons', 'obedience'): 1,\n",
       "         ('summons', 'public'): 1,\n",
       "         ('summons', 'repaired'): 1,\n",
       "         ('summons', 'present'): 1,\n",
       "         ('repaired', 'public'): 1,\n",
       "         ('repaired', 'summons'): 1,\n",
       "         ('repaired', 'present'): 1,\n",
       "         ('repaired', 'station'): 1,\n",
       "         ('present', 'summons'): 1,\n",
       "         ('present', 'repaired'): 1,\n",
       "         ('present', 'station'): 1,\n",
       "         ('present', 'peculiarly'): 1,\n",
       "         ('station', 'repaired'): 1,\n",
       "         ('station', 'present'): 1,\n",
       "         ('station', 'peculiarly'): 1,\n",
       "         ('station', 'improper'): 1,\n",
       "         ('peculiarly', 'present'): 1,\n",
       "         ('peculiarly', 'station'): 1,\n",
       "         ('peculiarly', 'improper'): 1,\n",
       "         ('peculiarly', 'omit'): 1,\n",
       "         ('improper', 'station'): 1,\n",
       "         ('improper', 'peculiarly'): 1,\n",
       "         ('improper', 'omit'): 1,\n",
       "         ('improper', 'official'): 1,\n",
       "         ('omit', 'peculiarly'): 1,\n",
       "         ('omit', 'improper'): 1,\n",
       "         ('omit', 'official'): 1,\n",
       "         ('omit', 'act'): 1,\n",
       "         ('official', 'improper'): 1,\n",
       "         ('official', 'omit'): 1,\n",
       "         ('official', 'act'): 2,\n",
       "         ('official', 'fervent'): 1,\n",
       "         ('act', 'omit'): 1,\n",
       "         ('act', 'official'): 2,\n",
       "         ('act', 'fervent'): 1,\n",
       "         ('act', 'supplications'): 1,\n",
       "         ('fervent', 'official'): 1,\n",
       "         ('fervent', 'act'): 1,\n",
       "         ('fervent', 'supplications'): 3,\n",
       "         ('fervent', 'almighty'): 3,\n",
       "         ('supplications', 'act'): 1,\n",
       "         ('supplications', 'fervent'): 3,\n",
       "         ('supplications', 'almighty'): 1,\n",
       "         ('supplications', 'rules'): 1,\n",
       "         ('almighty', 'fervent'): 3,\n",
       "         ('almighty', 'supplications'): 1,\n",
       "         ('almighty', 'rules'): 1,\n",
       "         ('almighty', 'universe'): 2,\n",
       "         ('rules', 'supplications'): 1,\n",
       "         ('rules', 'almighty'): 1,\n",
       "         ('rules', 'universe'): 2,\n",
       "         ('rules', 'presides'): 1,\n",
       "         ('universe', 'almighty'): 2,\n",
       "         ('universe', 'rules'): 2,\n",
       "         ('universe', 'presides'): 1,\n",
       "         ('universe', 'councils'): 2,\n",
       "         ('presides', 'rules'): 1,\n",
       "         ('presides', 'universe'): 1,\n",
       "         ('presides', 'councils'): 1,\n",
       "         ('presides', 'nations'): 2,\n",
       "         ('councils', 'universe'): 2,\n",
       "         ('councils', 'presides'): 1,\n",
       "         ('councils', 'nations'): 1,\n",
       "         ('councils', 'providential'): 1,\n",
       "         ('nations', 'presides'): 2,\n",
       "         ('nations', 'councils'): 1,\n",
       "         ('nations', 'providential'): 1,\n",
       "         ('nations', 'aids'): 1,\n",
       "         ('providential', 'councils'): 1,\n",
       "         ('providential', 'nations'): 1,\n",
       "         ('providential', 'aids'): 1,\n",
       "         ('providential', 'supply'): 1,\n",
       "         ('aids', 'nations'): 1,\n",
       "         ('aids', 'providential'): 1,\n",
       "         ('aids', 'supply'): 2,\n",
       "         ('aids', 'human'): 1,\n",
       "         ('supply', 'providential'): 1,\n",
       "         ('supply', 'aids'): 2,\n",
       "         ('supply', 'human'): 1,\n",
       "         ('supply', 'defect'): 1,\n",
       "         ('human', 'aids'): 1,\n",
       "         ('human', 'supply'): 1,\n",
       "         ('human', 'defect'): 1,\n",
       "         ('human', 'benediction'): 1,\n",
       "         ('defect', 'supply'): 1,\n",
       "         ('defect', 'human'): 1,\n",
       "         ('defect', 'benediction'): 1,\n",
       "         ('defect', 'consecrate'): 1,\n",
       "         ('benediction', 'human'): 1,\n",
       "         ('benediction', 'defect'): 1,\n",
       "         ('benediction', 'consecrate'): 1,\n",
       "         ('benediction', 'liberties'): 1,\n",
       "         ('consecrate', 'defect'): 1,\n",
       "         ('consecrate', 'benediction'): 1,\n",
       "         ('consecrate', 'liberties'): 1,\n",
       "         ('consecrate', 'happiness'): 1,\n",
       "         ('liberties', 'benediction'): 1,\n",
       "         ('liberties', 'consecrate'): 1,\n",
       "         ('liberties', 'happiness'): 1,\n",
       "         ('liberties', 'people'): 4,\n",
       "         ('happiness', 'consecrate'): 1,\n",
       "         ('happiness', 'liberties'): 1,\n",
       "         ('happiness', 'people'): 5,\n",
       "         ('happiness', 'united'): 1,\n",
       "         ('people', 'liberties'): 4,\n",
       "         ('people', 'happiness'): 5,\n",
       "         ('people', 'united'): 19,\n",
       "         ('people', 'states'): 29,\n",
       "         ('united', 'happiness'): 1,\n",
       "         ('united', 'people'): 19,\n",
       "         ('united', 'states'): 164,\n",
       "         ('united', 'government'): 14,\n",
       "         ('states', 'people'): 29,\n",
       "         ('states', 'united'): 164,\n",
       "         ('states', 'government'): 23,\n",
       "         ('states', 'instituted'): 1,\n",
       "         ('government', 'united'): 14,\n",
       "         ('government', 'states'): 23,\n",
       "         ('government', 'instituted'): 4,\n",
       "         ('government', 'essential'): 3,\n",
       "         ('instituted', 'states'): 1,\n",
       "         ('instituted', 'government'): 4,\n",
       "         ('instituted', 'essential'): 1,\n",
       "         ('instituted', 'purposes'): 1,\n",
       "         ('essential', 'government'): 3,\n",
       "         ('essential', 'instituted'): 1,\n",
       "         ('essential', 'purposes'): 1,\n",
       "         ('essential', 'enable'): 1,\n",
       "         ('purposes', 'instituted'): 1,\n",
       "         ('purposes', 'essential'): 1,\n",
       "         ('purposes', 'enable'): 1,\n",
       "         ('purposes', 'instrument'): 2,\n",
       "         ('enable', 'essential'): 1,\n",
       "         ('enable', 'purposes'): 1,\n",
       "         ('enable', 'instrument'): 1,\n",
       "         ('enable', 'employed'): 1,\n",
       "         ('instrument', 'purposes'): 2,\n",
       "         ('instrument', 'enable'): 1,\n",
       "         ('instrument', 'employed'): 1,\n",
       "         ('instrument', 'administration'): 1,\n",
       "         ('employed', 'enable'): 1,\n",
       "         ('employed', 'instrument'): 1,\n",
       "         ('employed', 'administration'): 1,\n",
       "         ('employed', 'execute'): 1,\n",
       "         ('administration', 'instrument'): 1,\n",
       "         ('administration', 'employed'): 1,\n",
       "         ('administration', 'execute'): 1,\n",
       "         ('administration', 'success'): 1,\n",
       "         ('execute', 'employed'): 1,\n",
       "         ('execute', 'administration'): 1,\n",
       "         ('execute', 'success'): 1,\n",
       "         ('execute', 'functions'): 2,\n",
       "         ('success', 'administration'): 1,\n",
       "         ('success', 'execute'): 1,\n",
       "         ('success', 'functions'): 1,\n",
       "         ('success', 'allotted'): 1,\n",
       "         ('functions', 'execute'): 2,\n",
       "         ('functions', 'success'): 1,\n",
       "         ('functions', 'allotted'): 1,\n",
       "         ('functions', 'charge'): 1,\n",
       "         ('allotted', 'success'): 1,\n",
       "         ('allotted', 'functions'): 1,\n",
       "         ('allotted', 'charge'): 1,\n",
       "         ('charge', 'functions'): 1,\n",
       "         ('charge', 'allotted'): 1,\n",
       "         ('tendering', 'homage'): 1,\n",
       "         ('tendering', 'great'): 1,\n",
       "         ('homage', 'tendering'): 1,\n",
       "         ('homage', 'great'): 1,\n",
       "         ('homage', 'author'): 1,\n",
       "         ('great', 'tendering'): 1,\n",
       "         ('great', 'homage'): 1,\n",
       "         ('great', 'author'): 1,\n",
       "         ('great', 'public'): 5,\n",
       "         ('author', 'homage'): 1,\n",
       "         ('author', 'great'): 1,\n",
       "         ('author', 'public'): 1,\n",
       "         ('author', 'private'): 1,\n",
       "         ('public', 'great'): 5,\n",
       "         ('public', 'author'): 1,\n",
       "         ('public', 'private'): 7,\n",
       "         ('public', 'good'): 6,\n",
       "         ('private', 'author'): 1,\n",
       "         ('private', 'public'): 7,\n",
       "         ('private', 'good'): 1,\n",
       "         ('private', 'assure'): 1,\n",
       "         ('good', 'public'): 6,\n",
       "         ('good', 'private'): 1,\n",
       "         ('good', 'assure'): 2,\n",
       "         ('good', 'expresses'): 1,\n",
       "         ('assure', 'private'): 1,\n",
       "         ('assure', 'good'): 2,\n",
       "         ('assure', 'expresses'): 1,\n",
       "         ('assure', 'sentiments'): 1,\n",
       "         ('expresses', 'good'): 1,\n",
       "         ('expresses', 'assure'): 1,\n",
       "         ('expresses', 'sentiments'): 1,\n",
       "         ('expresses', 'fellow'): 1,\n",
       "         ('sentiments', 'assure'): 1,\n",
       "         ('sentiments', 'expresses'): 1,\n",
       "         ('sentiments', 'fellow'): 1,\n",
       "         ('sentiments', 'citizens'): 1,\n",
       "         ('fellow', 'expresses'): 1,\n",
       "         ('fellow', 'sentiments'): 1,\n",
       "         ('fellow', 'large'): 3,\n",
       "         ('citizens', 'sentiments'): 1,\n",
       "         ('citizens', 'large'): 5,\n",
       "         ('large', 'fellow'): 3,\n",
       "         ('large', 'citizens'): 5,\n",
       "         ('people', 'bound'): 2,\n",
       "         ('people', 'acknowledge'): 1,\n",
       "         ('bound', 'people'): 2,\n",
       "         ('bound', 'acknowledge'): 1,\n",
       "         ('bound', 'adore'): 1,\n",
       "         ('acknowledge', 'people'): 1,\n",
       "         ('acknowledge', 'bound'): 1,\n",
       "         ('acknowledge', 'adore'): 1,\n",
       "         ('acknowledge', 'invisible'): 1,\n",
       "         ('adore', 'bound'): 1,\n",
       "         ('adore', 'acknowledge'): 1,\n",
       "         ('adore', 'invisible'): 1,\n",
       "         ('adore', 'hand'): 1,\n",
       "         ('invisible', 'acknowledge'): 1,\n",
       "         ('invisible', 'adore'): 1,\n",
       "         ('invisible', 'hand'): 1,\n",
       "         ('invisible', 'conducts'): 1,\n",
       "         ('hand', 'adore'): 1,\n",
       "         ('hand', 'invisible'): 1,\n",
       "         ('hand', 'conducts'): 1,\n",
       "         ('hand', 'affairs'): 1,\n",
       "         ('conducts', 'invisible'): 1,\n",
       "         ('conducts', 'hand'): 1,\n",
       "         ('conducts', 'affairs'): 1,\n",
       "         ('conducts', 'men'): 1,\n",
       "         ('affairs', 'hand'): 1,\n",
       "         ('affairs', 'conducts'): 1,\n",
       "         ('affairs', 'men'): 2,\n",
       "         ('affairs', 'united'): 1,\n",
       "         ('men', 'conducts'): 1,\n",
       "         ('men', 'affairs'): 2,\n",
       "         ('men', 'united'): 2,\n",
       "         ('men', 'states'): 2,\n",
       "         ('united', 'affairs'): 1,\n",
       "         ('united', 'men'): 2,\n",
       "         ('states', 'men'): 2,\n",
       "         ('step', 'advanced'): 2,\n",
       "         ('step', 'character'): 1,\n",
       "         ('advanced', 'step'): 2,\n",
       "         ('advanced', 'character'): 1,\n",
       "         ('advanced', 'independent'): 1,\n",
       "         ('character', 'step'): 1,\n",
       "         ('character', 'advanced'): 1,\n",
       "         ('character', 'independent'): 1,\n",
       "         ('character', 'nation'): 2,\n",
       "         ('independent', 'advanced'): 1,\n",
       "         ('independent', 'character'): 1,\n",
       "         ('independent', 'nation'): 3,\n",
       "         ('independent', 'distinguished'): 1,\n",
       "         ('nation', 'character'): 2,\n",
       "         ('nation', 'independent'): 3,\n",
       "         ('nation', 'distinguished'): 1,\n",
       "         ('nation', 'token'): 1,\n",
       "         ('distinguished', 'independent'): 1,\n",
       "         ('distinguished', 'nation'): 1,\n",
       "         ('distinguished', 'token'): 1,\n",
       "         ('distinguished', 'providential'): 1,\n",
       "         ('token', 'nation'): 1,\n",
       "         ('token', 'distinguished'): 1,\n",
       "         ('token', 'providential'): 1,\n",
       "         ('token', 'agency'): 1,\n",
       "         ('providential', 'distinguished'): 1,\n",
       "         ('providential', 'token'): 1,\n",
       "         ('providential', 'agency'): 1,\n",
       "         ('providential', 'important'): 1,\n",
       "         ('agency', 'token'): 1,\n",
       "         ('agency', 'providential'): 1,\n",
       "         ('agency', 'important'): 1,\n",
       "         ('agency', 'revolution'): 1,\n",
       "         ('important', 'providential'): 1,\n",
       "         ('important', 'agency'): 1,\n",
       "         ('important', 'revolution'): 1,\n",
       "         ('important', 'accomplished'): 1,\n",
       "         ('revolution', 'agency'): 1,\n",
       "         ('revolution', 'important'): 1,\n",
       "         ('revolution', 'accomplished'): 1,\n",
       "         ('revolution', 'system'): 1,\n",
       "         ('accomplished', 'important'): 1,\n",
       "         ('accomplished', 'revolution'): 1,\n",
       "         ('accomplished', 'system'): 1,\n",
       "         ('accomplished', 'united'): 1,\n",
       "         ('system', 'revolution'): 1,\n",
       "         ('system', 'accomplished'): 1,\n",
       "         ('system', 'united'): 2,\n",
       "         ('system', 'government'): 11,\n",
       "         ('united', 'accomplished'): 1,\n",
       "         ('united', 'system'): 2,\n",
       "         ('united', 'tranquil'): 1,\n",
       "         ('government', 'system'): 11,\n",
       "         ('government', 'tranquil'): 1,\n",
       "         ('government', 'deliberations'): 1,\n",
       "         ('tranquil', 'united'): 1,\n",
       "         ('tranquil', 'government'): 1,\n",
       "         ('tranquil', 'deliberations'): 1,\n",
       "         ('tranquil', 'voluntary'): 1,\n",
       "         ('deliberations', 'government'): 1,\n",
       "         ('deliberations', 'tranquil'): 1,\n",
       "         ('deliberations', 'voluntary'): 1,\n",
       "         ('deliberations', 'consent'): 1,\n",
       "         ('voluntary', 'tranquil'): 1,\n",
       "         ('voluntary', 'deliberations'): 1,\n",
       "         ('voluntary', 'consent'): 1,\n",
       "         ('voluntary', 'distinct'): 1,\n",
       "         ('consent', 'deliberations'): 1,\n",
       "         ('consent', 'voluntary'): 1,\n",
       "         ('consent', 'distinct'): 1,\n",
       "         ('consent', 'communities'): 1,\n",
       "         ('distinct', 'voluntary'): 1,\n",
       "         ('distinct', 'consent'): 1,\n",
       "         ('distinct', 'communities'): 2,\n",
       "         ('distinct', 'event'): 1,\n",
       "         ('communities', 'consent'): 1,\n",
       "         ('communities', 'distinct'): 2,\n",
       "         ('communities', 'event'): 1,\n",
       "         ('communities', 'resulted'): 1,\n",
       "         ('event', 'distinct'): 1,\n",
       "         ('event', 'communities'): 1,\n",
       "         ('event', 'resulted'): 1,\n",
       "         ('event', 'compared'): 1,\n",
       "         ('resulted', 'communities'): 1,\n",
       "         ('resulted', 'event'): 1,\n",
       "         ('resulted', 'compared'): 1,\n",
       "         ('resulted', 'means'): 1,\n",
       "         ('compared', 'event'): 1,\n",
       "         ('compared', 'resulted'): 1,\n",
       "         ('compared', 'means'): 1,\n",
       "         ('compared', 'governments'): 1,\n",
       "         ('means', 'resulted'): 1,\n",
       "         ('means', 'compared'): 1,\n",
       "         ('means', 'governments'): 1,\n",
       "         ('means', 'established'): 1,\n",
       "         ('governments', 'compared'): 1,\n",
       "         ('governments', 'means'): 1,\n",
       "         ('governments', 'established'): 2,\n",
       "         ('governments', 'return'): 1,\n",
       "         ('established', 'means'): 1,\n",
       "         ('established', 'governments'): 2,\n",
       "         ('established', 'return'): 1,\n",
       "         ('established', 'pious'): 1,\n",
       "         ('return', 'governments'): 1,\n",
       "         ('return', 'established'): 1,\n",
       "         ('return', 'pious'): 1,\n",
       "         ('return', 'gratitude'): 2,\n",
       "         ('pious', 'established'): 1,\n",
       "         ('pious', 'return'): 1,\n",
       "         ('pious', 'gratitude'): 1,\n",
       "         ('pious', 'humble'): 1,\n",
       "         ('gratitude', 'return'): 2,\n",
       "         ('gratitude', 'pious'): 1,\n",
       "         ('gratitude', 'humble'): 2,\n",
       "         ('gratitude', 'anticipation'): 1,\n",
       "         ('humble', 'pious'): 1,\n",
       "         ('humble', 'gratitude'): 2,\n",
       "         ('humble', 'anticipation'): 1,\n",
       "         ('humble', 'future'): 1,\n",
       "         ('anticipation', 'gratitude'): 1,\n",
       "         ('anticipation', 'humble'): 1,\n",
       "         ('anticipation', 'future'): 1,\n",
       "         ('anticipation', 'blessings'): 1,\n",
       "         ('future', 'humble'): 1,\n",
       "         ('future', 'anticipation'): 1,\n",
       "         ('future', 'blessings'): 1,\n",
       "         ('future', 'past'): 5,\n",
       "         ('blessings', 'anticipation'): 1,\n",
       "         ('blessings', 'future'): 1,\n",
       "         ('blessings', 'past'): 1,\n",
       "         ('blessings', 'presage'): 1,\n",
       "         ('past', 'future'): 5,\n",
       "         ('past', 'blessings'): 1,\n",
       "         ('past', 'presage'): 1,\n",
       "         ('presage', 'blessings'): 1,\n",
       "         ('presage', 'past'): 1,\n",
       "         ('reflections', 'arising'): 1,\n",
       "         ('reflections', 'present'): 1,\n",
       "         ('arising', 'reflections'): 1,\n",
       "         ('arising', 'present'): 1,\n",
       "         ('arising', 'crisis'): 1,\n",
       "         ('present', 'reflections'): 1,\n",
       "         ('present', 'arising'): 1,\n",
       "         ('present', 'crisis'): 2,\n",
       "         ('present', 'forced'): 1,\n",
       "         ('crisis', 'arising'): 1,\n",
       "         ('crisis', 'present'): 2,\n",
       "         ('crisis', 'forced'): 1,\n",
       "         ('crisis', 'strongly'): 1,\n",
       "         ('forced', 'present'): 1,\n",
       "         ('forced', 'crisis'): 1,\n",
       "         ('forced', 'strongly'): 1,\n",
       "         ('forced', 'mind'): 1,\n",
       "         ('strongly', 'crisis'): 1,\n",
       "         ('strongly', 'forced'): 1,\n",
       "         ('strongly', 'mind'): 1,\n",
       "         ('strongly', 'suppressed'): 1,\n",
       "         ('mind', 'forced'): 1,\n",
       "         ('mind', 'strongly'): 1,\n",
       "         ('mind', 'suppressed'): 1,\n",
       "         ('suppressed', 'strongly'): 1,\n",
       "         ('suppressed', 'mind'): 1,\n",
       "         ('join', 'trust'): 1,\n",
       "         ('join', 'thinking'): 1,\n",
       "         ('trust', 'join'): 1,\n",
       "         ('trust', 'thinking'): 1,\n",
       "         ('trust', 'influence'): 1,\n",
       "         ('thinking', 'join'): 1,\n",
       "         ('thinking', 'trust'): 1,\n",
       "         ('thinking', 'influence'): 1,\n",
       "         ('thinking', 'proceedings'): 1,\n",
       "         ('influence', 'trust'): 1,\n",
       "         ('influence', 'thinking'): 1,\n",
       "         ('influence', 'proceedings'): 1,\n",
       "         ('influence', 'new'): 3,\n",
       "         ('proceedings', 'thinking'): 1,\n",
       "         ('proceedings', 'influence'): 1,\n",
       "         ('proceedings', 'new'): 1,\n",
       "         ('proceedings', 'free'): 1,\n",
       "         ('new', 'influence'): 3,\n",
       "         ('new', 'proceedings'): 1,\n",
       "         ('new', 'free'): 2,\n",
       "         ('new', 'government'): 15,\n",
       "         ('free', 'proceedings'): 1,\n",
       "         ('free', 'new'): 2,\n",
       "         ('free', 'government'): 16,\n",
       "         ('free', 'auspiciously'): 1,\n",
       "         ('government', 'new'): 15,\n",
       "         ('government', 'free'): 16,\n",
       "         ('government', 'auspiciously'): 1,\n",
       "         ('government', 'commence'): 2,\n",
       "         ('auspiciously', 'free'): 1,\n",
       "         ('auspiciously', 'government'): 1,\n",
       "         ('auspiciously', 'commence'): 1,\n",
       "         ('commence', 'government'): 2,\n",
       "         ('commence', 'auspiciously'): 1,\n",
       "         ('article', 'establishing'): 1,\n",
       "         ('article', 'executive'): 1,\n",
       "         ('establishing', 'article'): 1,\n",
       "         ('establishing', 'executive'): 1,\n",
       "         ('establishing', 'department'): 1,\n",
       "         ('executive', 'article'): 1,\n",
       "         ('executive', 'establishing'): 1,\n",
       "         ('executive', 'department'): 8,\n",
       "         ('executive', 'duty'): 3,\n",
       "         ('department', 'establishing'): 1,\n",
       "         ('department', 'executive'): 8,\n",
       "         ('department', 'duty'): 1,\n",
       "         ('department', 'president'): 1,\n",
       "         ('duty', 'executive'): 3,\n",
       "         ('duty', 'department'): 1,\n",
       "         ('duty', 'president'): 5,\n",
       "         ('duty', 'recommend'): 4,\n",
       "         ('president', 'department'): 1,\n",
       "         ('president', 'duty'): 5,\n",
       "         ('president', 'recommend'): 1,\n",
       "         ('president', 'consideration'): 1,\n",
       "         ('recommend', 'duty'): 4,\n",
       "         ('recommend', 'president'): 1,\n",
       "         ('recommend', 'consideration'): 1,\n",
       "         ('recommend', 'measures'): 5,\n",
       "         ('consideration', 'president'): 1,\n",
       "         ('consideration', 'recommend'): 1,\n",
       "         ('consideration', 'measures'): 1,\n",
       "         ('consideration', 'shall'): 3,\n",
       "         ('measures', 'recommend'): 5,\n",
       "         ('measures', 'consideration'): 1,\n",
       "         ('measures', 'shall'): 4,\n",
       "         ('measures', 'judge'): 2,\n",
       "         ('shall', 'consideration'): 3,\n",
       "         ('shall', 'measures'): 4,\n",
       "         ('shall', 'judge'): 2,\n",
       "         ('shall', 'necessary'): 2,\n",
       "         ('judge', 'measures'): 2,\n",
       "         ('judge', 'shall'): 2,\n",
       "         ('judge', 'necessary'): 2,\n",
       "         ('judge', 'expedient'): 1,\n",
       "         ('necessary', 'shall'): 2,\n",
       "         ('necessary', 'judge'): 2,\n",
       "         ('necessary', 'expedient'): 2,\n",
       "         ('necessary', '.\"'): 1,\n",
       "         ('expedient', 'judge'): 1,\n",
       "         ('expedient', 'necessary'): 2,\n",
       "         ('expedient', '.\"'): 1,\n",
       "         ('.\"', 'necessary'): 1,\n",
       "         ('.\"', 'expedient'): 1,\n",
       "         ('circumstances', 'meet'): 1,\n",
       "         ('circumstances', 'acquit'): 1,\n",
       "         ('meet', 'circumstances'): 1,\n",
       "         ('meet', 'acquit'): 1,\n",
       "         ('meet', 'entering'): 1,\n",
       "         ('acquit', 'circumstances'): 1,\n",
       "         ('acquit', 'meet'): 1,\n",
       "         ('acquit', 'entering'): 1,\n",
       "         ('acquit', 'subject'): 1,\n",
       "         ('entering', 'meet'): 1,\n",
       "         ('entering', 'acquit'): 1,\n",
       "         ('entering', 'subject'): 1,\n",
       "         ('entering', 'refer'): 1,\n",
       "         ('subject', 'acquit'): 1,\n",
       "         ('subject', 'entering'): 1,\n",
       "         ('subject', 'refer'): 1,\n",
       "         ('subject', 'great'): 2,\n",
       "         ('refer', 'entering'): 1,\n",
       "         ('refer', 'subject'): 1,\n",
       "         ('refer', 'great'): 1,\n",
       "         ('refer', 'constitutional'): 1,\n",
       "         ('great', 'subject'): 2,\n",
       "         ('great', 'refer'): 1,\n",
       "         ('great', 'constitutional'): 3,\n",
       "         ('great', 'charter'): 1,\n",
       "         ('constitutional', 'refer'): 1,\n",
       "         ('constitutional', 'great'): 3,\n",
       "         ('constitutional', 'charter'): 1,\n",
       "         ('constitutional', 'assembled'): 1,\n",
       "         ('charter', 'great'): 1,\n",
       "         ('charter', 'constitutional'): 1,\n",
       "         ('charter', 'assembled'): 1,\n",
       "         ('charter', 'defining'): 1,\n",
       "         ('assembled', 'constitutional'): 1,\n",
       "         ('assembled', 'charter'): 1,\n",
       "         ('assembled', 'defining'): 1,\n",
       "         ('assembled', 'powers'): 1,\n",
       "         ('defining', 'charter'): 1,\n",
       "         ('defining', 'assembled'): 1,\n",
       "         ('defining', 'powers'): 1,\n",
       "         ...})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count co-occurences in the skipgrams\n",
    "X_ik_skipgram = Counter(skip_grams)\n",
    "\n",
    "X_ik_skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(X_ik_skipgram[('fellow', 'citizens')])\n",
    "print(X_ik_skipgram[('fellow', 'communists')])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Weighting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the weighting function\n",
    "def weighting(w_i, w_j, X_ik):\n",
    "    \n",
    "    x_ij = X_ik[(w_i, w_j)]\n",
    "\n",
    "    #label smoothing if there is no co-occurence (i.e., x_ij is 0)\n",
    "    if x_ij == 0:\n",
    "        x_ij = 1\n",
    "        \n",
    "    #maximum co-occurrences is 100 according to the paper\n",
    "    x_max = 100\n",
    "    alpha = 0.75\n",
    "    \n",
    "    #if the co-occurrences does not exceed x_max, scale it down based on some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij/x_max) ** alpha\n",
    "    else:\n",
    "        result = 1 #the maximum probability\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#test the weighting function\n",
    "w_i  = 'fellow'\n",
    "w_j  = 'citizens'\n",
    "\n",
    "print(weighting(w_i, w_j, X_ik_skipgram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03162277660168379\n",
      "0.03162277660168379\n"
     ]
    }
   ],
   "source": [
    "w_i  = 'fellow'\n",
    "w_j  = 'communists'\n",
    "\n",
    "print(weighting(w_i, w_j, X_ik_skipgram))\n",
    "print((1 / 100) ** 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply this weighting to all possible pairs\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {} #for keeping the co-occurrences\n",
    "weighting_dic = {} #for keeping all the probabilities after passing through the weighting function\n",
    "\n",
    "for bigram in combinations_with_replacement(vocabs, 2):  #we need to also think its reverse\n",
    "    #if this bigram exists in X_ik_skipgrams\n",
    "    #we gonna add this to our co-occurence matrix\n",
    "    if X_ik_skipgram.get(bigram) is not None:\n",
    "        cooc = X_ik_skipgram[bigram]  #get the co-occurrence\n",
    "        X_ik[bigram] = cooc + 1 #this is again basically label smoothing\n",
    "        X_ik[(bigram[1], bigram[0])] = cooc + 1  #trick to get all pairs\n",
    "    else: #otherwise, put 0\n",
    "        X_ik[bigram] = 0\n",
    "        X_ik[(bigram[1], bigram[0])] = 0\n",
    "\n",
    "    #apply the weighting function using this co-occurrence matrix thingy    \n",
    "    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "0.10573712634405642\n",
      "0.10573712634405642\n",
      "0.10573712634405642\n"
     ]
    }
   ],
   "source": [
    "#test the weighting function\n",
    "print(X_ik_skipgram[('senate', 'house')])\n",
    "print(X_ik_skipgram[('house', 'senate')])\n",
    "\n",
    "print(X_ik[('senate', 'house')])\n",
    "print(X_ik[('house', 'senate')])\n",
    "\n",
    "print(weighting_dic[('senate', 'house')])\n",
    "print(weighting_dic[('house', 'senate')])\n",
    "\n",
    "print((5 / 100) ** 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0.03162277660168379\n",
      "0.03162277660168379\n"
     ]
    }
   ],
   "source": [
    "print(X_ik_skipgram[('communists', 'communists')])\n",
    "print(X_ik[('communists', 'communists')])\n",
    "print(weighting_dic[('communists', 'communists')])\n",
    "print((1 / 100) ** 0.75)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random batch for GloVe with generic batch size, corpus and skipgrams\n",
    "import math\n",
    "\n",
    "def random_batch_glove(batch_size, corpus, skip_grams, X_ik, weighting_dic):\n",
    "    \n",
    "    #change words in the skipgrams to idices\n",
    "    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    \n",
    "    #randomly pick \"batch_size\" indices\n",
    "    number_of_choices = len(skip_grams_id)\n",
    "    random_index = np.random.choice(number_of_choices, batch_size, replace=False) #no repeating indexes among these random indexes\n",
    "    \n",
    "    random_inputs = [] #xi, wi (in batches)\n",
    "    random_labels = [] #xj, wj (in batches)\n",
    "    random_coocs  = [] #Xij (in batches)\n",
    "    random_weightings = [] #weighting_dic(Xij) (in batches)\n",
    "    #for each of the sample in these indexes\n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams_id[i][0]])\n",
    "        random_labels.append([skip_grams_id[i][1]])\n",
    "        \n",
    "        #get cooc\n",
    "        #first check whether it exists...\n",
    "        pair = skip_grams[i]\n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1 #label smoothing\n",
    "            \n",
    "        random_coocs.append([math.log(cooc)])\n",
    "        #log according to the cost function equation\n",
    "        #bracket because neural network requires size ( , 1)\n",
    "        \n",
    "        #get weighting\n",
    "        weighting = weighting_dic[pair]\n",
    "        random_weightings.append(weighting)\n",
    "\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  56],\n",
       "        [7525]]),\n",
       " array([[6625],\n",
       "        [ 345]]),\n",
       " array([[0.69314718],\n",
       "        [0.69314718]]),\n",
       " array([0.05318296, 0.05318296]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the method\n",
    "batch_size = 2\n",
    "inputs, targets, coocs, weightings = random_batch_glove(batch_size, corpus_tokenized, skip_grams, X_ik, weighting_dic)\n",
    "\n",
    "inputs, targets, coocs, weightings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size,embed_size):\n",
    "        super(GloVe,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, embed_size) # center embedding\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size) # context embedding\n",
    "        \n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "        \n",
    "    def forward(self, center_words, context_words, coocs, weightings):\n",
    "        center_embeds = self.embedding_v(center_words) # [batch_size, 1, emb_size]\n",
    "        context_embeds = self.embedding_u(context_words) # [batch_size, 1, emb_size]\n",
    "        \n",
    "        center_bias = self.v_bias(center_words).squeeze(1)\n",
    "        context_bias = self.u_bias(context_words).squeeze(1)\n",
    "        \n",
    "        inner_product = context_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "        \n",
    "        #note that coocs is already log\n",
    "        loss = weightings * torch.pow(inner_product + center_bias + context_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set training parameters\n",
    "batch_size  = 10\n",
    "vocab_size  = len(vocabs)\n",
    "emb_size    = 50\n",
    "model       = GloVe(vocab_size, emb_size)\n",
    "optimizer   = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate epoch time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time // 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | cost: 169.326706 | time: 0m 12s\n",
      "Epoch: 200 | cost: 438.742828 | time: 0m 23s\n",
      "Epoch: 300 | cost: 397.676025 | time: 0m 32s\n",
      "Epoch: 400 | cost: 244.789230 | time: 0m 41s\n",
      "Epoch: 500 | cost: 277.003906 | time: 0m 50s\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "import time\n",
    "\n",
    "num_epochs = 500\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    inputs, targets, coocs, weightings = random_batch_glove(batch_size, corpus_tokenized, skip_grams, X_ik, weighting_dic)\n",
    "    input_batch     = torch.LongTensor(inputs)\n",
    "    target_batch    = torch.LongTensor(targets)\n",
    "    cooc_batch      = torch.FloatTensor(coocs)\n",
    "    weighting_batch = torch.FloatTensor(weightings)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(input_batch, target_batch, cooc_batch, weighting_batch)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        end = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6. Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the GloVe model with pickle\n",
    "import pickle\n",
    "\n",
    "pickle.dump(model, open('GloVe.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random batch for CBOW model with default window size and batch size of 1 each\n",
    "def random_batch_cbow(window_size=1, batch_size=1): \n",
    "    cbow = []\n",
    "    for sentence in corpus_tokenized:\n",
    "        #I include the first and last words\n",
    "        #so that every word could be a center word\n",
    "        for i in range(len(sentence)):\n",
    "            center = word2index[sentence[i]]\n",
    "            context = []\n",
    "            context_idx = []\n",
    "            #group the indices of the context words\n",
    "            for j in range(window_size):\n",
    "                context_idx.append(i - window_size + j)\n",
    "            for k in range(1, window_size + 1):\n",
    "                context_idx.append(i + k)\n",
    "            #append the context words based on their indices\n",
    "            #append <UNK> if there is no word at an index\n",
    "            for idx in context_idx:\n",
    "                if idx < 0:\n",
    "                    context.append(word2index['<UNK>'])\n",
    "                elif idx >= len(sentence):\n",
    "                    context.append(word2index['<UNK>'])\n",
    "                else:\n",
    "                    context.append(word2index[sentence[idx]])\n",
    "            cbow.append([context, center])\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(cbow)), batch_size, replace=False)\n",
    "    for i in random_index:\n",
    "        random_inputs.append(cbow[i][0])\n",
    "        random_labels.append([cbow[i][1]])\n",
    "            \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [[8367 5438 4505 1839]\n",
      " [3984 3826 2334 8096]\n",
      " [9019 9019 4128 4089]\n",
      " [9019 9019 1185 1228]\n",
      " [5017 2953 8987 1042]\n",
      " [2184 2026   11 8636]\n",
      " [9019 3391 7083 2157]\n",
      " [6373 4975 9019 9019]\n",
      " [2667 1318 8653 9019]\n",
      " [8052 3949 2053 7294]]\n",
      "Target:  [[4792]\n",
      " [6238]\n",
      " [3387]\n",
      " [5745]\n",
      " [4788]\n",
      " [8092]\n",
      " [ 135]\n",
      " [2481]\n",
      " [1837]\n",
      " [7294]]\n"
     ]
    }
   ],
   "source": [
    "#test the CBOW method\n",
    "input_batch, target_batch = random_batch_cbow(2, 10)\n",
    "\n",
    "print(\"Input: \", input_batch)\n",
    "print(\"Target: \", target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CBOW model\n",
    "class CBOW(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(CBOW,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, emb_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, emb_size)\n",
    "    \n",
    "    def forward(self, context_words, center_words, all_vocabs):\n",
    "        center_embeds = self.embedding_v(center_words)  #[batch_size, 1, emb_size]\n",
    "        context_embeds = self.embedding_u(context_words)  #[batch_size, window_size, emb_size]\n",
    "        all_embeds    = self.embedding_v(all_vocabs)    #[batch_size, vocab_size, emb_size]\n",
    "        \n",
    "        scores      = center_embeds.bmm(context_embeds.transpose(1, 2)).squeeze(2)\n",
    "\n",
    "        norm_scores = all_embeds.bmm(context_embeds.transpose(1, 2)).squeeze(2)\n",
    "\n",
    "        nll = -torch.mean(torch.log(torch.exp(scores)/torch.sum(torch.exp(norm_scores), 1).unsqueeze(1)))\n",
    "            \n",
    "        return nll # negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | cost: 25.349369 | time: 0m 36s\n",
      "Epoch: 200 | cost: 23.935705 | time: 1m 12s\n",
      "Epoch: 300 | cost: 25.939209 | time: 1m 47s\n",
      "Epoch: 400 | cost: 24.941343 | time: 15m 19s\n",
      "Epoch: 500 | cost: 22.499769 | time: 16m 2s\n"
     ]
    }
   ],
   "source": [
    "#set training parameters\n",
    "window_size = 2\n",
    "batch_size = 10\n",
    "vocab_size = len(vocabs)\n",
    "emb_size = 50\n",
    "model = CBOW(vocab_size, emb_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#convert all vocabs to tensors\n",
    "def prepare_sequence(vocabs, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], vocabs))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, vocab_size)\n",
    "\n",
    "#calculate epoch time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time // 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "#train the model\n",
    "import time\n",
    "\n",
    "num_epochs = 500\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    input_batch, target_batch = random_batch_cbow(window_size, batch_size)\n",
    "    input_batch  = torch.LongTensor(input_batch)  #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch) #[batch_size, 1]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(input_batch, target_batch, all_vocabs)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        end = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the CBOW model\n",
    "pickle.dump(model, open('CBOW.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random batch for skip-gram model with default window size and batch size of 1 each\n",
    "def random_batch_skip_gram(window_size=1, batch_size=1): \n",
    "    skip_grams = []\n",
    "    for sentence in corpus_tokenized:\n",
    "        #I include the first and last words\n",
    "        #so that every word could be a center word\n",
    "        for i in range(len(sentence)):\n",
    "            center = word2index[sentence[i]]\n",
    "            context = []\n",
    "            for j in range(window_size):\n",
    "                if (i - window_size + j) >= 0:\n",
    "                    context.append(word2index[sentence[i - window_size + j]])\n",
    "            for k in range(1, window_size + 1):\n",
    "                if (i + k) < len(sentence):\n",
    "                    context.append(word2index[sentence[i + k]])\n",
    "            for w in context:\n",
    "                skip_grams.append([center, w])\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False)\n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams[i][0]])\n",
    "        random_labels.append([skip_grams[i][1]])\n",
    "            \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [[3455]\n",
      " [2551]\n",
      " [4094]\n",
      " [7439]\n",
      " [6469]\n",
      " [6625]\n",
      " [3436]\n",
      " [6905]\n",
      " [1234]\n",
      " [5449]]\n",
      "Target:  [[4894]\n",
      " [7053]\n",
      " [2895]\n",
      " [7617]\n",
      " [4584]\n",
      " [6814]\n",
      " [6491]\n",
      " [8896]\n",
      " [1349]\n",
      " [8217]]\n"
     ]
    }
   ],
   "source": [
    "#test the skip-gram method\n",
    "input_batch, target_batch = random_batch_skip_gram(2, 10)\n",
    "\n",
    "print(\"Input: \", input_batch)\n",
    "print(\"Target: \", target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip-gram model\n",
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(Skipgram,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, emb_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, emb_size)\n",
    "    \n",
    "    def forward(self, center_words, context_words, all_vocabs):\n",
    "        center_embeds = self.embedding_v(center_words)  #[batch_size, 1, emb_size]\n",
    "        context_embeds = self.embedding_u(context_words)  #[batch_size, 1, emb_size]\n",
    "        all_embeds    = self.embedding_u(all_vocabs)    #[batch_size, vocab_size, emb_size]\n",
    "        \n",
    "        scores      = context_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "\n",
    "        norm_scores = all_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, vocab_size, emb_size] @ [batch_size, emb_size, 1] = [batch_size, vocab_size, 1] = [batch_size, vocab_size]\n",
    "\n",
    "        nll = -torch.mean(torch.log(torch.exp(scores)/torch.sum(torch.exp(norm_scores), 1).unsqueeze(1))) # log-softmax\n",
    "        # scalar (loss must be scalar)    \n",
    "            \n",
    "        return nll # negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | cost: 29.787497 | time: 0m 49s\n",
      "Epoch: 200 | cost: 26.413868 | time: 1m 42s\n",
      "Epoch: 300 | cost: 22.013834 | time: 2m 30s\n",
      "Epoch: 400 | cost: 27.883722 | time: 3m 19s\n",
      "Epoch: 500 | cost: 24.870415 | time: 4m 10s\n"
     ]
    }
   ],
   "source": [
    "#set training parameters\n",
    "window_size = 2\n",
    "batch_size = 10\n",
    "vocab_size = len(vocabs)\n",
    "emb_size = 50\n",
    "model = Skipgram(vocab_size, emb_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#convert all vocabs to tensors\n",
    "def prepare_sequence(vocabs, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], vocabs))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, vocab_size)\n",
    "\n",
    "#calculate epoch time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time // 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "#train the model\n",
    "import time\n",
    "\n",
    "num_epochs = 500\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    input_batch, target_batch = random_batch_skip_gram(window_size, batch_size)\n",
    "    input_batch  = torch.LongTensor(input_batch)  #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch) #[batch_size, 1]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(input_batch, target_batch, all_vocabs)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        end = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the skip-gram model\n",
    "pickle.dump(model, open('Skipgram.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Skip-gram with Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count all the occurrences of vocabs\n",
    "from collections import Counter\n",
    "word_count = Counter(flatten(corpus_tokenized))\n",
    "\n",
    "#count the number of total words\n",
    "num_total_words = sum([c for w, c in word_count.items()])\n",
    "\n",
    "#create the scaled-up unigram distribution table for vocabs\n",
    "z = 0.001 #the scaler\n",
    "unigram_table = []\n",
    "for v in vocabs:\n",
    "    unigram_table.extend([v] * int(((word_count[v]/num_total_words)**0.75)/z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert word indices to tensors\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "#generate random negative samples\n",
    "import random\n",
    "\n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.size(0)\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):\n",
    "        nsample = []\n",
    "        target_index = targets[i].item()\n",
    "        while len(nsample) < k:\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).view(1, -1))\n",
    "    \n",
    "    return torch.cat(neg_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 428, 8653, 4880, 8177, 7439],\n",
       "        [5473, 8597, 1859,  331, 5428],\n",
       "        [3728, 4937, 2743, 1669, 7692],\n",
       "        [6625, 2721, 3684, 1550, 1463],\n",
       "        [5562,  510, 2999, 6043,  860],\n",
       "        [5215, 3519, 3717, 4921, 5148],\n",
       "        [4921, 4692, 8174, 8307, 7032],\n",
       "        [5829, 1043, 6905, 3438, 1076],\n",
       "        [1349, 4629, 6296, 3418, 7333],\n",
       "        [ 705, 8393, 7032, 5528, 2979]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the negative sampling method\n",
    "input_batch, target_batch = random_batch_skip_gram(2, 10)\n",
    "input_batch = torch.LongTensor(input_batch)\n",
    "target_batch = torch.LongTensor(target_batch)\n",
    "\n",
    "num_neg = 5 #number of negative samples for each target word\n",
    "\n",
    "neg_samples = negative_sampling(target_batch, unigram_table, num_neg)\n",
    "\n",
    "neg_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip-gram with negative sampling model\n",
    "class SkipgramNegSampling(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(SkipgramNegSampling, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, emb_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, emb_size)\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "                    \n",
    "    def forward(self, center_words, context_words, neg_samples):\n",
    "        center_embeds  = self.embedding_v(center_words) # [batch_size, 1, emb_size]\n",
    "        context_embeds = self.embedding_u(context_words) # [batch_size, 1, emb_size]\n",
    "        neg_embeds     = self.embedding_u(neg_samples) # [batch_size, num_neg, emb_size]\n",
    "        \n",
    "        positive_score = context_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "        \n",
    "        negative_score = -neg_embeds.bmm(center_embeds.transpose(1, 2))\n",
    "        #[batch_size, k, emb_size] @ [batch_size, emb_size, 1] = [batch_size, num_neg, 1]\n",
    "        \n",
    "        loss = -torch.mean(self.logsigmoid(positive_score) + torch.sum(self.logsigmoid(negative_score), 1))\n",
    "                \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | cost: 28.518932 | time: 0m 48s\n",
      "Epoch: 200 | cost: 37.753193 | time: 1m 36s\n",
      "Epoch: 300 | cost: 41.765404 | time: 2m 24s\n",
      "Epoch: 400 | cost: 25.305706 | time: 3m 9s\n",
      "Epoch: 500 | cost: 35.595299 | time: 3m 54s\n"
     ]
    }
   ],
   "source": [
    "#set parameters\n",
    "window_size = 2\n",
    "batch_size  = 10\n",
    "vocab_size  = len(vocabs)\n",
    "emb_size    = 50\n",
    "model       = SkipgramNegSampling(vocab_size, emb_size)\n",
    "num_neg     = 10\n",
    "optimizer   = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#calculate epoch time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time // 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "#train the model\n",
    "import time\n",
    "\n",
    "num_epochs = 500\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):    \n",
    "    input_batch, target_batch = random_batch_skip_gram(window_size, batch_size)\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    target_batch = torch.LongTensor(target_batch)\n",
    "    neg_samples = negative_sampling(target_batch, unigram_table, num_neg)\n",
    "    \n",
    "    optimizer.zero_grad()   \n",
    "    loss = model(input_batch, target_batch, neg_samples)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        end = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the skip-gram with negative sampling model\n",
    "pickle.dump(model, open('SkipgramNegSampling.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. Preparing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[': capital-common-countries',\n",
       " 'Athens Greece Baghdad Iraq',\n",
       " 'Athens Greece Bangkok Thailand',\n",
       " 'Athens Greece Beijing China',\n",
       " 'Athens Greece Berlin Germany']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the dataset for testing\n",
    "file_path = \"C:/Users/MARC/Downloads/Datasets/questions-words.txt\"\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    contents = f.read()\n",
    "    data = contents.split('\\n')\n",
    "\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : capital-common-countries\n",
      "507 : capital-world\n",
      "5032 : currency\n",
      "5899 : city-in-state\n",
      "8367 : family\n",
      "8874 : gram1-adjective-to-adverb\n",
      "9867 : gram2-opposite\n",
      "10680 : gram3-comparative\n",
      "12013 : gram4-superlative\n",
      "13136 : gram5-present-participle\n",
      "14193 : gram6-nationality-adjective\n",
      "15793 : gram7-past-tense\n",
      "17354 : gram8-plural\n",
      "18687 : gram9-plural-verbs\n"
     ]
    }
   ],
   "source": [
    "#explore the dataset\n",
    "for idx, sent in enumerate(data):\n",
    "    if sent[0] == ':':\n",
    "        print(idx, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['boy', 'girl', 'brother', 'sister'], ['boy', 'girl', 'brothers', 'sisters'], ['boy', 'girl', 'dad', 'mom'], ['boy', 'girl', 'father', 'mother'], ['boy', 'girl', 'grandfather', 'grandmother']]\n",
      "[['banana', 'bananas', 'bird', 'birds'], ['banana', 'bananas', 'bottle', 'bottles'], ['banana', 'bananas', 'building', 'buildings'], ['banana', 'bananas', 'car', 'cars'], ['banana', 'bananas', 'cat', 'cats']]\n"
     ]
    }
   ],
   "source": [
    "#create the corpora for testing\n",
    "family = data[8368:8874]\n",
    "family_tokenized = [sent.split(' ') for sent in family]\n",
    "print(family_tokenized[:5])\n",
    "\n",
    "plural = data[17355:18687]\n",
    "plural_tokenized = [sent.split(' ') for sent in plural]\n",
    "print(plural_tokenized[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get unique words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "family_vocabs  = list(set(flatten(family_tokenized)))\n",
    "plural_vocabs  = list(set(flatten(plural_tokenized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numericalize the vocabs\n",
    "family_word2index = {w: i for i, w in enumerate(family_vocabs)}\n",
    "plural_word2index = {w: i for i, w in enumerate(plural_vocabs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append <UNK>\n",
    "family_vocabs.append('<UNK>')\n",
    "family_word2index['<UNK>'] = len(family_word2index)\n",
    "\n",
    "plural_vocabs.append('<UNK>')\n",
    "plural_word2index['<UNK>'] = len(plural_word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare index2word\n",
    "family_index2word = {i:w for w, i in family_word2index.items()}\n",
    "plural_index2word = {i:w for w, i in plural_word2index.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. Syntactic Test\n",
    "\n",
    "##### The 'plural' corpus will be used for syntactic analogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embedding\n",
    "def get_embed(word):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except:\n",
    "        index = word2index['<UNK>']\n",
    "    \n",
    "    word = torch.LongTensor([index])\n",
    "\n",
    "    center_embed  = model.embedding_v(word)\n",
    "    context_embed = model.embedding_u(word)\n",
    "    \n",
    "    embed = (center_embed + context_embed) / 2\n",
    "    \n",
    "    return  embed[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare word2index and index2word\n",
    "word2index = plural_word2index\n",
    "index2word = {i:w for w, i in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings for GloVe\n",
    "model = pickle.load(open('GloVe.pkl', 'rb'))\n",
    "GloVe_embeds = {}\n",
    "\n",
    "for i in range(len(word2index)):\n",
    "    word = index2word[i]\n",
    "    embed = get_embed(word)\n",
    "    GloVe_embeds.update({word: embed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings for CBOW\n",
    "model = pickle.load(open('CBOW.pkl', 'rb'))\n",
    "CBOW_embeds = {}\n",
    "\n",
    "for i in range(len(word2index)):\n",
    "    word = index2word[i]\n",
    "    embed = get_embed(word)\n",
    "    CBOW_embeds.update({word: embed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings for Skipgram\n",
    "model = pickle.load(open('Skipgram.pkl', 'rb'))\n",
    "Skipgram_embeds = {}\n",
    "\n",
    "for i in range(len(word2index)):\n",
    "    word = index2word[i]\n",
    "    embed = get_embed(word)\n",
    "    Skipgram_embeds.update({word: embed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings for SkipgramNegSampling\n",
    "model = pickle.load(open('SkipgramNegSampling.pkl', 'rb'))\n",
    "SkipgramNegSampling_embeds = {}\n",
    "\n",
    "for i in range(len(word2index)):\n",
    "    word = index2word[i]\n",
    "    embed = get_embed(word)\n",
    "    SkipgramNegSampling_embeds.update({word: embed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine similarity function\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analogy function\n",
    "def analogy(a, b, c, embeds, vocabs):\n",
    "    d_vector = embeds[c] - embeds[a] + embeds[b]\n",
    "\n",
    "    similarity = -1\n",
    "    for vocab in vocabs:\n",
    "        if vocab not in [a, b, c]:\n",
    "            if cos_sim(d_vector, embeds[vocab]) > similarity:\n",
    "                similarity = cos_sim(d_vector, embeds[vocab])\n",
    "                d = (vocab, similarity)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fingers', 0.41037777)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy('car', 'cars', 'lion', GloVe_embeds, plural_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy function\n",
    "def accuracy(label, pred):\n",
    "    if pred == label:\n",
    "        True\n",
    "    else:\n",
    "        False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of GloVe is 0.0%.\n",
      "The accuracy of CBOW is 0.0%.\n",
      "The accuracy of Skipgram is 0.0%.\n",
      "The accuracy of SkipgramNegSampling is 0.0%.\n"
     ]
    }
   ],
   "source": [
    "#count the syntactic accuracies of model embeddings\n",
    "models = ['GloVe', 'CBOW', 'Skipgram', 'SkipgramNegSampling']\n",
    "embeds = [GloVe_embeds, CBOW_embeds, Skipgram_embeds, SkipgramNegSampling_embeds]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    embed = embeds[i]\n",
    "    accuracy_count = 0\n",
    "    for sent in plural_tokenized:\n",
    "        label = sent[-1]\n",
    "        a, b, c = sent[:-1]\n",
    "        pred = analogy(a, b, c, embed, plural_vocabs)[0]\n",
    "        if accuracy(label, pred) is True:\n",
    "            accuracy_count += 1\n",
    "    \n",
    "    print(f'The accuracy of {model} is {accuracy_count/len(plural_tokenized)}%.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Semantic Test\n",
    "\n",
    "The 'family' corpus will be used for semantic analogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare word2index and index2word\n",
    "word2index = family_word2index\n",
    "index2word = {i:w for w, i in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings for GloVe\n",
    "model = pickle.load(open('GloVe.pkl', 'rb'))\n",
    "GloVe_embeds = {}\n",
    "\n",
    "for i in range(len(word2index)):\n",
    "    word = index2word[i]\n",
    "    embed = get_embed(word)\n",
    "    GloVe_embeds.update({word: embed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings for CBOW\n",
    "model = pickle.load(open('CBOW.pkl', 'rb'))\n",
    "CBOW_embeds = {}\n",
    "\n",
    "for i in range(len(word2index)):\n",
    "    word = index2word[i]\n",
    "    embed = get_embed(word)\n",
    "    CBOW_embeds.update({word: embed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings for Skipgram\n",
    "model = pickle.load(open('Skipgram.pkl', 'rb'))\n",
    "Skipgram_embeds = {}\n",
    "\n",
    "for i in range(len(word2index)):\n",
    "    word = index2word[i]\n",
    "    embed = get_embed(word)\n",
    "    Skipgram_embeds.update({word: embed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embeddings for SkipgramNegSampling\n",
    "model = pickle.load(open('SkipgramNegSampling.pkl', 'rb'))\n",
    "SkipgramNegSampling_embeds = {}\n",
    "\n",
    "for i in range(len(word2index)):\n",
    "    word = index2word[i]\n",
    "    embed = get_embed(word)\n",
    "    SkipgramNegSampling_embeds.update({word: embed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of GloVe is 0.0%.\n",
      "The accuracy of CBOW is 0.0%.\n",
      "The accuracy of Skipgram is 0.0%.\n",
      "The accuracy of SkipgramNegSampling is 0.0%.\n"
     ]
    }
   ],
   "source": [
    "#count the semantic accuracies of model embeddings\n",
    "models = ['GloVe', 'CBOW', 'Skipgram', 'SkipgramNegSampling']\n",
    "embeds = [GloVe_embeds, CBOW_embeds, Skipgram_embeds, SkipgramNegSampling_embeds]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    embed = embeds[i]\n",
    "    accuracy_count = 0\n",
    "    for sent in family_tokenized:\n",
    "        label = sent[-1]\n",
    "        a, b, c = sent[:-1]\n",
    "        pred = analogy(a, b, c, embed, family_vocabs)[0]\n",
    "        if accuracy(label, pred) is True:\n",
    "            accuracy_count += 1\n",
    "    \n",
    "    print(f'The accuracy of {model} is {accuracy_count/len(plural_tokenized)}%.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4. Findings and Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The 4 embedding models were trained with the following parameters:**\n",
    "- window size = 2\n",
    "- batch size = 10\n",
    "- embedding size = 50\n",
    "- number of negative samples = 10\n",
    "- optimizer = Adam\n",
    "- learning rate = 0.001\n",
    "\n",
    "**Then, the embeddings were tested on syntactic and semantic analogies. The test results are as follows:**\n",
    "\n",
    "| Model | Syntactic Accuracy | Semantic Accuracy  |\n",
    "| --- | --- | ---  |\n",
    "| GloVe | 0.0 | 0.0  |\n",
    "| CBOW | 0.0 | 0.0  |\n",
    "| Skip-gram | 0.0 | 0.0  |\n",
    "| Skip-gram (Neg) | 0.0 | 0.0  |\n",
    "\n",
    "**All models achieved 0% accuracy. It may be due to the limited size of the corpus.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plane</td>\n",
       "      <td>car</td>\n",
       "      <td>5.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>car</td>\n",
       "      <td>6.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>television</td>\n",
       "      <td>radio</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1      2\n",
       "0       tiger    cat   7.35\n",
       "1       tiger  tiger  10.00\n",
       "2       plane    car   5.77\n",
       "3       train    car   6.31\n",
       "4  television  radio   6.77"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "import pandas as pd\n",
    "file_path = \"C:/Users/MARC/Downloads/Datasets/wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\"\n",
    "df = pd.read_table(file_path, header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract features and labels\n",
    "x1 = df.iloc[:, 0]\n",
    "x2 = df.iloc[:, 1]\n",
    "y  = df.iloc[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fellow', 'citizens', 'senate', 'house', 'representatives'], ['vicissitudes', 'incident', 'life', 'event', 'filled', 'greater', 'anxieties', 'notification', 'transmitted', 'order', 'received', '14th', 'day', 'present', 'month'], ['hand', 'summoned', 'country', 'voice', 'hear', 'veneration', 'love', 'retreat', 'chosen', 'fondest', 'predilection', 'flattering', 'hopes', 'immutable', 'decision', 'asylum', 'declining', 'years', 'retreat', 'rendered', 'day', 'necessary', 'dear', 'addition', 'habit', 'inclination', 'frequent', 'interruptions', 'health', 'gradual', 'waste', 'committed', 'time']]\n"
     ]
    }
   ],
   "source": [
    "#recall the Inaugural corpus\n",
    "print(corpus_tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare word2index\n",
    "vocabs = list(set(flatten(corpus_tokenized)))\n",
    "word2index = {w: i for i, w in enumerate(vocabs)}\n",
    "\n",
    "vocabs.append('<UNK>')\n",
    "word2index['<UNK>'] = len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embedding\n",
    "def get_embed(word):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except:\n",
    "        index = word2index['<UNK>']\n",
    "    \n",
    "    word = torch.LongTensor([index])\n",
    "\n",
    "    center_embed  = model.embedding_v(word)\n",
    "    context_embed = model.embedding_u(word)\n",
    "    \n",
    "    embed = (center_embed + context_embed) / 2\n",
    "    \n",
    "    return  embed[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate Spearman's correlation\n",
    "from scipy import stats\n",
    "\n",
    "def correlation(x1, x2):\n",
    "    x1_embed = get_embed(x1)\n",
    "    x2_embed = get_embed(x2)\n",
    "    return stats.spearmanr(x1_embed, x2_embed)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1771428571428571\n",
      "0.9999999999999999\n",
      "-0.05555822328931572\n",
      "-0.03202881152460984\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "model = pickle.load(open('GloVe.pkl', 'rb'))\n",
    "\n",
    "print(correlation('tiger', 'cat'))\n",
    "print(correlation('tiger', 'tiger'))\n",
    "print(correlation('plane', 'car'))\n",
    "print(correlation('train', 'car'))\n",
    "print(correlation('television', 'radio'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12384153661464585\n",
      "0.9999999999999999\n",
      "-0.05181272509003601\n",
      "-0.1865546218487395\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "model = pickle.load(open('CBOW.pkl', 'rb'))\n",
    "\n",
    "print(correlation('tiger', 'cat'))\n",
    "print(correlation('tiger', 'tiger'))\n",
    "print(correlation('plane', 'car'))\n",
    "print(correlation('train', 'car'))\n",
    "print(correlation('television', 'radio'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.771428571428571, 9.999999999999998, -0.5555822328931572, -0.3202881152460984, 9.999999999999998]\n",
      "[1.2384153661464585, 9.999999999999998, -0.51812725090036, -1.865546218487395, 9.999999999999998]\n",
      "[-0.9253301320528211, 9.999999999999998, -1.090516206482593, 0.6516206482593037, 9.999999999999998]\n",
      "[-1.3968787515006, 9.999999999999998, -0.5229291716686675, 1.3767106842737096, 9.999999999999998]\n"
     ]
    }
   ],
   "source": [
    "#calcuate word similarity correlations of model embeddings\n",
    "models = ['GloVe.pkl', 'CBOW.pkl', 'Skipgram.pkl', 'SkipgramNegSampling.pkl']\n",
    "yhats = [[]] * len(models)\n",
    "\n",
    "for m in range(len(models)):\n",
    "    model = pickle.load(open(models[m], 'rb'))\n",
    "\n",
    "    yhat_list = []\n",
    "    for i in range(len(y)):\n",
    "        yhat_list.append(correlation(x1[i], x2[i])*10)\n",
    "\n",
    "    yhats[m] = yhat_list\n",
    "    \n",
    "    print(yhats[m][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of the word similarity correlations of GloVe is 28.71.\n",
      "MSE of the word similarity correlations of CBOW is 28.03.\n",
      "MSE of the word similarity correlations of Skipgram is 30.43.\n",
      "MSE of the word similarity correlations of SkipgramNegSampling is 29.53.\n"
     ]
    }
   ],
   "source": [
    "#evaluate word similarity correlations of model embeddings\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "models = ['GloVe', 'CBOW', 'Skipgram', 'SkipgramNegSampling']\n",
    "\n",
    "for m in range(len(models)):\n",
    "    yhat = yhats[m]\n",
    "    loss = mse(y, yhat)\n",
    "    print(f'MSE of the word similarity correlations of {models[m]} is {loss:.2f}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CBOW model gets the best performance in terms of word similarity.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7ca07084f99cae884d00a2401f5a915152405bd446c47609c53897335e04337"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
