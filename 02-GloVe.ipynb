{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19 Jan - GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the training corpus\n",
    "#I use the Australian Broadcasting Comission 2006 Corpus from NLTK\n",
    "import nltk\n",
    "from nltk.corpus import abc\n",
    "corpus = nltk.corpus.abc.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['PM', 'denies', 'knowledge', 'of', 'AWB', 'kickbacks', 'The', 'Prime', 'Minister', 'has', 'denied', 'he', 'knew', 'AWB', 'was', 'paying', 'kickbacks', 'to', 'Iraq', 'despite', 'writing', 'to', 'the', 'wheat', 'exporter', 'asking', 'to', 'be', 'kept', 'fully', 'informed', 'on', 'Iraq', 'wheat', 'sales', '.'], ['Letters', 'from', 'John', 'Howard', 'and', 'Deputy', 'Prime', 'Minister', 'Mark', 'Vaile', 'to', 'AWB', 'have', 'been', 'released', 'by', 'the', 'Cole', 'inquiry', 'into', 'the', 'oil', 'for', 'food', 'program', '.'], ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the corpus is already tokenized\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['pm', 'denies', 'knowledge', 'of', 'awb', 'kickbacks', 'the', 'prime', 'minister', 'has', 'denied', 'he', 'knew', 'awb', 'was', 'paying', 'kickbacks', 'to', 'iraq', 'despite', 'writing', 'to', 'the', 'wheat', 'exporter', 'asking', 'to', 'be', 'kept', 'fully', 'informed', 'on', 'iraq', 'wheat', 'sales', '.'], ['letters', 'from', 'john', 'howard', 'and', 'deputy', 'prime', 'minister', 'mark', 'vaile', 'to', 'awb', 'have', 'been', 'released', 'by', 'the', 'cole', 'inquiry', 'into', 'the', 'oil', 'for', 'food', 'program', '.'], ['in', 'one', 'of', 'the', 'letters', 'mr', 'howard', 'asks', 'awb', 'managing', 'director', 'andrew', 'lindberg', 'to', 'remain', 'in', 'close', 'contact', 'with', 'the', 'government', 'on', 'iraq', 'wheat', 'sales', '.'], ['the', 'opposition', \"'\", 's', 'gavan', 'o', \"'\", 'connor', 'says', 'the', 'letter', 'was', 'sent', 'in', '2002', ',', 'the', 'same', 'time', 'awb', 'was', 'paying', 'kickbacks', 'to', 'iraq', 'though', 'a', 'jordanian', 'trucking', 'company', '.'], ['he', 'says', 'the', 'government', 'can', 'longer', 'wipe', 'its', 'hands', 'of', 'the', 'illicit', 'payments', ',', 'which', 'totalled', '$', '290', 'million', '.']]\n"
     ]
    }
   ],
   "source": [
    "#convert the words in the corpus into lower case\n",
    "corpus_tokenized = [[]] * len(corpus)\n",
    "for i in range(len(corpus)):\n",
    "    corpus_tokenized[i] = [word.lower() for word in corpus[i]]\n",
    "\n",
    "print(corpus_tokenized[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['pm', 'denies', 'knowledge', 'awb', 'kickbacks', 'prime', 'minister', 'denied', 'knew', 'awb', 'paying', 'kickbacks', 'iraq', 'despite', 'writing', 'wheat', 'exporter', 'asking', 'kept', 'fully', 'informed', 'iraq', 'wheat', 'sales', '.'], ['letters', 'john', 'howard', 'deputy', 'prime', 'minister', 'mark', 'vaile', 'awb', 'released', 'cole', 'inquiry', 'oil', 'food', 'program', '.'], ['letters', 'mr', 'howard', 'asks', 'awb', 'managing', 'director', 'andrew', 'lindberg', 'remain', 'close', 'contact', 'government', 'iraq', 'wheat', 'sales', '.'], ['opposition', \"'\", 's', 'gavan', 'o', \"'\", 'connor', 'says', 'letter', 'sent', '2002', ',', 'time', 'awb', 'paying', 'kickbacks', 'iraq', 'jordanian', 'trucking', 'company', '.'], ['says', 'government', 'longer', 'wipe', 'hands', 'illicit', 'payments', ',', 'totalled', '$', '290', 'million', '.']]\n"
     ]
    }
   ],
   "source": [
    "#remove stop words with SpaCy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "for sentence in corpus_tokenized:\n",
    "    for word in sentence[:]:\n",
    "        if word in stopwords:\n",
    "            sentence.remove(word)\n",
    "\n",
    "print(corpus_tokenized[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['pm', 'denies', 'knowledge', 'awb', 'kickbacks', 'prime', 'minister', 'denied', 'knew', 'awb', 'paying', 'kickbacks', 'iraq', 'despite', 'writing', 'wheat', 'exporter', 'asking', 'kept', 'fully', 'informed', 'iraq', 'wheat', 'sales'], ['letters', 'john', 'howard', 'deputy', 'prime', 'minister', 'mark', 'vaile', 'awb', 'released', 'cole', 'inquiry', 'oil', 'food', 'program'], ['letters', 'mr', 'howard', 'asks', 'awb', 'managing', 'director', 'andrew', 'lindberg', 'remain', 'close', 'contact', 'government', 'iraq', 'wheat', 'sales'], ['opposition', 's', 'gavan', 'o', 'connor', 'says', 'letter', 'sent', '2002', 'time', 'awb', 'paying', 'kickbacks', 'iraq', 'jordanian', 'trucking', 'company'], ['says', 'government', 'longer', 'wipe', 'hands', 'illicit', 'payments', 'totalled', '290', 'million']]\n"
     ]
    }
   ],
   "source": [
    "#remove punctutations with String\n",
    "import string\n",
    "punctutations = string.punctuation\n",
    "\n",
    "for sentence in corpus_tokenized:\n",
    "    for word in sentence[:]:\n",
    "        if word in punctutations:\n",
    "            sentence.remove(word)\n",
    "\n",
    "print(corpus_tokenized[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27444\n"
     ]
    }
   ],
   "source": [
    "#get unique words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabs = list(set(flatten(corpus_tokenized)))\n",
    "\n",
    "print(len(vocabs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pace': 0,\n",
       " 'damaged': 1,\n",
       " 'offloading': 2,\n",
       " 'squib': 3,\n",
       " 'shacks': 4,\n",
       " 'campfire': 5,\n",
       " 'households': 6,\n",
       " 'matscape': 7,\n",
       " 'tongues': 8,\n",
       " 'indefinite': 9,\n",
       " '1990s': 10,\n",
       " 'absorbs': 11,\n",
       " 'comte': 12,\n",
       " 'buoys': 13,\n",
       " 'hooks': 14,\n",
       " 'companion': 15,\n",
       " 'tas2r38': 16,\n",
       " 'imaged': 17,\n",
       " 'adobe': 18,\n",
       " 'skeletal': 19,\n",
       " 'trent': 20,\n",
       " 'flooding': 21,\n",
       " 'inaccurate': 22,\n",
       " 'lemurs': 23,\n",
       " 'mandil': 24,\n",
       " 'relief': 25,\n",
       " 'seeney': 26,\n",
       " 'bucked': 27,\n",
       " 'preschool': 28,\n",
       " 'chilli': 29,\n",
       " 'collecting': 30,\n",
       " 'emperors': 31,\n",
       " 'sounding': 32,\n",
       " 'prosthetics': 33,\n",
       " 'ngc': 34,\n",
       " 'toll': 35,\n",
       " 'fare': 36,\n",
       " '4395': 37,\n",
       " 'reproductive': 38,\n",
       " 'collaborated': 39,\n",
       " 'folded': 40,\n",
       " 'plagiarism': 41,\n",
       " 'empathise': 42,\n",
       " 'daydreams': 43,\n",
       " 'partnership': 44,\n",
       " 'rose': 45,\n",
       " 'globe': 46,\n",
       " 'hail': 47,\n",
       " 'provider': 48,\n",
       " 'clings': 49,\n",
       " 'shakeup': 50,\n",
       " 'ladines': 51,\n",
       " 'susceptibilities': 52,\n",
       " 'recorded': 53,\n",
       " 'affirms': 54,\n",
       " 'snacks': 55,\n",
       " 'providence': 56,\n",
       " 'retirement': 57,\n",
       " 'nott': 58,\n",
       " 'pais': 59,\n",
       " 'serpins': 60,\n",
       " 'whinnies': 61,\n",
       " 'uraeus': 62,\n",
       " 'gap': 63,\n",
       " 'onshore': 64,\n",
       " 'mussel': 65,\n",
       " 'piezoelectric': 66,\n",
       " 'clue': 67,\n",
       " 'govts': 68,\n",
       " 'katherine': 69,\n",
       " 'representatives': 70,\n",
       " '775': 71,\n",
       " 'detain': 72,\n",
       " 'insulation': 73,\n",
       " 'blackberry': 74,\n",
       " 'students': 75,\n",
       " 'bannon': 76,\n",
       " 'faecal': 77,\n",
       " 'francesconi': 78,\n",
       " 'sinosteel': 79,\n",
       " 'rabinowicz': 80,\n",
       " 'closure': 81,\n",
       " 'powder': 82,\n",
       " 'polystyrene': 83,\n",
       " 'decide': 84,\n",
       " 'melanie': 85,\n",
       " 'flying': 86,\n",
       " 'region': 87,\n",
       " 'tiff': 88,\n",
       " 'making': 89,\n",
       " 'clifton': 90,\n",
       " 'jundah': 91,\n",
       " 'atholl': 92,\n",
       " 'killing': 93,\n",
       " 'laos': 94,\n",
       " 'blockade': 95,\n",
       " 'rockers': 96,\n",
       " 'p2': 97,\n",
       " 'cargo': 98,\n",
       " 'crushed': 99,\n",
       " 'tournament': 100,\n",
       " 'hampering': 101,\n",
       " 'fear': 102,\n",
       " 'blg': 103,\n",
       " 'awfully': 104,\n",
       " 'predisposes': 105,\n",
       " 'horizontally': 106,\n",
       " 'morphological': 107,\n",
       " 'anticipate': 108,\n",
       " 'useability': 109,\n",
       " 'sanction': 110,\n",
       " 'technical': 111,\n",
       " 'pipfruit': 112,\n",
       " 'port': 113,\n",
       " 'cystic': 114,\n",
       " 'colville': 115,\n",
       " 'jonathon': 116,\n",
       " 'boneless': 117,\n",
       " 'sheepdogs': 118,\n",
       " 'balonne': 119,\n",
       " 'bondi': 120,\n",
       " 'greengrocers': 121,\n",
       " 'identifiers': 122,\n",
       " 'rix': 123,\n",
       " 'nathan': 124,\n",
       " 'loxton': 125,\n",
       " 'theophanous': 126,\n",
       " 'hungry': 127,\n",
       " 'marinated': 128,\n",
       " 'emtions': 129,\n",
       " 'ninth': 130,\n",
       " 'bargained': 131,\n",
       " 'disappointment': 132,\n",
       " 'aami': 133,\n",
       " 'growths': 134,\n",
       " 'goldfish': 135,\n",
       " 'shampoos': 136,\n",
       " 'annus': 137,\n",
       " 'hagan': 138,\n",
       " 'patmos': 139,\n",
       " 'dexterous': 140,\n",
       " 'wistar': 141,\n",
       " 'bureaucrat': 142,\n",
       " 'flown': 143,\n",
       " 'eelco': 144,\n",
       " 'signs': 145,\n",
       " 'morwood': 146,\n",
       " 'require': 147,\n",
       " 'bobbie': 148,\n",
       " 'merriwa': 149,\n",
       " 'licinio': 150,\n",
       " 'newsouth': 151,\n",
       " 'wreaths': 152,\n",
       " 'outdated': 153,\n",
       " 'hotels': 154,\n",
       " 'froze': 155,\n",
       " 'encrypted': 156,\n",
       " 'mussels': 157,\n",
       " 'grains': 158,\n",
       " 'exchanger': 159,\n",
       " 'mmc': 160,\n",
       " 'forecasting': 161,\n",
       " 'misrepresents': 162,\n",
       " 'troposphere': 163,\n",
       " 'ordered': 164,\n",
       " 'soccer': 165,\n",
       " 'performed': 166,\n",
       " '188': 167,\n",
       " 'nurses': 168,\n",
       " 'wingspan': 169,\n",
       " 'joyce': 170,\n",
       " 'opioid': 171,\n",
       " 'contend': 172,\n",
       " 'analysed': 173,\n",
       " 'chute': 174,\n",
       " 'reemerged': 175,\n",
       " 'erbium': 176,\n",
       " 'ips': 177,\n",
       " 'nozzle': 178,\n",
       " 'interlock': 179,\n",
       " 'reimposed': 180,\n",
       " 'zajac': 181,\n",
       " '949': 182,\n",
       " '825': 183,\n",
       " 'halitosis': 184,\n",
       " 'amblyrhynchos': 185,\n",
       " 'vote': 186,\n",
       " 'proposals': 187,\n",
       " 'orderly': 188,\n",
       " 'gunnar': 189,\n",
       " 'fuelling': 190,\n",
       " 'stephenson': 191,\n",
       " 'interpreter': 192,\n",
       " 'strife': 193,\n",
       " 'stardust': 194,\n",
       " 'progressively': 195,\n",
       " 'context': 196,\n",
       " 'pine': 197,\n",
       " 'pigeons': 198,\n",
       " 'pretense': 199,\n",
       " 'bigelow': 200,\n",
       " 'trapping': 201,\n",
       " 'puertasaurus': 202,\n",
       " 'cormick': 203,\n",
       " 'koperberg': 204,\n",
       " 'festival': 205,\n",
       " 'liddicoat': 206,\n",
       " 'unpatterned': 207,\n",
       " 'ok': 208,\n",
       " 'boycotted': 209,\n",
       " 'pensioners': 210,\n",
       " 'kune': 211,\n",
       " 'lode': 212,\n",
       " 'religious': 213,\n",
       " 'snorts': 214,\n",
       " 'kalgoorlie': 215,\n",
       " 'snoozing': 216,\n",
       " 'trudi': 217,\n",
       " 'roll': 218,\n",
       " 'daydream': 219,\n",
       " 'men': 220,\n",
       " 'abu': 221,\n",
       " 'fatty': 222,\n",
       " 'issuing': 223,\n",
       " 'traversed': 224,\n",
       " 'fil': 225,\n",
       " 'gruia': 226,\n",
       " 'primeval': 227,\n",
       " 'voumard': 228,\n",
       " '630': 229,\n",
       " 'restore': 230,\n",
       " 'ancestral': 231,\n",
       " 'cma': 232,\n",
       " 'fifth': 233,\n",
       " 'burst': 234,\n",
       " 'spinach': 235,\n",
       " 'remained': 236,\n",
       " 'ruler': 237,\n",
       " 'renews': 238,\n",
       " 'smiling': 239,\n",
       " 'perform': 240,\n",
       " 'copyrighted': 241,\n",
       " '1447': 242,\n",
       " 'humann': 243,\n",
       " 'sparkling': 244,\n",
       " 'involved': 245,\n",
       " 'paradox': 246,\n",
       " 'muraca': 247,\n",
       " 'tyler': 248,\n",
       " 'colorado': 249,\n",
       " 'credible': 250,\n",
       " 'grooves': 251,\n",
       " 'g√∂teborg': 252,\n",
       " 'located': 253,\n",
       " 'charon': 254,\n",
       " 'dendritic': 255,\n",
       " 'crest': 256,\n",
       " 'distills': 257,\n",
       " 'rogers': 258,\n",
       " 'rocketed': 259,\n",
       " 'christianity': 260,\n",
       " 'optional': 261,\n",
       " '5pc': 262,\n",
       " 'tore': 263,\n",
       " 'exists': 264,\n",
       " 'fogs': 265,\n",
       " 'hips': 266,\n",
       " 'evenly': 267,\n",
       " 'monies': 268,\n",
       " 'dealer': 269,\n",
       " 'arachnids': 270,\n",
       " 'landholders': 271,\n",
       " 'learning': 272,\n",
       " 'leonhardt': 273,\n",
       " 'angel': 274,\n",
       " 'unpicked': 275,\n",
       " 'physically': 276,\n",
       " 'waltham': 277,\n",
       " 'function': 278,\n",
       " 'seafoods': 279,\n",
       " 'depicting': 280,\n",
       " 'toughened': 281,\n",
       " 'forestry': 282,\n",
       " 'seekport': 283,\n",
       " '1294': 284,\n",
       " 'stricker': 285,\n",
       " 'adjoining': 286,\n",
       " 'threatening': 287,\n",
       " 'illinois': 288,\n",
       " 'refurbish': 289,\n",
       " 'afgorce': 290,\n",
       " 'stall': 291,\n",
       " 'gear': 292,\n",
       " 'kipunji': 293,\n",
       " 'experimental': 294,\n",
       " 'ljubo': 295,\n",
       " 'madison': 296,\n",
       " 'khalil': 297,\n",
       " 'manageable': 298,\n",
       " 'correspondence': 299,\n",
       " 'lobbyist': 300,\n",
       " 'anders': 301,\n",
       " 'piloting': 302,\n",
       " 'gould': 303,\n",
       " 'elliptical': 304,\n",
       " 'panic': 305,\n",
       " 'belonging': 306,\n",
       " 'restaurant': 307,\n",
       " 'herbarium': 308,\n",
       " 'inclusive': 309,\n",
       " 'lionel': 310,\n",
       " 'dilute': 311,\n",
       " 'arrowheads': 312,\n",
       " 'bolzano': 313,\n",
       " 'theoreticians': 314,\n",
       " 'vulgar': 315,\n",
       " 'transactions': 316,\n",
       " 'astronomy': 317,\n",
       " 'keelty': 318,\n",
       " 'sexuality': 319,\n",
       " 'profitable': 320,\n",
       " 'colleagues': 321,\n",
       " 'listen': 322,\n",
       " 'almanac': 323,\n",
       " 'label': 324,\n",
       " 'complementary': 325,\n",
       " 'imagining': 326,\n",
       " 'gajerrong': 327,\n",
       " 'gin': 328,\n",
       " 'newer': 329,\n",
       " 'hop': 330,\n",
       " '78': 331,\n",
       " 'shijian': 332,\n",
       " 'ghostbusters': 333,\n",
       " 'standstill': 334,\n",
       " 'dives': 335,\n",
       " 'elkins': 336,\n",
       " 'whyalla': 337,\n",
       " 'wages': 338,\n",
       " 'traders': 339,\n",
       " 'parthenon': 340,\n",
       " 'encourages': 341,\n",
       " 'elects': 342,\n",
       " 'accelerator': 343,\n",
       " 'bursts': 344,\n",
       " 'newsrooms': 345,\n",
       " 'prospective': 346,\n",
       " 'stone': 347,\n",
       " 'websense': 348,\n",
       " 'greenpeace': 349,\n",
       " 'challenges': 350,\n",
       " 'artificial': 351,\n",
       " 'bail': 352,\n",
       " 'purposes': 353,\n",
       " 'carl': 354,\n",
       " 'emerald': 355,\n",
       " 'profiles': 356,\n",
       " 'macular': 357,\n",
       " 'mobile': 358,\n",
       " 'csi': 359,\n",
       " 'fussy': 360,\n",
       " 'cleve': 361,\n",
       " 'michelle': 362,\n",
       " 'mri': 363,\n",
       " 'sliding': 364,\n",
       " 'declare': 365,\n",
       " 'sulfur': 366,\n",
       " 'ambiguous': 367,\n",
       " 'quizzed': 368,\n",
       " 'belt': 369,\n",
       " 'reeling': 370,\n",
       " 'wilkinson': 371,\n",
       " 'hindu': 372,\n",
       " 'rutledge': 373,\n",
       " 'clement': 374,\n",
       " 'plagued': 375,\n",
       " 'categorises': 376,\n",
       " 'claudia': 377,\n",
       " 'itching': 378,\n",
       " 'fend': 379,\n",
       " 'bogus': 380,\n",
       " 'eternity': 381,\n",
       " 'unauthorised': 382,\n",
       " 'devoncourt': 383,\n",
       " 'plunge': 384,\n",
       " 'carcinogen': 385,\n",
       " 'savannah': 386,\n",
       " 'commons': 387,\n",
       " 'hiroshi': 388,\n",
       " 'meridional': 389,\n",
       " 'offences': 390,\n",
       " 'batten': 391,\n",
       " 'coordinates': 392,\n",
       " 'jolie': 393,\n",
       " 'terracotta': 394,\n",
       " 'graffitist': 395,\n",
       " 'petuna': 396,\n",
       " 'extinct': 397,\n",
       " 'compensating': 398,\n",
       " 'whale': 399,\n",
       " 'wing': 400,\n",
       " 'spatial': 401,\n",
       " 'concorde': 402,\n",
       " 'fraudster': 403,\n",
       " 'therapies': 404,\n",
       " 'coordinate': 405,\n",
       " 'historians': 406,\n",
       " 'renege': 407,\n",
       " 'killer': 408,\n",
       " 'schwartzkopff': 409,\n",
       " 'flight': 410,\n",
       " 'outcomes': 411,\n",
       " 'severely': 412,\n",
       " 'cleans': 413,\n",
       " 'prosecution': 414,\n",
       " 'teen': 415,\n",
       " 'unions': 416,\n",
       " 'jelena': 417,\n",
       " 'rigidity': 418,\n",
       " 'pia': 419,\n",
       " 'pitcher': 420,\n",
       " 'syntax': 421,\n",
       " 'trans': 422,\n",
       " 'heike': 423,\n",
       " 'poisoned': 424,\n",
       " 'substantiate': 425,\n",
       " 'grabs': 426,\n",
       " '768': 427,\n",
       " 'utilities': 428,\n",
       " 'transgenic': 429,\n",
       " 'kynuna': 430,\n",
       " 'shortcomings': 431,\n",
       " 'broke': 432,\n",
       " 'vintage': 433,\n",
       " 'platypus': 434,\n",
       " 'adventures': 435,\n",
       " 'schneider': 436,\n",
       " 'rambling': 437,\n",
       " 'washington': 438,\n",
       " 'outstrip': 439,\n",
       " 'forgetting': 440,\n",
       " 'acupuncture': 441,\n",
       " 'lyndley': 442,\n",
       " 'itokawa': 443,\n",
       " 'runs': 444,\n",
       " 'nationals': 445,\n",
       " 'restart': 446,\n",
       " 'sharpening': 447,\n",
       " 'fluctuation': 448,\n",
       " 'vanderbilt': 449,\n",
       " 'erickson': 450,\n",
       " 'bodes': 451,\n",
       " 'turmoil': 452,\n",
       " 'ludicrous': 453,\n",
       " 'radically': 454,\n",
       " 'surf': 455,\n",
       " 'engh': 456,\n",
       " 'sarah': 457,\n",
       " 'chronology': 458,\n",
       " 'lithium': 459,\n",
       " 'inaccuracy': 460,\n",
       " 'elders': 461,\n",
       " 'clipped': 462,\n",
       " 'shiny': 463,\n",
       " 'gathers': 464,\n",
       " 'mince': 465,\n",
       " 'vanhaeren': 466,\n",
       " 'underclass': 467,\n",
       " 'koroit': 468,\n",
       " 'lomonosov': 469,\n",
       " 'loo': 470,\n",
       " 'diligent': 471,\n",
       " 'persevere': 472,\n",
       " 'anger': 473,\n",
       " 'rocklands': 474,\n",
       " 'accountant': 475,\n",
       " 'actual': 476,\n",
       " 'taxi': 477,\n",
       " 'operate': 478,\n",
       " 'vlacic': 479,\n",
       " 'lieutenant': 480,\n",
       " 'kilomgram': 481,\n",
       " 'reassuring': 482,\n",
       " 'herschel': 483,\n",
       " '457': 484,\n",
       " 'exhausted': 485,\n",
       " 'indirectly': 486,\n",
       " 'tuck': 487,\n",
       " 'sapiens': 488,\n",
       " 'capanna': 489,\n",
       " 'elements': 490,\n",
       " 'vibes': 491,\n",
       " 'audience': 492,\n",
       " 'medial': 493,\n",
       " 'plot': 494,\n",
       " 'historic': 495,\n",
       " '375': 496,\n",
       " 'hover': 497,\n",
       " 'extinction': 498,\n",
       " 'stripping': 499,\n",
       " 'unapproved': 500,\n",
       " 'capella': 501,\n",
       " 'scripts': 502,\n",
       " 'wider': 503,\n",
       " 'herbivores': 504,\n",
       " 'relativity': 505,\n",
       " 'sailors': 506,\n",
       " 'wintry': 507,\n",
       " 'dodgem': 508,\n",
       " 'vasco': 509,\n",
       " 'wessex': 510,\n",
       " 'restaurants': 511,\n",
       " 'edna': 512,\n",
       " 'sticky': 513,\n",
       " 'shahnaz': 514,\n",
       " 'sticking': 515,\n",
       " 'stated': 516,\n",
       " 'morrish': 517,\n",
       " 'thermostat': 518,\n",
       " 'vecchi': 519,\n",
       " 'petroglyphs': 520,\n",
       " 'snow': 521,\n",
       " 'ascherio': 522,\n",
       " 'avocados': 523,\n",
       " 'barbara': 524,\n",
       " 'brook': 525,\n",
       " 'negated': 526,\n",
       " 'november': 527,\n",
       " 'cochlea': 528,\n",
       " 'deceptively': 529,\n",
       " 'telstra': 530,\n",
       " 'parasitologist': 531,\n",
       " '2011': 532,\n",
       " 'dimensions': 533,\n",
       " 'penis': 534,\n",
       " 'decommissioned': 535,\n",
       " 'seeing': 536,\n",
       " 'experience': 537,\n",
       " 'logo': 538,\n",
       " 'punt': 539,\n",
       " 'cyberspace': 540,\n",
       " 'lodge': 541,\n",
       " 'ambulance': 542,\n",
       " 'dongara': 543,\n",
       " 'korv': 544,\n",
       " 'rowling': 545,\n",
       " 'bacon': 546,\n",
       " 'contracted': 547,\n",
       " 'targets': 548,\n",
       " 'metabolites': 549,\n",
       " 'synthetics': 550,\n",
       " 'fidelity': 551,\n",
       " 'ska': 552,\n",
       " 'cheryl': 553,\n",
       " 'thrombin': 554,\n",
       " 'dillards': 555,\n",
       " 'gnawed': 556,\n",
       " 'subsistance': 557,\n",
       " 'yearling': 558,\n",
       " 'categorised': 559,\n",
       " 'appointments': 560,\n",
       " 'audits': 561,\n",
       " 'synchronously': 562,\n",
       " 'alginate': 563,\n",
       " 'tuesday': 564,\n",
       " 'characterised': 565,\n",
       " 'mileura': 566,\n",
       " 'improvisation': 567,\n",
       " 'timor': 568,\n",
       " 'stomach': 569,\n",
       " 'boral': 570,\n",
       " 'tons': 571,\n",
       " 'martial': 572,\n",
       " 'responsibility': 573,\n",
       " 'denying': 574,\n",
       " 'medications': 575,\n",
       " 'labaran': 576,\n",
       " 'unacceptable': 577,\n",
       " 'visited': 578,\n",
       " 'chelonia': 579,\n",
       " 'naturalist': 580,\n",
       " 'endophyte': 581,\n",
       " 'gum': 582,\n",
       " 'dissolve': 583,\n",
       " 'capture': 584,\n",
       " 'casualty': 585,\n",
       " 'excema': 586,\n",
       " 'execs': 587,\n",
       " 'farish': 588,\n",
       " 'kin': 589,\n",
       " 'implication': 590,\n",
       " 'hindsight': 591,\n",
       " 'neurotheology': 592,\n",
       " 'aspect': 593,\n",
       " 'insofar': 594,\n",
       " 'shattered': 595,\n",
       " 'reliving': 596,\n",
       " 'vanity': 597,\n",
       " 'cape': 598,\n",
       " 'cardinality': 599,\n",
       " 'v': 600,\n",
       " 'steroid': 601,\n",
       " 'parcel': 602,\n",
       " 'sliver': 603,\n",
       " 'business': 604,\n",
       " 'holds': 605,\n",
       " 'nudged': 606,\n",
       " 'currents': 607,\n",
       " 'petered': 608,\n",
       " 'sheltering': 609,\n",
       " 'tap': 610,\n",
       " 'pcr': 611,\n",
       " 'petitions': 612,\n",
       " 'suppressing': 613,\n",
       " 'maraganore': 614,\n",
       " 'mckirdie': 615,\n",
       " 'attentive': 616,\n",
       " 'thames': 617,\n",
       " 'hailing': 618,\n",
       " 'vintages': 619,\n",
       " 'payload': 620,\n",
       " 'eeg': 621,\n",
       " 'valet': 622,\n",
       " 'fatherly': 623,\n",
       " 'grapegrower': 624,\n",
       " 'xena': 625,\n",
       " 'vampires': 626,\n",
       " 'beauty': 627,\n",
       " 'redder': 628,\n",
       " 'period': 629,\n",
       " 'mags': 630,\n",
       " 'octavia': 631,\n",
       " 'castle': 632,\n",
       " 'appledore': 633,\n",
       " 'interbred': 634,\n",
       " 'biotech': 635,\n",
       " '2603': 636,\n",
       " 'enjoyed': 637,\n",
       " 'breaths': 638,\n",
       " 'yabby': 639,\n",
       " 'subdivided': 640,\n",
       " 'span': 641,\n",
       " 'fold': 642,\n",
       " 'oversaw': 643,\n",
       " 'tugs': 644,\n",
       " 'ionise': 645,\n",
       " 'cave': 646,\n",
       " 'tavares': 647,\n",
       " 'lounging': 648,\n",
       " 'propofol': 649,\n",
       " 'fry': 650,\n",
       " 'whiff': 651,\n",
       " 'plundered': 652,\n",
       " 'prozac': 653,\n",
       " 'lyrics': 654,\n",
       " 'upsets': 655,\n",
       " 'murcott': 656,\n",
       " 'clades': 657,\n",
       " 'glanville': 658,\n",
       " 'dot': 659,\n",
       " 'decrease': 660,\n",
       " 'chi': 661,\n",
       " 'warplanes': 662,\n",
       " 'cans': 663,\n",
       " 'sandstorms': 664,\n",
       " 'tropical': 665,\n",
       " 'matrices': 666,\n",
       " 'operates': 667,\n",
       " 'shutdowns': 668,\n",
       " 'doney': 669,\n",
       " 'olson': 670,\n",
       " 'rate': 671,\n",
       " 'endanger': 672,\n",
       " 'mccaffrey': 673,\n",
       " 'judy': 674,\n",
       " 'carcase': 675,\n",
       " 'duels': 676,\n",
       " 'stevensen': 677,\n",
       " 'blown': 678,\n",
       " 'sharp': 679,\n",
       " 'berkshire': 680,\n",
       " 'slate': 681,\n",
       " 'considerably': 682,\n",
       " 'perverse': 683,\n",
       " 'aid': 684,\n",
       " 'behold': 685,\n",
       " 'policy': 686,\n",
       " 'sugars': 687,\n",
       " 'diffraction': 688,\n",
       " 'relation': 689,\n",
       " 'progression': 690,\n",
       " 'slumping': 691,\n",
       " 'acuity': 692,\n",
       " 'contextual': 693,\n",
       " 'adopt': 694,\n",
       " 'en': 695,\n",
       " 'spectacles': 696,\n",
       " 'desalinate': 697,\n",
       " 'tears': 698,\n",
       " 'disabilities': 699,\n",
       " 'occupations': 700,\n",
       " 'experiencing': 701,\n",
       " 'lymphoid': 702,\n",
       " 'internal': 703,\n",
       " 'emotion': 704,\n",
       " 'ruling': 705,\n",
       " 'permeability': 706,\n",
       " 'widening': 707,\n",
       " 'abi': 708,\n",
       " 'eastwell': 709,\n",
       " 'crn': 710,\n",
       " 'earthy': 711,\n",
       " 'jiulongshan': 712,\n",
       " 'improved': 713,\n",
       " 'slightly': 714,\n",
       " 'maud': 715,\n",
       " 'mulled': 716,\n",
       " 'stores': 717,\n",
       " 'grainbelt': 718,\n",
       " 'ing': 719,\n",
       " 'waterholders': 720,\n",
       " 'junee': 721,\n",
       " 'lend': 722,\n",
       " 'et': 723,\n",
       " 'tariana': 724,\n",
       " 'determinants': 725,\n",
       " 'deprivation': 726,\n",
       " 'henner': 727,\n",
       " 'withholding': 728,\n",
       " 'anthem': 729,\n",
       " 'route': 730,\n",
       " 'bans': 731,\n",
       " 'photovoltaic': 732,\n",
       " 'zenith': 733,\n",
       " 'produced': 734,\n",
       " 'darkness': 735,\n",
       " 'witchcraft': 736,\n",
       " 'brawn': 737,\n",
       " 'refused': 738,\n",
       " 'tightly': 739,\n",
       " 'microengineered': 740,\n",
       " 'guerrilla': 741,\n",
       " 'squares': 742,\n",
       " 'us14': 743,\n",
       " 'rumour': 744,\n",
       " 'assumed': 745,\n",
       " 'baker': 746,\n",
       " 'meyers': 747,\n",
       " 'wolves': 748,\n",
       " 'racegoers': 749,\n",
       " 'tipsy': 750,\n",
       " 'stacking': 751,\n",
       " 'questioned': 752,\n",
       " 'hollow': 753,\n",
       " 'rap': 754,\n",
       " 'fading': 755,\n",
       " 'biomimetic': 756,\n",
       " 'moves': 757,\n",
       " '62k': 758,\n",
       " 'confronting': 759,\n",
       " 'allegations': 760,\n",
       " 'humanely': 761,\n",
       " 'dens': 762,\n",
       " 'crimes': 763,\n",
       " 'benches': 764,\n",
       " '726': 765,\n",
       " 'tzi': 766,\n",
       " '5mls': 767,\n",
       " 'progenetica': 768,\n",
       " 'silo': 769,\n",
       " 'dressings': 770,\n",
       " 'kathy': 771,\n",
       " 'mcgarry': 772,\n",
       " 'moa': 773,\n",
       " 'herbert': 774,\n",
       " 'statutory': 775,\n",
       " 'elens': 776,\n",
       " 'bates': 777,\n",
       " 'persecutory': 778,\n",
       " 'mazzatorta': 779,\n",
       " '164': 780,\n",
       " 'smut': 781,\n",
       " 'tokamak': 782,\n",
       " 'buoyant': 783,\n",
       " 'secrete': 784,\n",
       " 'puffs': 785,\n",
       " 'billy': 786,\n",
       " 'progressed': 787,\n",
       " 'mushotzky': 788,\n",
       " 'adjust': 789,\n",
       " 'addressing': 790,\n",
       " 'juxtapositioning': 791,\n",
       " 'impassable': 792,\n",
       " 'identifies': 793,\n",
       " 'rotstayn': 794,\n",
       " 'posters': 795,\n",
       " 'inbuilt': 796,\n",
       " 'finalises': 797,\n",
       " 'rodeos': 798,\n",
       " 'catherine': 799,\n",
       " 'confers': 800,\n",
       " 'insurer': 801,\n",
       " 'ridding': 802,\n",
       " 'valance': 803,\n",
       " 'tokenistic': 804,\n",
       " 'controversial': 805,\n",
       " 'decimating': 806,\n",
       " 'marginalised': 807,\n",
       " 'whitwell': 808,\n",
       " 'capsules': 809,\n",
       " 'past': 810,\n",
       " 'witnessing': 811,\n",
       " 'compassion': 812,\n",
       " 'anecdotal': 813,\n",
       " 'soared': 814,\n",
       " 'homeric': 815,\n",
       " 'prihantoro': 816,\n",
       " 'shapes': 817,\n",
       " 'carmakers': 818,\n",
       " 'databank': 819,\n",
       " 'resettle': 820,\n",
       " 'drugs': 821,\n",
       " 'facilitates': 822,\n",
       " 'falter': 823,\n",
       " 'sphere': 824,\n",
       " 'carnival': 825,\n",
       " 'cataclysm': 826,\n",
       " 'information': 827,\n",
       " 'waratah': 828,\n",
       " 'rewiring': 829,\n",
       " 'cosmetics': 830,\n",
       " 'monotremes': 831,\n",
       " 'eddie': 832,\n",
       " 'bachner': 833,\n",
       " 'amprion': 834,\n",
       " '751': 835,\n",
       " 'homosexual': 836,\n",
       " 'ordeal': 837,\n",
       " 'modelling': 838,\n",
       " 'largish': 839,\n",
       " 'spits': 840,\n",
       " 'cults': 841,\n",
       " 'transformation': 842,\n",
       " 'tennessee': 843,\n",
       " 'including': 844,\n",
       " 'cutting': 845,\n",
       " 'dundee': 846,\n",
       " 'wearers': 847,\n",
       " 'botanic': 848,\n",
       " 'provocation': 849,\n",
       " '1882': 850,\n",
       " 'hydroponically': 851,\n",
       " 'pigd': 852,\n",
       " 'claridge': 853,\n",
       " 'weaning': 854,\n",
       " 'rated': 855,\n",
       " 'capitals': 856,\n",
       " 'augusts': 857,\n",
       " 'wanda': 858,\n",
       " 'schembri': 859,\n",
       " 'transform': 860,\n",
       " 'charge': 861,\n",
       " 'tullow': 862,\n",
       " 'connecticut': 863,\n",
       " 'flattered': 864,\n",
       " 'resumes': 865,\n",
       " 'timed': 866,\n",
       " 'juiced': 867,\n",
       " 'stargazers': 868,\n",
       " 'pearson': 869,\n",
       " 'interior': 870,\n",
       " 'butchering': 871,\n",
       " 'successes': 872,\n",
       " 'detect': 873,\n",
       " 'reserves': 874,\n",
       " 'grafted': 875,\n",
       " 'pinot': 876,\n",
       " 'tidied': 877,\n",
       " 'ewe': 878,\n",
       " 'governance': 879,\n",
       " 'reflexes': 880,\n",
       " 'america': 881,\n",
       " 'antennas': 882,\n",
       " 'waterlogged': 883,\n",
       " 'ticklish': 884,\n",
       " 'inspection': 885,\n",
       " 'cressy': 886,\n",
       " 'projection': 887,\n",
       " 'find': 888,\n",
       " 'egan': 889,\n",
       " 'bridle': 890,\n",
       " 'politics': 891,\n",
       " 'cooks': 892,\n",
       " 'clones': 893,\n",
       " 'discoverer': 894,\n",
       " 'watson': 895,\n",
       " 'calculable': 896,\n",
       " 'bioplastic': 897,\n",
       " 'dangers': 898,\n",
       " 'hinchcliff': 899,\n",
       " 'blurring': 900,\n",
       " 'plasma': 901,\n",
       " 'kuroshio': 902,\n",
       " 'earns': 903,\n",
       " 'took': 904,\n",
       " 'bolivar': 905,\n",
       " 'tamra': 906,\n",
       " 'legacy': 907,\n",
       " 'dunstan': 908,\n",
       " 'insurrection': 909,\n",
       " 'particle': 910,\n",
       " 'wallwork': 911,\n",
       " 'sprinklers': 912,\n",
       " 'salinity': 913,\n",
       " 'allsopp': 914,\n",
       " 'immense': 915,\n",
       " 'hitch': 916,\n",
       " 'prosthetic': 917,\n",
       " 'interpreted': 918,\n",
       " 'grader': 919,\n",
       " 'kate': 920,\n",
       " 'cabling': 921,\n",
       " 'haire': 922,\n",
       " 'actor': 923,\n",
       " 'smokers': 924,\n",
       " 'polycyclic': 925,\n",
       " 'ive': 926,\n",
       " 'anna': 927,\n",
       " 'princess': 928,\n",
       " 'marisela': 929,\n",
       " 'plantations': 930,\n",
       " 'papagrigorakis': 931,\n",
       " 'went': 932,\n",
       " 'proclivity': 933,\n",
       " 'died': 934,\n",
       " 'swing': 935,\n",
       " 'imager': 936,\n",
       " 'colonise': 937,\n",
       " '%.\"': 938,\n",
       " 'joseph': 939,\n",
       " 'braat': 940,\n",
       " 'representative': 941,\n",
       " 'arbiter': 942,\n",
       " 'us65c': 943,\n",
       " 'solomon': 944,\n",
       " 'portray': 945,\n",
       " 'force': 946,\n",
       " 'coomboona': 947,\n",
       " 'cutback': 948,\n",
       " 'drummond': 949,\n",
       " 'gale': 950,\n",
       " 'talents': 951,\n",
       " 'fletcher': 952,\n",
       " 'colliery': 953,\n",
       " 'doron': 954,\n",
       " 'vocalisation': 955,\n",
       " 'isisfordia': 956,\n",
       " '7077': 957,\n",
       " 'wilder': 958,\n",
       " '124': 959,\n",
       " 'perilously': 960,\n",
       " 'androgynous': 961,\n",
       " 'lusis': 962,\n",
       " 'attentional': 963,\n",
       " 'wholefoods': 964,\n",
       " 'steele': 965,\n",
       " 'analgesic': 966,\n",
       " 'principals': 967,\n",
       " 'freehold': 968,\n",
       " 'walkways': 969,\n",
       " 'plunging': 970,\n",
       " 'scooped': 971,\n",
       " 'rockets': 972,\n",
       " 'grassland': 973,\n",
       " 'lieske': 974,\n",
       " 'possibly': 975,\n",
       " 'downer': 976,\n",
       " 'muchea': 977,\n",
       " 'vastly': 978,\n",
       " 'fibre': 979,\n",
       " 'tsuhan': 980,\n",
       " 'artifically': 981,\n",
       " 'balco': 982,\n",
       " 'winegrape': 983,\n",
       " 'burridge': 984,\n",
       " 'carers': 985,\n",
       " 'siv': 986,\n",
       " 'focuses': 987,\n",
       " 'monstrous': 988,\n",
       " 'rutgers': 989,\n",
       " 'pent': 990,\n",
       " 'backside': 991,\n",
       " 'orgasm': 992,\n",
       " 'sussex': 993,\n",
       " 'offending': 994,\n",
       " 'strewn': 995,\n",
       " 'descriptions': 996,\n",
       " 'slice': 997,\n",
       " 'savoury': 998,\n",
       " 'dermatological': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#numericalize the vocabs\n",
    "word2index = {w: i for i, w in enumerate(vocabs)}\n",
    "\n",
    "word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27445"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#append <UNK>\n",
    "vocabs.append('<UNK>')\n",
    "word2index['<UNK>'] = 27444\n",
    "\n",
    "len(word2index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the frequency of each word\n",
    "from collections import Counter\n",
    "\n",
    "X_i = Counter(flatten(corpus_tokenized))\n",
    "\n",
    "X_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a skipgram of window size 1\n",
    "skip_grams = []\n",
    "\n",
    "for sent in corpus_tokenized:\n",
    "    for i in range(1, len(sent)-1):\n",
    "        target  = sent[i]\n",
    "        context = [sent[i+1], sent[i-1]]\n",
    "        for c in context:\n",
    "            skip_grams.append((target, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the co-occurrence matrix\n",
    "X_ik_skipgram = Counter(skip_grams)\n",
    "X_ik_skipgram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Weighting function f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting(w_i, w_j, X_ik):\n",
    "    \n",
    "    #check whether the co-occurrences between these two word exists???\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "    except:\n",
    "        x_ij = 1  #why one, so that the probability thingy won't break...(label smoothing)\n",
    "        \n",
    "    #maximum co-occurrences; we follow the paper\n",
    "    x_max = 100\n",
    "    alpha = 0.75\n",
    "    \n",
    "    #if the co-occurrences does not exceed x_max, scale it down based on some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij/x_max) ** alpha\n",
    "    else:\n",
    "        result = 1 #this is the maximum probability you can have\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_i  = 'democracy'\n",
    "w_j  = 'freedom'\n",
    "\n",
    "print(weighting(w_i, w_j, X_ik_skipgram))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now apply this weighting to all possible pairs\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {} #for keeping the co-occurrences\n",
    "weighting_dic = {} #for keeping all the probability after passing through the weighting function\n",
    "\n",
    "for bigram in combinations_with_replacement(vocabs, 2):  #we need to also think its reverse\n",
    "    #if this bigram exists in X_ik_skipgrams\n",
    "    #we gonna add this to our co-occurence matrix\n",
    "    if X_ik_skipgram.get(bigram) is not None:\n",
    "        cooc = X_ik_skipgram[bigram]  #get the co-occurrence\n",
    "        X_ik[bigram] = cooc + 1 #this is again basically label smoothing....(stability issues (especially when divide something))\n",
    "        X_ik[(bigram[1], bigram[0])] = cooc + 1  #trick to get all pairs\n",
    "    else: #otherwise, do nothing\n",
    "        pass\n",
    "    \n",
    "    #apply the weighting function using this co-occurrence matrix thingy    \n",
    "    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_ik_skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighting_dic  #give small probability to never-occurred is called \"label smoothing\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare train data\n",
    "You move the window along, and create those tuples as we said in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in corpus_tokenized:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def random_batch(batch_size, word_sequence, skip_grams, X_ik, weighting_dic):\n",
    "    \n",
    "    #loop through this skipgram, and change it id  because when sending model, it must number\n",
    "    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    \n",
    "    #randomly pick \"batch_size\" indexes\n",
    "    number_of_choices = len(skip_grams_id)\n",
    "    random_index = np.random.choice(number_of_choices, batch_size, replace=False) #no repeating indexes among these random indexes\n",
    "    \n",
    "    random_inputs = [] #xi, wi (in batches)\n",
    "    random_labels = [] #xj, wj (in batches)\n",
    "    random_coocs  = [] #Xij (in batches)\n",
    "    random_weighting = [] #f(Xij) (in batches)\n",
    "    #for each of the sample in these indexes\n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams_id[i][0]]) #same reason why i put bracket here....\n",
    "        random_labels.append([skip_grams_id[i][1]])\n",
    "        \n",
    "        #get cooc\n",
    "        #first check whether it exists...\n",
    "        pair = skip_grams[i]  #e.g., ('banana', 'fruit)\n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1 #label smoothing\n",
    "            \n",
    "        random_coocs.append([math.log(cooc)])  #1. why log, #2, why bracket -> size ==> (, 1)  #my neural network expects (, 1)\n",
    "        \n",
    "        #get weighting\n",
    "        weighting = weighting_dic[pair]  #why not use try....maybe it does not exist....\n",
    "        random_weighting.append(weighting)\n",
    "\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weighting)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "input, target, cooc, weightin = random_batch(batch_size, corpus_tokenized, skip_grams, X_ik, weighting_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, target, cooc, weightin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model\n",
    "\n",
    "<img src =\"../figures/glove.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size,embed_size):\n",
    "        super(GloVe,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, embed_size) # center embedding\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size) # out embedding\n",
    "        \n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "        \n",
    "    def forward(self, center_words, target_words, coocs, weighting):\n",
    "        center_embeds = self.embedding_v(center_words) # [batch_size, 1, emb_size]\n",
    "        target_embeds = self.embedding_u(target_words) # [batch_size, 1, emb_size]\n",
    "        \n",
    "        center_bias = self.v_bias(center_words).squeeze(1)\n",
    "        target_bias = self.u_bias(target_words).squeeze(1)\n",
    "        \n",
    "        inner_product = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "        \n",
    "        #note that coocs already got log\n",
    "        loss = weighting*torch.pow(inner_product +center_bias + target_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size   = len(vocabs)\n",
    "batch_size = 2 #why?  no reason; \n",
    "emb_size   = 2 #why?  no reason; usually 50, 100, 300, but 2 so we can plot (50 can also plot, but need PCA)\n",
    "model      = GloVe(voc_size, emb_size)\n",
    "\n",
    "optimizer  = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5000\n",
    "#for epoch\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #get random batch\n",
    "    input, target, cooc, weightin = random_batch(batch_size, corpus_tokenized, skip_grams, X_ik, weighting_dic)\n",
    "    input_batch    = torch.LongTensor(input)\n",
    "    target_batch   = torch.LongTensor(target)\n",
    "    cooc_batch     = torch.FloatTensor(cooc)\n",
    "    weightin_batch = torch.FloatTensor(weightin)\n",
    "    \n",
    "    \n",
    "    # print(input_batch.shape, label_batch.shape, cooc_batch.shape, weightin_batch)\n",
    "    \n",
    "    #loss = model\n",
    "    loss = model(input_batch, target_batch, cooc_batch, weightin_batch)\n",
    "    \n",
    "    #backpropagate\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print epoch loss\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss:.6f} | Time: ??\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot the embeddings\n",
    "\n",
    "Is really the related stuff are close to each other, and vice versa?\n",
    "\n",
    "The most fun part:  Will \"banana\" closer to \"fruit\" than \"cat\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banana = torch.LongTensor([word2index['banana']])\n",
    "banana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banana_center_embed = model.embedding_v(banana)\n",
    "banana_outisde_embed = model.embedding_u(banana)\n",
    "\n",
    "banana_embed = (banana_center_embed + banana_outisde_embed) / 2\n",
    "banana_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(word):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except:\n",
    "        index = word2index['<UNK>']\n",
    "    \n",
    "    word = torch.LongTensor([index])\n",
    "\n",
    "    center_embed  = model.embedding_v(word)\n",
    "    outside_embed = model.embedding_u(word)\n",
    "    \n",
    "    embed = (center_embed + outside_embed) / 2\n",
    "    \n",
    "    return  embed[0][0].item(), embed[0][1].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find embedding of fruit, cat\n",
    "print(get_embed('fruit'))\n",
    "print(get_embed('cat'))\n",
    "print(get_embed('chaky'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help me plot fruit cat banana on matplotlib\n",
    "plt.figure(figsize=(6,3))\n",
    "for i, word in enumerate(vocabs[:20]): #loop each unique vocab\n",
    "    x, y = get_embed(word)\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(word, xy=(x, y), xytext=(5, 2), textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cosine similarity\n",
    "\n",
    "How do (from scratch) calculate cosine similarity?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7ca07084f99cae884d00a2401f5a915152405bd446c47609c53897335e04337"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
