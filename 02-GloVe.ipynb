{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19 Jan - GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the training corpus\n",
    "#I use the Inaugural Address Corpus from NLTK\n",
    "import nltk\n",
    "from nltk.corpus import inaugural\n",
    "corpus = nltk.corpus.inaugural.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the', 'House', 'of', 'Representatives', ':'], ['Among', 'the', 'vicissitudes', 'incident', 'to', 'life', 'no', 'event', 'could', 'have', 'filled', 'me', 'with', 'greater', 'anxieties', 'than', 'that', 'of', 'which', 'the', 'notification', 'was', 'transmitted', 'by', 'your', 'order', ',', 'and', 'received', 'on', 'the', '14th', 'day', 'of', 'the', 'present', 'month', '.'], ['On', 'the', 'one', 'hand', ',', 'I', 'was', 'summoned', 'by', 'my', 'Country', ',', 'whose', 'voice', 'I', 'can', 'never', 'hear', 'but', 'with', 'veneration', 'and', 'love', ',', 'from', 'a', 'retreat', 'which', 'I', 'had', 'chosen', 'with', 'the', 'fondest', 'predilection', ',', 'and', ',', 'in', 'my', 'flattering', 'hopes', ',', 'with', 'an', 'immutable', 'decision', ',', 'as', 'the', 'asylum', 'of', 'my', 'declining', 'years', '--', 'a', 'retreat', 'which', 'was', 'rendered', 'every', 'day', 'more', 'necessary', 'as', 'well', 'as', 'more', 'dear', 'to', 'me', 'by', 'the', 'addition', 'of', 'habit', 'to', 'inclination', ',', 'and', 'of', 'frequent', 'interruptions', 'in', 'my', 'health', 'to', 'the', 'gradual', 'waste', 'committed', 'on', 'it', 'by', 'time', '.']]\n"
     ]
    }
   ],
   "source": [
    "#the corpus is already tokenized\n",
    "print(corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fellow', '-', 'citizens', 'of', 'the', 'senate', 'and', 'of', 'the', 'house', 'of', 'representatives', ':'], ['among', 'the', 'vicissitudes', 'incident', 'to', 'life', 'no', 'event', 'could', 'have', 'filled', 'me', 'with', 'greater', 'anxieties', 'than', 'that', 'of', 'which', 'the', 'notification', 'was', 'transmitted', 'by', 'your', 'order', ',', 'and', 'received', 'on', 'the', '14th', 'day', 'of', 'the', 'present', 'month', '.'], ['on', 'the', 'one', 'hand', ',', 'i', 'was', 'summoned', 'by', 'my', 'country', ',', 'whose', 'voice', 'i', 'can', 'never', 'hear', 'but', 'with', 'veneration', 'and', 'love', ',', 'from', 'a', 'retreat', 'which', 'i', 'had', 'chosen', 'with', 'the', 'fondest', 'predilection', ',', 'and', ',', 'in', 'my', 'flattering', 'hopes', ',', 'with', 'an', 'immutable', 'decision', ',', 'as', 'the', 'asylum', 'of', 'my', 'declining', 'years', '--', 'a', 'retreat', 'which', 'was', 'rendered', 'every', 'day', 'more', 'necessary', 'as', 'well', 'as', 'more', 'dear', 'to', 'me', 'by', 'the', 'addition', 'of', 'habit', 'to', 'inclination', ',', 'and', 'of', 'frequent', 'interruptions', 'in', 'my', 'health', 'to', 'the', 'gradual', 'waste', 'committed', 'on', 'it', 'by', 'time', '.']]\n"
     ]
    }
   ],
   "source": [
    "#convert the words in the corpus into lower case\n",
    "corpus_tokenized = [[]] * len(corpus)\n",
    "for i in range(len(corpus)):\n",
    "    corpus_tokenized[i] = [word.lower() for word in corpus[i]]\n",
    "\n",
    "print(corpus_tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fellow', '-', 'citizens', 'senate', 'house', 'representatives', ':'], ['vicissitudes', 'incident', 'life', 'event', 'filled', 'greater', 'anxieties', 'notification', 'transmitted', 'order', ',', 'received', '14th', 'day', 'present', 'month', '.'], ['hand', ',', 'summoned', 'country', ',', 'voice', 'hear', 'veneration', 'love', ',', 'retreat', 'chosen', 'fondest', 'predilection', ',', ',', 'flattering', 'hopes', ',', 'immutable', 'decision', ',', 'asylum', 'declining', 'years', '--', 'retreat', 'rendered', 'day', 'necessary', 'dear', 'addition', 'habit', 'inclination', ',', 'frequent', 'interruptions', 'health', 'gradual', 'waste', 'committed', 'time', '.']]\n"
     ]
    }
   ],
   "source": [
    "#remove stop words with SpaCy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "for sentence in corpus_tokenized:\n",
    "    for word in sentence[:]:\n",
    "        if word in stopwords:\n",
    "            sentence.remove(word)\n",
    "\n",
    "print(corpus_tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fellow', 'citizens', 'senate', 'house', 'representatives'], ['vicissitudes', 'incident', 'life', 'event', 'filled', 'greater', 'anxieties', 'notification', 'transmitted', 'order', 'received', '14th', 'day', 'present', 'month'], ['hand', 'summoned', 'country', 'voice', 'hear', 'veneration', 'love', 'retreat', 'chosen', 'fondest', 'predilection', 'flattering', 'hopes', 'immutable', 'decision', 'asylum', 'declining', 'years', '--', 'retreat', 'rendered', 'day', 'necessary', 'dear', 'addition', 'habit', 'inclination', 'frequent', 'interruptions', 'health', 'gradual', 'waste', 'committed', 'time']]\n"
     ]
    }
   ],
   "source": [
    "#remove punctutations with String\n",
    "import string\n",
    "punctutations = string.punctuation\n",
    "\n",
    "for sentence in corpus_tokenized:\n",
    "    for word in sentence[:]:\n",
    "        if word in punctutations:\n",
    "            sentence.remove(word)\n",
    "\n",
    "print(corpus_tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fellow', 'citizens', 'senate', 'house', 'representatives'], ['vicissitudes', 'incident', 'life', 'event', 'filled', 'greater', 'anxieties', 'notification', 'transmitted', 'order', 'received', '14th', 'day', 'present', 'month'], ['hand', 'summoned', 'country', 'voice', 'hear', 'veneration', 'love', 'retreat', 'chosen', 'fondest', 'predilection', 'flattering', 'hopes', 'immutable', 'decision', 'asylum', 'declining', 'years', 'retreat', 'rendered', 'day', 'necessary', 'dear', 'addition', 'habit', 'inclination', 'frequent', 'interruptions', 'health', 'gradual', 'waste', 'committed', 'time']]\n"
     ]
    }
   ],
   "source": [
    "#remove '--'\n",
    "for sentence in corpus_tokenized:\n",
    "    for word in sentence[:]:\n",
    "        if word == '--':\n",
    "            sentence.remove(word)\n",
    "\n",
    "print(corpus_tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9019\n"
     ]
    }
   ],
   "source": [
    "#get unique words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabs = list(set(flatten(corpus_tokenized)))\n",
    "\n",
    "print(len(vocabs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9019"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#numericalize the vocabs\n",
    "word2index = {w: i for i, w in enumerate(vocabs)}\n",
    "\n",
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9020"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#append <UNK>\n",
    "vocabs.append('<UNK>')\n",
    "word2index['<UNK>'] = 9019\n",
    "\n",
    "len(word2index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GloVe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Co-occurence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count the frequency of each word\n",
    "from collections import Counter\n",
    "\n",
    "X_i = Counter(flatten(corpus_tokenized))\n",
    "\n",
    "X_i['fellow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate skipgrams with a generic window size\n",
    "def generate_skip_gram(window_size): \n",
    "    skip_grams = []\n",
    "    for sentence in corpus_tokenized:\n",
    "        #I include the first and last words\n",
    "        #so that every word could be a center word\n",
    "        for i in range(len(sentence)):\n",
    "            center = sentence[i]\n",
    "            context = []\n",
    "            for j in range(window_size):\n",
    "                if (i - window_size + j) >= 0:\n",
    "                    context.append(sentence[i - window_size + j])\n",
    "            for k in range(1, window_size + 1):\n",
    "                if (i + k) < len(sentence):\n",
    "                    context.append(sentence[i + k])\n",
    "            for w in context:\n",
    "                skip_grams.append((center, w))\n",
    "        \n",
    "    return skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fellow', 'citizens'),\n",
       " ('fellow', 'senate'),\n",
       " ('citizens', 'fellow'),\n",
       " ('citizens', 'senate'),\n",
       " ('citizens', 'house')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare skipgrams with window size of 2\n",
    "skip_grams = generate_skip_gram(2)\n",
    "\n",
    "skip_grams[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175374"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count co-occurences in the skipgrams\n",
    "X_ik_skipgram = Counter(skip_grams)\n",
    "\n",
    "len(X_ik_skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(X_ik_skipgram[('fellow', 'citizens')])\n",
    "print(X_ik_skipgram[('fellow', 'communists')])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Weighting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the weighting function\n",
    "def weighting(w_i, w_j, X_ik):\n",
    "    \n",
    "    x_ij = X_ik[(w_i, w_j)]\n",
    "\n",
    "    #label smoothing if there is no co-occurence (i.e., x_ij is 0)\n",
    "    if x_ij == 0:\n",
    "        x_ij = 1\n",
    "        \n",
    "    #maximum co-occurrences is 100 according to the paper\n",
    "    x_max = 100\n",
    "    alpha = 0.75\n",
    "    \n",
    "    #if the co-occurrences does not exceed x_max, scale it down based on some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij/x_max) ** alpha\n",
    "    else:\n",
    "        result = 1 #the maximum probability\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#test the weighting function\n",
    "w_i  = 'fellow'\n",
    "w_j  = 'citizens'\n",
    "\n",
    "print(weighting(w_i, w_j, X_ik_skipgram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03162277660168379\n",
      "0.03162277660168379\n"
     ]
    }
   ],
   "source": [
    "w_i  = 'fellow'\n",
    "w_j  = 'communists'\n",
    "\n",
    "print(weighting(w_i, w_j, X_ik_skipgram))\n",
    "print((1 / 100) ** 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply this weighting to all possible pairs\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {} #for keeping the co-occurrences\n",
    "weighting_dic = {} #for keeping all the probabilities after passing through the weighting function\n",
    "\n",
    "for bigram in combinations_with_replacement(vocabs, 2):  #we need to also think its reverse\n",
    "    #if this bigram exists in X_ik_skipgrams\n",
    "    #we gonna add this to our co-occurence matrix\n",
    "    if X_ik_skipgram.get(bigram) is not None:\n",
    "        cooc = X_ik_skipgram[bigram]  #get the co-occurrence\n",
    "        X_ik[bigram] = cooc + 1 #this is again basically label smoothing\n",
    "        X_ik[(bigram[1], bigram[0])] = cooc + 1  #trick to get all pairs\n",
    "    else: #otherwise, put 0\n",
    "        X_ik[bigram] = 0\n",
    "        X_ik[(bigram[1], bigram[0])] = 0\n",
    "\n",
    "    #apply the weighting function using this co-occurrence matrix thingy    \n",
    "    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "0.10573712634405642\n",
      "0.10573712634405642\n",
      "0.10573712634405642\n"
     ]
    }
   ],
   "source": [
    "#test the weighting function\n",
    "print(X_ik_skipgram[('senate', 'house')])\n",
    "print(X_ik_skipgram[('house', 'senate')])\n",
    "\n",
    "print(X_ik[('senate', 'house')])\n",
    "print(X_ik[('house', 'senate')])\n",
    "\n",
    "print(weighting_dic[('senate', 'house')])\n",
    "print(weighting_dic[('house', 'senate')])\n",
    "\n",
    "print((5 / 100) ** 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0.03162277660168379\n",
      "0.03162277660168379\n"
     ]
    }
   ],
   "source": [
    "print(X_ik_skipgram[('communists', 'communists')])\n",
    "print(X_ik[('communists', 'communists')])\n",
    "print(weighting_dic[('communists', 'communists')])\n",
    "print((1 / 100) ** 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(weighting_dic[('fellow', 'citizens')])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random batch for GloVe with generic batch size, corpus and skipgrams\n",
    "import math\n",
    "\n",
    "def random_batch_glove(batch_size, corpus, skip_grams, X_ik, weighting_dic):\n",
    "    \n",
    "    #change words in the skipgrams to idices\n",
    "    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    \n",
    "    #randomly pick \"batch_size\" indices\n",
    "    number_of_choices = len(skip_grams_id)\n",
    "    random_index = np.random.choice(number_of_choices, batch_size, replace=False) #no repeating indexes among these random indexes\n",
    "    \n",
    "    random_inputs = [] #xi, wi (in batches)\n",
    "    random_labels = [] #xj, wj (in batches)\n",
    "    random_coocs  = [] #Xij (in batches)\n",
    "    random_weightings = [] #weighting_dic(Xij) (in batches)\n",
    "    #for each of the sample in these indexes\n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams_id[i][0]])\n",
    "        random_labels.append([skip_grams_id[i][1]])\n",
    "        \n",
    "        #get cooc\n",
    "        #first check whether it exists...\n",
    "        pair = skip_grams[i]\n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1 #label smoothing\n",
    "            \n",
    "        random_coocs.append([math.log(cooc)])\n",
    "        #log according to the cost function equation\n",
    "        #bracket because neural network requires size ( , 1)\n",
    "        \n",
    "        #get weighting\n",
    "        weighting = weighting_dic[pair]\n",
    "        random_weightings.append(weighting)\n",
    "\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[4483],\n",
       "        [5031]]),\n",
       " array([[6190],\n",
       "        [ 511]]),\n",
       " array([[0.69314718],\n",
       "        [0.69314718]]),\n",
       " array([0.05318296, 0.05318296]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the method\n",
    "batch_size = 2\n",
    "inputs, targets, coocs, weightings = random_batch_glove(batch_size, corpus_tokenized, skip_grams, X_ik, weighting_dic)\n",
    "\n",
    "inputs, targets, coocs, weightings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size,embed_size):\n",
    "        super(GloVe,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, embed_size) # center embedding\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size) # context embedding\n",
    "        \n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "        \n",
    "    def forward(self, center_words, context_words, coocs, weightings):\n",
    "        center_embeds = self.embedding_v(center_words) # [batch_size, 1, emb_size]\n",
    "        context_embeds = self.embedding_u(context_words) # [batch_size, 1, emb_size]\n",
    "        \n",
    "        center_bias = self.v_bias(center_words).squeeze(1)\n",
    "        context_bias = self.u_bias(context_words).squeeze(1)\n",
    "        \n",
    "        inner_product = context_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "        \n",
    "        #note that coocs is already log\n",
    "        loss = weightings * torch.pow(inner_product + center_bias + context_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set training parameters\n",
    "batch_size  = 10\n",
    "vocab_size  = len(vocabs)\n",
    "emb_size    = 50\n",
    "model       = GloVe(vocab_size, emb_size)\n",
    "optimizer   = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate epoch time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time // 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | cost: 442.558044 | time: 0m 12s\n",
      "Epoch: 200 | cost: 273.391937 | time: 0m 23s\n",
      "Epoch: 300 | cost: 483.341827 | time: 0m 34s\n",
      "Epoch: 400 | cost: 171.230286 | time: 0m 45s\n",
      "Epoch: 500 | cost: 66.754929 | time: 0m 55s\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "import time\n",
    "\n",
    "num_epochs = 500\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    inputs, targets, coocs, weightings = random_batch_glove(batch_size, corpus_tokenized, skip_grams, X_ik, weighting_dic)\n",
    "    input_batch     = torch.LongTensor(inputs)\n",
    "    target_batch    = torch.LongTensor(targets)\n",
    "    cooc_batch      = torch.FloatTensor(coocs)\n",
    "    weighting_batch = torch.FloatTensor(weightings)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(input_batch, target_batch, cooc_batch, weighting_batch)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        end = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6. Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get embedding\n",
    "def get_embed(word):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except:\n",
    "        index = word2index['<UNK>']\n",
    "    \n",
    "    word = torch.LongTensor([index])\n",
    "\n",
    "    center_embed  = model.embedding_v(word)\n",
    "    context_embed = model.embedding_u(word)\n",
    "    \n",
    "    embed = (center_embed + context_embed) / 2\n",
    "        \n",
    "    return  embed[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select a few vocabs and get their embeddings\n",
    "liberty = get_embed('liberty')\n",
    "freedom = get_embed('freedom')\n",
    "democracy = get_embed('democracy')\n",
    "magic = get_embed('magic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define cosine similarity\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    cos_sim = dot(a, b.T)/(norm(a)*norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liberty vs. freedom:  0.015220157\n",
      "liberty vs. democracy:  0.07938496\n",
      "liberty vs. magic:  -0.1280151\n"
     ]
    }
   ],
   "source": [
    "#print cosine similarities between the selected vocabs\n",
    "print(f\"liberty vs. freedom: \", cos_sim(liberty, freedom))\n",
    "print(f\"liberty vs. democracy: \", cos_sim(liberty, democracy))\n",
    "print(f\"liberty vs. magic: \", cos_sim(liberty, magic))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random batch for CBOW model with default window size and batch size of 1 each\n",
    "def random_batch_cbow(window_size=1, batch_size=1): \n",
    "    cbow = []\n",
    "    for sentence in corpus_tokenized:\n",
    "        #I include the first and last words\n",
    "        #so that every word could be a center word\n",
    "        for i in range(len(sentence)):\n",
    "            center = word2index[sentence[i]]\n",
    "            context = []\n",
    "            context_idx = []\n",
    "            #group the indices of the context words\n",
    "            for j in range(window_size):\n",
    "                context_idx.append(i - window_size + j)\n",
    "            for k in range(1, window_size + 1):\n",
    "                context_idx.append(i + k)\n",
    "            #append the context words based on their indices\n",
    "            #append <UNK> if there is no word at an index\n",
    "            for idx in context_idx:\n",
    "                if idx < 0:\n",
    "                    context.append(word2index['<UNK>'])\n",
    "                elif idx >= len(sentence):\n",
    "                    context.append(word2index['<UNK>'])\n",
    "                else:\n",
    "                    context.append(word2index[sentence[idx]])\n",
    "            cbow.append([context, center])\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(cbow)), batch_size, replace=False)\n",
    "    for i in random_index:\n",
    "        random_inputs.append(cbow[i][0])\n",
    "        random_labels.append([cbow[i][1]])\n",
    "            \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [[6042 4555 8011 8946]\n",
      " [6414 5041 6109 5264]\n",
      " [1619 8348 3100 3456]\n",
      " [2173 8246 1232 2548]\n",
      " [5620 3773 2005 6140]\n",
      " [9019 7613 3462 5369]\n",
      " [6910 1464 6224 8474]\n",
      " [ 135 1016 4284  837]\n",
      " [1607 5031 2933 2110]\n",
      " [6327 3417 7557 5518]]\n",
      "Target:  [[8870]\n",
      " [6659]\n",
      " [6416]\n",
      " [5031]\n",
      " [7178]\n",
      " [3651]\n",
      " [ 703]\n",
      " [4367]\n",
      " [  68]\n",
      " [6206]]\n"
     ]
    }
   ],
   "source": [
    "#test the CBOW method\n",
    "input_batch, target_batch = random_batch_cbow(2, 10)\n",
    "\n",
    "print(\"Input: \", input_batch)\n",
    "print(\"Target: \", target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CBOW model\n",
    "class CBOW(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(CBOW,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, emb_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, emb_size)\n",
    "    \n",
    "    def forward(self, context_words, center_words, all_vocabs):\n",
    "        center_embeds = self.embedding_v(center_words)  #[batch_size, 1, emb_size]\n",
    "        context_embeds = self.embedding_u(context_words)  #[batch_size, window_size, emb_size]\n",
    "        all_embeds    = self.embedding_v(all_vocabs)    #[batch_size, vocab_size, emb_size]\n",
    "        \n",
    "        scores      = center_embeds.bmm(context_embeds.transpose(1, 2)).squeeze(2)\n",
    "\n",
    "        norm_scores = all_embeds.bmm(context_embeds.transpose(1, 2)).squeeze(2)\n",
    "\n",
    "        nll = -torch.mean(torch.log(torch.exp(scores)/torch.sum(torch.exp(norm_scores), 1).unsqueeze(1)))\n",
    "            \n",
    "        return nll # negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | cost: 27.911234 | time: 0m 41s\n",
      "Epoch: 200 | cost: 24.047354 | time: 1m 30s\n",
      "Epoch: 300 | cost: 23.462894 | time: 2m 27s\n",
      "Epoch: 400 | cost: 21.904446 | time: 3m 12s\n",
      "Epoch: 500 | cost: 24.435104 | time: 3m 57s\n"
     ]
    }
   ],
   "source": [
    "#set training parameters\n",
    "window_size = 2\n",
    "batch_size = 10\n",
    "vocab_size = len(vocabs)\n",
    "emb_size = 50\n",
    "model = CBOW(vocab_size, emb_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#convert all vocabs to tensors\n",
    "def prepare_sequence(vocabs, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], vocabs))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, vocab_size)\n",
    "\n",
    "#calculate epoch time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time // 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "#train the model\n",
    "import time\n",
    "\n",
    "num_epochs = 500\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    input_batch, target_batch = random_batch_cbow(window_size, batch_size)\n",
    "    input_batch  = torch.LongTensor(input_batch)  #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch) #[batch_size, 1]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(input_batch, target_batch, all_vocabs)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        end = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liberty vs. freedom:  -0.106405206\n",
      "liberty vs. democracy:  -0.04164266\n",
      "liberty vs. magic:  0.021748003\n"
     ]
    }
   ],
   "source": [
    "#select a few vocabs and get their embeddings\n",
    "liberty = get_embed('liberty')\n",
    "freedom = get_embed('freedom')\n",
    "democracy = get_embed('democracy')\n",
    "magic = get_embed('magic')\n",
    "\n",
    "#define cosine similarity\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim\n",
    "\n",
    "#print cosine similarities between the selected vocabs\n",
    "print(f\"liberty vs. freedom: \", cos_sim(liberty, freedom))\n",
    "print(f\"liberty vs. democracy: \", cos_sim(liberty, democracy))\n",
    "print(f\"liberty vs. magic: \", cos_sim(liberty, magic))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random batch for Skip-gram model with default window size and batch size of 1 each\n",
    "def random_batch_skip_gram(window_size=1, batch_size=1): \n",
    "    skip_grams = []\n",
    "    for sentence in corpus_tokenized:\n",
    "        #I include the first and last words\n",
    "        #so that every word could be a center word\n",
    "        for i in range(len(sentence)):\n",
    "            center = word2index[sentence[i]]\n",
    "            context = []\n",
    "            for j in range(window_size):\n",
    "                if (i - window_size + j) >= 0:\n",
    "                    context.append(word2index[sentence[i - window_size + j]])\n",
    "            for k in range(1, window_size + 1):\n",
    "                if (i + k) < len(sentence):\n",
    "                    context.append(word2index[sentence[i + k]])\n",
    "            for w in context:\n",
    "                skip_grams.append([center, w])\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False)\n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams[i][0]])\n",
    "        random_labels.append([skip_grams[i][1]])\n",
    "            \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [[6416]\n",
      " [4253]\n",
      " [4564]\n",
      " [6983]\n",
      " [2867]\n",
      " [2867]\n",
      " [3078]\n",
      " [8982]\n",
      " [6277]\n",
      " [2028]]\n",
      "Target:  [[4098]\n",
      " [1736]\n",
      " [6947]\n",
      " [6923]\n",
      " [4126]\n",
      " [7810]\n",
      " [ 103]\n",
      " [7151]\n",
      " [4778]\n",
      " [3180]]\n"
     ]
    }
   ],
   "source": [
    "#test the Skip-gram method\n",
    "input_batch, target_batch = random_batch_skip_gram(2, 10)\n",
    "\n",
    "print(\"Input: \", input_batch)\n",
    "print(\"Target: \", target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip-gram model\n",
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(Skipgram,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, emb_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, emb_size)\n",
    "    \n",
    "    def forward(self, center_words, context_words, all_vocabs):\n",
    "        center_embeds = self.embedding_v(center_words)  #[batch_size, 1, emb_size]\n",
    "        context_embeds = self.embedding_u(context_words)  #[batch_size, 1, emb_size]\n",
    "        all_embeds    = self.embedding_u(all_vocabs)    #[batch_size, vocab_size, emb_size]\n",
    "        \n",
    "        scores      = context_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "\n",
    "        norm_scores = all_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, vocab_size, emb_size] @ [batch_size, emb_size, 1] = [batch_size, vocab_size, 1] = [batch_size, vocab_size]\n",
    "\n",
    "        nll = -torch.mean(torch.log(torch.exp(scores)/torch.sum(torch.exp(norm_scores), 1).unsqueeze(1))) # log-softmax\n",
    "        # scalar (loss must be scalar)    \n",
    "            \n",
    "        return nll # negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | cost: 23.803759 | time: 0m 58s\n",
      "Epoch: 200 | cost: 22.742805 | time: 1m 57s\n",
      "Epoch: 300 | cost: 29.244822 | time: 2m 56s\n",
      "Epoch: 400 | cost: 25.805033 | time: 3m 58s\n",
      "Epoch: 500 | cost: 26.699390 | time: 4m 56s\n"
     ]
    }
   ],
   "source": [
    "#set training parameters\n",
    "window_size = 2\n",
    "batch_size = 10\n",
    "vocab_size = len(vocabs)\n",
    "emb_size = 50\n",
    "model = Skipgram(vocab_size, emb_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#convert all vocabs to tensors\n",
    "def prepare_sequence(vocabs, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], vocabs))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, vocab_size)\n",
    "\n",
    "#calculate epoch time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time // 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "#train the model\n",
    "import time\n",
    "\n",
    "num_epochs = 500\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    input_batch, target_batch = random_batch_skip_gram(window_size, batch_size)\n",
    "    input_batch  = torch.LongTensor(input_batch)  #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch) #[batch_size, 1]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(input_batch, target_batch, all_vocabs)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        end = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liberty vs. freedom:  0.0568597\n",
      "liberty vs. democracy:  0.17239296\n",
      "liberty vs. magic:  -0.22155902\n"
     ]
    }
   ],
   "source": [
    "#select a few vocabs and get their embeddings\n",
    "liberty = get_embed('liberty')\n",
    "freedom = get_embed('freedom')\n",
    "democracy = get_embed('democracy')\n",
    "magic = get_embed('magic')\n",
    "\n",
    "#define cosine similarity\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim\n",
    "\n",
    "#print cosine similarities between the selected vocabs\n",
    "print(f\"liberty vs. freedom: \", cos_sim(liberty, freedom))\n",
    "print(f\"liberty vs. democracy: \", cos_sim(liberty, democracy))\n",
    "print(f\"liberty vs. magic: \", cos_sim(liberty, magic))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Skip-gram with Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count all the occurrences of vocabs\n",
    "from collections import Counter\n",
    "word_count = Counter(flatten(corpus_tokenized))\n",
    "\n",
    "#count the number of total words\n",
    "num_total_words = sum([c for w, c in word_count.items()])\n",
    "\n",
    "#create the scaled-up unigram distribution table for vocabs\n",
    "z = 0.001 #the scaler\n",
    "unigram_table = []\n",
    "for v in vocabs:\n",
    "    unigram_table.extend([v] * int(((word_count[v]/num_total_words)**0.75)/z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert word indices to tensors\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "#generate random negative samples\n",
    "import random\n",
    "\n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.size(0)\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):\n",
    "        nsample = []\n",
    "        target_index = targets[i].item()\n",
    "        while len(nsample) < k:\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).view(1, -1))\n",
    "    \n",
    "    return torch.cat(neg_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1080, 3944, 6707, 2867, 5840],\n",
       "        [6802, 6985, 8232, 2235,  157],\n",
       "        [7194, 7025, 5030, 3932, 7494],\n",
       "        [ 481, 2372, 3547, 7641, 3790],\n",
       "        [7502, 2085, 6152, 6416, 5823],\n",
       "        [1506, 6309, 1184, 7608, 8735],\n",
       "        [1344, 5084, 1770, 8137, 1642],\n",
       "        [1295, 3021, 6402,  287, 2332],\n",
       "        [ 439, 8065, 4156, 6924,  848],\n",
       "        [8965, 5113, 7641, 7299, 6983]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the negative sampling method\n",
    "input_batch, target_batch = random_batch_skip_gram(2, 10)\n",
    "input_batch = torch.LongTensor(input_batch)\n",
    "target_batch = torch.LongTensor(target_batch)\n",
    "\n",
    "num_neg = 5 #number of negative samples for each target word\n",
    "\n",
    "neg_samples = negative_sampling(target_batch, unigram_table, num_neg)\n",
    "\n",
    "neg_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip-gram with negative sampling model\n",
    "class SkipgramNegSampling(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(SkipgramNegSampling, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, emb_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, emb_size)\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "                    \n",
    "    def forward(self, center_words, context_words, neg_samples):\n",
    "        center_embeds  = self.embedding_v(center_words) # [batch_size, 1, emb_size]\n",
    "        context_embeds = self.embedding_u(context_words) # [batch_size, 1, emb_size]\n",
    "        neg_embeds     = self.embedding_u(neg_samples) # [batch_size, num_neg, emb_size]\n",
    "        \n",
    "        positive_score = context_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "        \n",
    "        negative_score = -neg_embeds.bmm(center_embeds.transpose(1, 2))\n",
    "        #[batch_size, k, emb_size] @ [batch_size, emb_size, 1] = [batch_size, num_neg, 1]\n",
    "        \n",
    "        loss = -torch.mean(self.logsigmoid(positive_score) + torch.sum(self.logsigmoid(negative_score), 1))\n",
    "                \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | cost: 37.782867 | time: 0m 56s\n",
      "Epoch: 200 | cost: 30.898441 | time: 1m 54s\n",
      "Epoch: 300 | cost: 22.255024 | time: 3m 1s\n",
      "Epoch: 400 | cost: 33.215740 | time: 3m 58s\n",
      "Epoch: 500 | cost: 33.090736 | time: 4m 54s\n"
     ]
    }
   ],
   "source": [
    "#set parameters\n",
    "window_size = 2\n",
    "batch_size  = 10\n",
    "vocab_size  = len(vocabs)\n",
    "emb_size    = 50\n",
    "model       = SkipgramNegSampling(vocab_size, emb_size)\n",
    "num_neg     = 10\n",
    "optimizer   = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#calculate epoch time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time // 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "#train the model\n",
    "import time\n",
    "\n",
    "num_epochs = 500\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):    \n",
    "    input_batch, target_batch = random_batch_skip_gram(window_size, batch_size)\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    target_batch = torch.LongTensor(target_batch)\n",
    "    neg_samples = negative_sampling(target_batch, unigram_table, num_neg)\n",
    "    \n",
    "    optimizer.zero_grad()   \n",
    "    loss = model(input_batch, target_batch, neg_samples)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        end = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liberty vs. freedom:  0.12642717\n",
      "liberty vs. democracy:  0.11941776\n",
      "liberty vs. magic:  0.16588287\n"
     ]
    }
   ],
   "source": [
    "#select a few vocabs and get their embeddings\n",
    "liberty = get_embed('liberty')\n",
    "freedom = get_embed('freedom')\n",
    "democracy = get_embed('democracy')\n",
    "magic = get_embed('magic')\n",
    "\n",
    "#define cosine similarity\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim\n",
    "\n",
    "#print cosine similarities between the selected vocabs\n",
    "print(f\"liberty vs. freedom: \", cos_sim(liberty, freedom))\n",
    "print(f\"liberty vs. democracy: \", cos_sim(liberty, democracy))\n",
    "print(f\"liberty vs. magic: \", cos_sim(liberty, magic))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. Syntactic Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Gensim\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = datapath('glove.6B.50d.txt')\n",
    "model = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Samantic Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3. Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7ca07084f99cae884d00a2401f5a915152405bd446c47609c53897335e04337"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
