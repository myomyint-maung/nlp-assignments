{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19 Jan - GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the training corpus\n",
    "#I use the Australian Broadcasting Commission corpus from NLTK\n",
    "import nltk\n",
    "from nltk.corpus import abc\n",
    "corpus = nltk.corpus.abc.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['PM', 'denies', 'knowledge', 'of', 'AWB', 'kickbacks', 'The', 'Prime', 'Minister', 'has', 'denied', 'he', 'knew', 'AWB', 'was', 'paying', 'kickbacks', 'to', 'Iraq', 'despite', 'writing', 'to', 'the', 'wheat', 'exporter', 'asking', 'to', 'be', 'kept', 'fully', 'informed', 'on', 'Iraq', 'wheat', 'sales', '.'], ['Letters', 'from', 'John', 'Howard', 'and', 'Deputy', 'Prime', 'Minister', 'Mark', 'Vaile', 'to', 'AWB', 'have', 'been', 'released', 'by', 'the', 'Cole', 'inquiry', 'into', 'the', 'oil', 'for', 'food', 'program', '.'], ['In', 'one', 'of', 'the', 'letters', 'Mr', 'Howard', 'asks', 'AWB', 'managing', 'director', 'Andrew', 'Lindberg', 'to', 'remain', 'in', 'close', 'contact', 'with', 'the', 'Government', 'on', 'Iraq', 'wheat', 'sales', '.']]\n"
     ]
    }
   ],
   "source": [
    "#the corpus is already tokenized\n",
    "print(corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['pm', 'denies', 'knowledge', 'of', 'awb', 'kickbacks', 'the', 'prime', 'minister', 'has', 'denied', 'he', 'knew', 'awb', 'was', 'paying', 'kickbacks', 'to', 'iraq', 'despite', 'writing', 'to', 'the', 'wheat', 'exporter', 'asking', 'to', 'be', 'kept', 'fully', 'informed', 'on', 'iraq', 'wheat', 'sales', '.'], ['letters', 'from', 'john', 'howard', 'and', 'deputy', 'prime', 'minister', 'mark', 'vaile', 'to', 'awb', 'have', 'been', 'released', 'by', 'the', 'cole', 'inquiry', 'into', 'the', 'oil', 'for', 'food', 'program', '.'], ['in', 'one', 'of', 'the', 'letters', 'mr', 'howard', 'asks', 'awb', 'managing', 'director', 'andrew', 'lindberg', 'to', 'remain', 'in', 'close', 'contact', 'with', 'the', 'government', 'on', 'iraq', 'wheat', 'sales', '.']]\n"
     ]
    }
   ],
   "source": [
    "#convert the words in the corpus into lower case\n",
    "corpus_tokenized = [[]] * len(corpus)\n",
    "for i in range(len(corpus)):\n",
    "    corpus_tokenized[i] = [word.lower() for word in corpus[i]]\n",
    "\n",
    "print(corpus_tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['pm', 'denies', 'knowledge', 'awb', 'kickbacks', 'prime', 'minister', 'denied', 'knew', 'awb', 'paying', 'kickbacks', 'iraq', 'despite', 'writing', 'wheat', 'exporter', 'asking', 'kept', 'fully', 'informed', 'iraq', 'wheat', 'sales', '.'], ['letters', 'john', 'howard', 'deputy', 'prime', 'minister', 'mark', 'vaile', 'awb', 'released', 'cole', 'inquiry', 'oil', 'food', 'program', '.'], ['letters', 'mr', 'howard', 'asks', 'awb', 'managing', 'director', 'andrew', 'lindberg', 'remain', 'close', 'contact', 'government', 'iraq', 'wheat', 'sales', '.']]\n"
     ]
    }
   ],
   "source": [
    "#remove stop words with SpaCy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "for sentence in corpus_tokenized:\n",
    "    for word in sentence[:]:\n",
    "        if word in stopwords:\n",
    "            sentence.remove(word)\n",
    "\n",
    "print(corpus_tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['pm', 'denies', 'knowledge', 'awb', 'kickbacks', 'prime', 'minister', 'denied', 'knew', 'awb', 'paying', 'kickbacks', 'iraq', 'despite', 'writing', 'wheat', 'exporter', 'asking', 'kept', 'fully', 'informed', 'iraq', 'wheat', 'sales'], ['letters', 'john', 'howard', 'deputy', 'prime', 'minister', 'mark', 'vaile', 'awb', 'released', 'cole', 'inquiry', 'oil', 'food', 'program'], ['letters', 'mr', 'howard', 'asks', 'awb', 'managing', 'director', 'andrew', 'lindberg', 'remain', 'close', 'contact', 'government', 'iraq', 'wheat', 'sales']]\n"
     ]
    }
   ],
   "source": [
    "#remove punctutations with String\n",
    "import string\n",
    "punctutations = string.punctuation\n",
    "\n",
    "for sentence in corpus_tokenized:\n",
    "    for word in sentence[:]:\n",
    "        if word in punctutations:\n",
    "            sentence.remove(word)\n",
    "\n",
    "print(corpus_tokenized[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get unique words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabs = list(set(flatten(corpus_tokenized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numericalize the vocabs\n",
    "word2index = {w: i for i, w in enumerate(vocabs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append <UNK>\n",
    "vocabs.append('<UNK>')\n",
    "word2index['<UNK>'] = len(word2index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GloVe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Co-occurence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count the frequency of each word\n",
    "from collections import Counter\n",
    "\n",
    "X_i = Counter(flatten(corpus_tokenized))\n",
    "\n",
    "X_i['minister']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate skipgrams with a generic window size\n",
    "def generate_skip_gram(window_size): \n",
    "    skip_grams = []\n",
    "    for sentence in corpus_tokenized:\n",
    "        #I include the first and last words\n",
    "        #so that every word could be a center word\n",
    "        for i in range(len(sentence)):\n",
    "            center = sentence[i]\n",
    "            context = []\n",
    "            for j in range(window_size):\n",
    "                if (i - window_size + j) >= 0:\n",
    "                    context.append(sentence[i - window_size + j])\n",
    "            for k in range(1, window_size + 1):\n",
    "                if (i + k) < len(sentence):\n",
    "                    context.append(sentence[i + k])\n",
    "            for w in context:\n",
    "                skip_grams.append((center, w))\n",
    "        \n",
    "    return skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pm', 'denies'),\n",
       " ('pm', 'knowledge'),\n",
       " ('denies', 'pm'),\n",
       " ('denies', 'knowledge'),\n",
       " ('denies', 'awb')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare skipgrams with window size of 2\n",
    "skip_grams = generate_skip_gram(2)\n",
    "\n",
    "skip_grams[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('pm', 'denies'): 1,\n",
       "         ('pm', 'knowledge'): 1,\n",
       "         ('denies', 'pm'): 1,\n",
       "         ('denies', 'knowledge'): 1,\n",
       "         ('denies', 'awb'): 1,\n",
       "         ('knowledge', 'pm'): 1,\n",
       "         ('knowledge', 'denies'): 1,\n",
       "         ('knowledge', 'awb'): 2,\n",
       "         ('knowledge', 'kickbacks'): 1,\n",
       "         ('awb', 'denies'): 1,\n",
       "         ('awb', 'knowledge'): 2,\n",
       "         ('awb', 'kickbacks'): 9,\n",
       "         ('awb', 'prime'): 1,\n",
       "         ('kickbacks', 'knowledge'): 1,\n",
       "         ('kickbacks', 'awb'): 9,\n",
       "         ('kickbacks', 'prime'): 1,\n",
       "         ('kickbacks', 'minister'): 1,\n",
       "         ('prime', 'awb'): 1,\n",
       "         ('prime', 'kickbacks'): 1,\n",
       "         ('prime', 'minister'): 92,\n",
       "         ('prime', 'denied'): 1,\n",
       "         ('minister', 'kickbacks'): 1,\n",
       "         ('minister', 'prime'): 92,\n",
       "         ('minister', 'denied'): 1,\n",
       "         ('minister', 'knew'): 2,\n",
       "         ('denied', 'prime'): 1,\n",
       "         ('denied', 'minister'): 1,\n",
       "         ('denied', 'knew'): 1,\n",
       "         ('denied', 'awb'): 2,\n",
       "         ('knew', 'minister'): 2,\n",
       "         ('knew', 'denied'): 1,\n",
       "         ('knew', 'awb'): 3,\n",
       "         ('knew', 'paying'): 1,\n",
       "         ('awb', 'denied'): 2,\n",
       "         ('awb', 'knew'): 3,\n",
       "         ('awb', 'paying'): 4,\n",
       "         ('paying', 'knew'): 1,\n",
       "         ('paying', 'awb'): 4,\n",
       "         ('paying', 'kickbacks'): 2,\n",
       "         ('paying', 'iraq'): 2,\n",
       "         ('kickbacks', 'paying'): 2,\n",
       "         ('kickbacks', 'iraq'): 7,\n",
       "         ('kickbacks', 'despite'): 1,\n",
       "         ('iraq', 'paying'): 2,\n",
       "         ('iraq', 'kickbacks'): 7,\n",
       "         ('iraq', 'despite'): 1,\n",
       "         ('iraq', 'writing'): 1,\n",
       "         ('despite', 'kickbacks'): 1,\n",
       "         ('despite', 'iraq'): 1,\n",
       "         ('despite', 'writing'): 1,\n",
       "         ('despite', 'wheat'): 3,\n",
       "         ('writing', 'iraq'): 1,\n",
       "         ('writing', 'despite'): 1,\n",
       "         ('writing', 'wheat'): 1,\n",
       "         ('writing', 'exporter'): 1,\n",
       "         ('wheat', 'despite'): 3,\n",
       "         ('wheat', 'writing'): 1,\n",
       "         ('wheat', 'exporter'): 52,\n",
       "         ('wheat', 'asking'): 1,\n",
       "         ('exporter', 'writing'): 1,\n",
       "         ('exporter', 'wheat'): 52,\n",
       "         ('exporter', 'asking'): 1,\n",
       "         ('exporter', 'kept'): 1,\n",
       "         ('asking', 'wheat'): 1,\n",
       "         ('asking', 'exporter'): 1,\n",
       "         ('asking', 'kept'): 1,\n",
       "         ('asking', 'fully'): 1,\n",
       "         ('kept', 'exporter'): 1,\n",
       "         ('kept', 'asking'): 1,\n",
       "         ('kept', 'fully'): 1,\n",
       "         ('kept', 'informed'): 1,\n",
       "         ('fully', 'asking'): 1,\n",
       "         ('fully', 'kept'): 1,\n",
       "         ('fully', 'informed'): 2,\n",
       "         ('fully', 'iraq'): 1,\n",
       "         ('informed', 'kept'): 1,\n",
       "         ('informed', 'fully'): 2,\n",
       "         ('informed', 'iraq'): 1,\n",
       "         ('informed', 'wheat'): 2,\n",
       "         ('iraq', 'fully'): 1,\n",
       "         ('iraq', 'informed'): 1,\n",
       "         ('iraq', 'wheat'): 43,\n",
       "         ('iraq', 'sales'): 3,\n",
       "         ('wheat', 'informed'): 2,\n",
       "         ('wheat', 'iraq'): 43,\n",
       "         ('wheat', 'sales'): 8,\n",
       "         ('sales', 'iraq'): 3,\n",
       "         ('sales', 'wheat'): 8,\n",
       "         ('letters', 'john'): 1,\n",
       "         ('letters', 'howard'): 2,\n",
       "         ('john', 'letters'): 1,\n",
       "         ('john', 'howard'): 35,\n",
       "         ('john', 'deputy'): 2,\n",
       "         ('howard', 'letters'): 2,\n",
       "         ('howard', 'john'): 35,\n",
       "         ('howard', 'deputy'): 1,\n",
       "         ('howard', 'prime'): 3,\n",
       "         ('deputy', 'john'): 2,\n",
       "         ('deputy', 'howard'): 1,\n",
       "         ('deputy', 'prime'): 22,\n",
       "         ('deputy', 'minister'): 23,\n",
       "         ('prime', 'howard'): 3,\n",
       "         ('prime', 'deputy'): 22,\n",
       "         ('prime', 'mark'): 11,\n",
       "         ('minister', 'deputy'): 23,\n",
       "         ('minister', 'mark'): 24,\n",
       "         ('minister', 'vaile'): 24,\n",
       "         ('mark', 'prime'): 11,\n",
       "         ('mark', 'minister'): 24,\n",
       "         ('mark', 'vaile'): 37,\n",
       "         ('mark', 'awb'): 1,\n",
       "         ('vaile', 'minister'): 24,\n",
       "         ('vaile', 'mark'): 37,\n",
       "         ('vaile', 'awb'): 1,\n",
       "         ('vaile', 'released'): 1,\n",
       "         ('awb', 'mark'): 1,\n",
       "         ('awb', 'vaile'): 1,\n",
       "         ('awb', 'released'): 3,\n",
       "         ('awb', 'cole'): 12,\n",
       "         ('released', 'vaile'): 1,\n",
       "         ('released', 'awb'): 3,\n",
       "         ('released', 'cole'): 3,\n",
       "         ('released', 'inquiry'): 4,\n",
       "         ('cole', 'awb'): 12,\n",
       "         ('cole', 'released'): 3,\n",
       "         ('cole', 'inquiry'): 90,\n",
       "         ('cole', 'oil'): 3,\n",
       "         ('inquiry', 'released'): 4,\n",
       "         ('inquiry', 'cole'): 90,\n",
       "         ('inquiry', 'oil'): 43,\n",
       "         ('inquiry', 'food'): 42,\n",
       "         ('oil', 'cole'): 3,\n",
       "         ('oil', 'inquiry'): 43,\n",
       "         ('oil', 'food'): 76,\n",
       "         ('oil', 'program'): 17,\n",
       "         ('food', 'inquiry'): 42,\n",
       "         ('food', 'oil'): 76,\n",
       "         ('food', 'program'): 18,\n",
       "         ('program', 'oil'): 17,\n",
       "         ('program', 'food'): 18,\n",
       "         ('letters', 'mr'): 1,\n",
       "         ('mr', 'letters'): 1,\n",
       "         ('mr', 'howard'): 19,\n",
       "         ('mr', 'asks'): 1,\n",
       "         ('howard', 'mr'): 19,\n",
       "         ('howard', 'asks'): 1,\n",
       "         ('howard', 'awb'): 1,\n",
       "         ('asks', 'mr'): 1,\n",
       "         ('asks', 'howard'): 1,\n",
       "         ('asks', 'awb'): 1,\n",
       "         ('asks', 'managing'): 1,\n",
       "         ('awb', 'howard'): 1,\n",
       "         ('awb', 'asks'): 1,\n",
       "         ('awb', 'managing'): 5,\n",
       "         ('awb', 'director'): 7,\n",
       "         ('managing', 'asks'): 1,\n",
       "         ('managing', 'awb'): 5,\n",
       "         ('managing', 'director'): 63,\n",
       "         ('managing', 'andrew'): 9,\n",
       "         ('director', 'awb'): 7,\n",
       "         ('director', 'managing'): 63,\n",
       "         ('director', 'andrew'): 10,\n",
       "         ('director', 'lindberg'): 7,\n",
       "         ('andrew', 'managing'): 9,\n",
       "         ('andrew', 'director'): 10,\n",
       "         ('andrew', 'lindberg'): 12,\n",
       "         ('andrew', 'remain'): 1,\n",
       "         ('lindberg', 'director'): 7,\n",
       "         ('lindberg', 'andrew'): 12,\n",
       "         ('lindberg', 'remain'): 1,\n",
       "         ('lindberg', 'close'): 1,\n",
       "         ('remain', 'andrew'): 1,\n",
       "         ('remain', 'lindberg'): 1,\n",
       "         ('remain', 'close'): 1,\n",
       "         ('remain', 'contact'): 1,\n",
       "         ('close', 'lindberg'): 1,\n",
       "         ('close', 'remain'): 1,\n",
       "         ('close', 'contact'): 4,\n",
       "         ('close', 'government'): 2,\n",
       "         ('contact', 'remain'): 1,\n",
       "         ('contact', 'close'): 4,\n",
       "         ('contact', 'government'): 1,\n",
       "         ('contact', 'iraq'): 1,\n",
       "         ('government', 'close'): 2,\n",
       "         ('government', 'contact'): 1,\n",
       "         ('government', 'iraq'): 3,\n",
       "         ('government', 'wheat'): 6,\n",
       "         ('iraq', 'contact'): 1,\n",
       "         ('iraq', 'government'): 3,\n",
       "         ('wheat', 'government'): 6,\n",
       "         ('opposition', 's'): 8,\n",
       "         ('opposition', 'gavan'): 1,\n",
       "         ('s', 'opposition'): 8,\n",
       "         ('s', 'gavan'): 1,\n",
       "         ('s', 'o'): 6,\n",
       "         ('gavan', 'opposition'): 1,\n",
       "         ('gavan', 's'): 1,\n",
       "         ('gavan', 'o'): 9,\n",
       "         ('gavan', 'connor'): 9,\n",
       "         ('o', 's'): 6,\n",
       "         ('o', 'gavan'): 9,\n",
       "         ('o', 'connor'): 27,\n",
       "         ('o', 'says'): 25,\n",
       "         ('connor', 'gavan'): 9,\n",
       "         ('connor', 'o'): 27,\n",
       "         ('connor', 'says'): 12,\n",
       "         ('connor', 'letter'): 1,\n",
       "         ('says', 'o'): 25,\n",
       "         ('says', 'connor'): 12,\n",
       "         ('says', 'letter'): 2,\n",
       "         ('says', 'sent'): 1,\n",
       "         ('letter', 'connor'): 1,\n",
       "         ('letter', 'says'): 2,\n",
       "         ('letter', 'sent'): 3,\n",
       "         ('letter', '2002'): 1,\n",
       "         ('sent', 'says'): 1,\n",
       "         ('sent', 'letter'): 3,\n",
       "         ('sent', '2002'): 1,\n",
       "         ('sent', 'time'): 1,\n",
       "         ('2002', 'letter'): 1,\n",
       "         ('2002', 'sent'): 1,\n",
       "         ('2002', 'time'): 2,\n",
       "         ('2002', 'awb'): 3,\n",
       "         ('time', 'sent'): 1,\n",
       "         ('time', '2002'): 2,\n",
       "         ('time', 'awb'): 1,\n",
       "         ('time', 'paying'): 2,\n",
       "         ('awb', '2002'): 3,\n",
       "         ('awb', 'time'): 1,\n",
       "         ('paying', 'time'): 2,\n",
       "         ('kickbacks', 'jordanian'): 1,\n",
       "         ('iraq', 'jordanian'): 1,\n",
       "         ('iraq', 'trucking'): 1,\n",
       "         ('jordanian', 'kickbacks'): 1,\n",
       "         ('jordanian', 'iraq'): 1,\n",
       "         ('jordanian', 'trucking'): 4,\n",
       "         ('jordanian', 'company'): 4,\n",
       "         ('trucking', 'iraq'): 1,\n",
       "         ('trucking', 'jordanian'): 4,\n",
       "         ('trucking', 'company'): 7,\n",
       "         ('company', 'jordanian'): 4,\n",
       "         ('company', 'trucking'): 7,\n",
       "         ('says', 'government'): 131,\n",
       "         ('says', 'longer'): 2,\n",
       "         ('government', 'says'): 131,\n",
       "         ('government', 'longer'): 1,\n",
       "         ('government', 'wipe'): 2,\n",
       "         ('longer', 'says'): 2,\n",
       "         ('longer', 'government'): 1,\n",
       "         ('longer', 'wipe'): 1,\n",
       "         ('longer', 'hands'): 1,\n",
       "         ('wipe', 'government'): 2,\n",
       "         ('wipe', 'longer'): 1,\n",
       "         ('wipe', 'hands'): 1,\n",
       "         ('wipe', 'illicit'): 1,\n",
       "         ('hands', 'longer'): 1,\n",
       "         ('hands', 'wipe'): 1,\n",
       "         ('hands', 'illicit'): 1,\n",
       "         ('hands', 'payments'): 1,\n",
       "         ('illicit', 'wipe'): 1,\n",
       "         ('illicit', 'hands'): 1,\n",
       "         ('illicit', 'payments'): 1,\n",
       "         ('illicit', 'totalled'): 1,\n",
       "         ('payments', 'hands'): 1,\n",
       "         ('payments', 'illicit'): 1,\n",
       "         ('payments', 'totalled'): 1,\n",
       "         ('payments', '290'): 1,\n",
       "         ('totalled', 'illicit'): 1,\n",
       "         ('totalled', 'payments'): 1,\n",
       "         ('totalled', '290'): 1,\n",
       "         ('totalled', 'million'): 1,\n",
       "         ('290', 'payments'): 1,\n",
       "         ('290', 'totalled'): 1,\n",
       "         ('290', 'million'): 3,\n",
       "         ('million', 'totalled'): 1,\n",
       "         ('million', '290'): 3,\n",
       "         ('responsibility', 'lay'): 1,\n",
       "         ('responsibility', 'squarely'): 1,\n",
       "         ('lay', 'responsibility'): 1,\n",
       "         ('lay', 'squarely'): 1,\n",
       "         ('lay', 'feet'): 1,\n",
       "         ('squarely', 'responsibility'): 1,\n",
       "         ('squarely', 'lay'): 1,\n",
       "         ('squarely', 'feet'): 1,\n",
       "         ('squarely', 'coalition'): 1,\n",
       "         ('feet', 'lay'): 1,\n",
       "         ('feet', 'squarely'): 1,\n",
       "         ('feet', 'coalition'): 1,\n",
       "         ('feet', 'ministers'): 1,\n",
       "         ('coalition', 'squarely'): 1,\n",
       "         ('coalition', 'feet'): 1,\n",
       "         ('coalition', 'ministers'): 1,\n",
       "         ('coalition', 'trade'): 1,\n",
       "         ('ministers', 'feet'): 1,\n",
       "         ('ministers', 'coalition'): 1,\n",
       "         ('ministers', 'trade'): 2,\n",
       "         ('ministers', 'agriculture'): 3,\n",
       "         ('trade', 'coalition'): 1,\n",
       "         ('trade', 'ministers'): 2,\n",
       "         ('trade', 'agriculture'): 4,\n",
       "         ('trade', 'prime'): 1,\n",
       "         ('agriculture', 'ministers'): 3,\n",
       "         ('agriculture', 'trade'): 4,\n",
       "         ('agriculture', 'prime'): 1,\n",
       "         ('agriculture', 'minister'): 91,\n",
       "         ('prime', 'trade'): 1,\n",
       "         ('prime', 'agriculture'): 1,\n",
       "         ('prime', ',\"'): 1,\n",
       "         ('minister', 'agriculture'): 91,\n",
       "         ('minister', ',\"'): 4,\n",
       "         ('minister', 'said'): 4,\n",
       "         (',\"', 'prime'): 1,\n",
       "         (',\"', 'minister'): 4,\n",
       "         (',\"', 'said'): 1822,\n",
       "         ('said', 'minister'): 4,\n",
       "         ('said', ',\"'): 1822,\n",
       "         ('prime', 'says'): 7,\n",
       "         ('minister', 'says'): 20,\n",
       "         ('minister', 'letters'): 1,\n",
       "         ('says', 'prime'): 7,\n",
       "         ('says', 'minister'): 20,\n",
       "         ('says', 'letters'): 1,\n",
       "         ('says', 'inquiring'): 1,\n",
       "         ('letters', 'minister'): 1,\n",
       "         ('letters', 'says'): 1,\n",
       "         ('letters', 'inquiring'): 1,\n",
       "         ('letters', 'future'): 1,\n",
       "         ('inquiring', 'says'): 1,\n",
       "         ('inquiring', 'letters'): 1,\n",
       "         ('inquiring', 'future'): 1,\n",
       "         ('inquiring', 'wheat'): 1,\n",
       "         ('future', 'letters'): 1,\n",
       "         ('future', 'inquiring'): 1,\n",
       "         ('future', 'wheat'): 11,\n",
       "         ('future', 'sales'): 2,\n",
       "         ('wheat', 'inquiring'): 1,\n",
       "         ('wheat', 'future'): 11,\n",
       "         ('sales', 'future'): 2,\n",
       "         ('sales', 'prove'): 1,\n",
       "         ('iraq', 'prove'): 1,\n",
       "         ('prove', 'sales'): 1,\n",
       "         ('prove', 'iraq'): 1,\n",
       "         ('prove', 'government'): 2,\n",
       "         ('prove', 'knew'): 1,\n",
       "         ('government', 'prove'): 2,\n",
       "         ('government', 'knew'): 1,\n",
       "         ('government', 'payments'): 2,\n",
       "         ('knew', 'prove'): 1,\n",
       "         ('knew', 'government'): 1,\n",
       "         ('knew', 'payments'): 1,\n",
       "         ('payments', 'government'): 2,\n",
       "         ('payments', 'knew'): 1,\n",
       "         ('astonishing', '2002'): 1,\n",
       "         ('astonishing', 'prime'): 1,\n",
       "         ('2002', 'astonishing'): 1,\n",
       "         ('2002', 'prime'): 1,\n",
       "         ('2002', 'minister'): 1,\n",
       "         ('prime', 'astonishing'): 1,\n",
       "         ('prime', '2002'): 1,\n",
       "         ('prime', 'hadn'): 1,\n",
       "         ('minister', '2002'): 1,\n",
       "         ('minister', 'hadn'): 1,\n",
       "         ('minister', 't'): 5,\n",
       "         ('hadn', 'prime'): 1,\n",
       "         ('hadn', 'minister'): 1,\n",
       "         ('hadn', 't'): 6,\n",
       "         ('hadn', 'possibly'): 1,\n",
       "         ('t', 'minister'): 5,\n",
       "         ('t', 'hadn'): 6,\n",
       "         ('t', 'possibly'): 2,\n",
       "         ('t', 'preserve'): 1,\n",
       "         ('possibly', 'hadn'): 1,\n",
       "         ('possibly', 't'): 2,\n",
       "         ('possibly', 'preserve'): 1,\n",
       "         ('possibly', 'australia'): 3,\n",
       "         ('preserve', 't'): 1,\n",
       "         ('preserve', 'possibly'): 1,\n",
       "         ('preserve', 'australia'): 1,\n",
       "         ('preserve', 's'): 1,\n",
       "         ('australia', 'possibly'): 3,\n",
       "         ('australia', 'preserve'): 1,\n",
       "         ('australia', 's'): 566,\n",
       "         ('australia', 'valuable'): 1,\n",
       "         ('s', 'preserve'): 1,\n",
       "         ('s', 'australia'): 566,\n",
       "         ('s', 'valuable'): 1,\n",
       "         ('s', 'wheat'): 43,\n",
       "         ('valuable', 'australia'): 1,\n",
       "         ('valuable', 's'): 1,\n",
       "         ('valuable', 'wheat'): 1,\n",
       "         ('valuable', 'market'): 1,\n",
       "         ('wheat', 's'): 43,\n",
       "         ('wheat', 'valuable'): 1,\n",
       "         ('wheat', 'market'): 16,\n",
       "         ('wheat', ',\"'): 13,\n",
       "         ('market', 'valuable'): 1,\n",
       "         ('market', 'wheat'): 16,\n",
       "         ('market', ',\"'): 40,\n",
       "         ('market', 'said'): 26,\n",
       "         (',\"', 'wheat'): 13,\n",
       "         (',\"', 'market'): 40,\n",
       "         ('said', 'market'): 26,\n",
       "         ('email', 'questions'): 2,\n",
       "         ('email', 'today'): 1,\n",
       "         ('questions', 'email'): 2,\n",
       "         ('questions', 'today'): 2,\n",
       "         ('questions', 'inquiry'): 1,\n",
       "         ('today', 'email'): 1,\n",
       "         ('today', 'questions'): 2,\n",
       "         ('today', 'inquiry'): 3,\n",
       "         ('today', 'awb'): 1,\n",
       "         ('inquiry', 'questions'): 1,\n",
       "         ('inquiry', 'today'): 3,\n",
       "         ('inquiry', 'awb'): 25,\n",
       "         ('inquiry', 'trading'): 2,\n",
       "         ('awb', 'today'): 1,\n",
       "         ('awb', 'inquiry'): 25,\n",
       "         ('awb', 'trading'): 3,\n",
       "         ('awb', 'manager'): 5,\n",
       "         ('trading', 'inquiry'): 2,\n",
       "         ('trading', 'awb'): 3,\n",
       "         ('trading', 'manager'): 1,\n",
       "         ('trading', 'peter'): 1,\n",
       "         ('manager', 'awb'): 5,\n",
       "         ('manager', 'trading'): 1,\n",
       "         ('manager', 'peter'): 3,\n",
       "         ('manager', 'geary'): 1,\n",
       "         ('peter', 'trading'): 1,\n",
       "         ('peter', 'manager'): 3,\n",
       "         ('peter', 'geary'): 3,\n",
       "         ('peter', 'questioned'): 1,\n",
       "         ('geary', 'manager'): 1,\n",
       "         ('geary', 'peter'): 3,\n",
       "         ('geary', 'questioned'): 1,\n",
       "         ('geary', 'email'): 1,\n",
       "         ('questioned', 'peter'): 1,\n",
       "         ('questioned', 'geary'): 1,\n",
       "         ('questioned', 'email'): 1,\n",
       "         ('questioned', 'received'): 1,\n",
       "         ('email', 'geary'): 1,\n",
       "         ('email', 'questioned'): 1,\n",
       "         ('email', 'received'): 1,\n",
       "         ('email', '2000'): 1,\n",
       "         ('received', 'questioned'): 1,\n",
       "         ('received', 'email'): 1,\n",
       "         ('received', '2000'): 1,\n",
       "         ('2000', 'email'): 1,\n",
       "         ('2000', 'received'): 1,\n",
       "         ('indicated', 'iraqi'): 1,\n",
       "         ('indicated', 'grains'): 1,\n",
       "         ('iraqi', 'indicated'): 1,\n",
       "         ('iraqi', 'grains'): 11,\n",
       "         ('iraqi', 'board'): 11,\n",
       "         ('grains', 'indicated'): 1,\n",
       "         ('grains', 'iraqi'): 11,\n",
       "         ('grains', 'board'): 12,\n",
       "         ('grains', 'approached'): 1,\n",
       "         ('board', 'iraqi'): 11,\n",
       "         ('board', 'grains'): 12,\n",
       "         ('board', 'approached'): 2,\n",
       "         ('board', 'awb'): 4,\n",
       "         ('approached', 'grains'): 1,\n",
       "         ('approached', 'board'): 2,\n",
       "         ('approached', 'awb'): 1,\n",
       "         ('approached', 'provide'): 1,\n",
       "         ('awb', 'board'): 4,\n",
       "         ('awb', 'approached'): 1,\n",
       "         ('awb', 'provide'): 2,\n",
       "         ('awb', 'sales'): 2,\n",
       "         ('provide', 'approached'): 1,\n",
       "         ('provide', 'awb'): 2,\n",
       "         ('provide', 'sales'): 1,\n",
       "         ('provide', 'service'): 2,\n",
       "         ('sales', 'awb'): 2,\n",
       "         ('sales', 'provide'): 1,\n",
       "         ('sales', 'service'): 3,\n",
       "         ('sales', '\".'): 1,\n",
       "         ('service', 'provide'): 2,\n",
       "         ('service', 'sales'): 3,\n",
       "         ('service', '\".'): 1,\n",
       "         ('\".', 'sales'): 1,\n",
       "         ('\".', 'service'): 1,\n",
       "         ('mr', 'geary'): 1,\n",
       "         ('mr', 'said'): 119,\n",
       "         ('geary', 'mr'): 1,\n",
       "         ('geary', 'said'): 1,\n",
       "         ('geary', 'forwarded'): 1,\n",
       "         ('said', 'mr'): 119,\n",
       "         ('said', 'geary'): 1,\n",
       "         ('said', 'forwarded'): 1,\n",
       "         ('said', 'email'): 3,\n",
       "         ('forwarded', 'geary'): 1,\n",
       "         ('forwarded', 'said'): 1,\n",
       "         ('forwarded', 'email'): 1,\n",
       "         ('forwarded', 'awb'): 1,\n",
       "         ('email', 'said'): 3,\n",
       "         ('email', 'forwarded'): 1,\n",
       "         ('email', 'awb'): 1,\n",
       "         ('email', 'colleagues'): 1,\n",
       "         ('awb', 'forwarded'): 1,\n",
       "         ('awb', 'email'): 1,\n",
       "         ('awb', 'colleagues'): 1,\n",
       "         ('awb', 'remember'): 1,\n",
       "         ('colleagues', 'email'): 1,\n",
       "         ('colleagues', 'awb'): 1,\n",
       "         ('colleagues', 'remember'): 1,\n",
       "         ('colleagues', 'reading'): 1,\n",
       "         ('remember', 'awb'): 1,\n",
       "         ('remember', 'colleagues'): 1,\n",
       "         ('remember', 'reading'): 1,\n",
       "         ('remember', 'said'): 1,\n",
       "         ('reading', 'colleagues'): 1,\n",
       "         ('reading', 'remember'): 1,\n",
       "         ('reading', 'said'): 1,\n",
       "         ('reading', 'skimmed'): 1,\n",
       "         ('said', 'remember'): 1,\n",
       "         ('said', 'reading'): 1,\n",
       "         ('said', 'skimmed'): 1,\n",
       "         ('skimmed', 'reading'): 1,\n",
       "         ('skimmed', 'said'): 1,\n",
       "         ('support', 'awb'): 4,\n",
       "         ('support', 'plenty'): 3,\n",
       "         ('awb', 'support'): 4,\n",
       "         ('awb', 'plenty'): 1,\n",
       "         ('plenty', 'support'): 3,\n",
       "         ('plenty', 'awb'): 1,\n",
       "         ('plenty', 'grain'): 1,\n",
       "         ('support', 'grain'): 2,\n",
       "         ('support', 'growers'): 8,\n",
       "         ('grain', 'plenty'): 1,\n",
       "         ('grain', 'support'): 2,\n",
       "         ('grain', 'growers'): 78,\n",
       "         ('grain', 'central'): 1,\n",
       "         ('growers', 'support'): 8,\n",
       "         ('growers', 'grain'): 78,\n",
       "         ('growers', 'central'): 3,\n",
       "         ('growers', 'western'): 17,\n",
       "         ('central', 'grain'): 1,\n",
       "         ('central', 'growers'): 3,\n",
       "         ('central', 'western'): 19,\n",
       "         ('central', 'new'): 26,\n",
       "         ('western', 'growers'): 17,\n",
       "         ('western', 'central'): 19,\n",
       "         ('western', 'new'): 39,\n",
       "         ('western', 'south'): 53,\n",
       "         ('new', 'central'): 26,\n",
       "         ('new', 'western'): 39,\n",
       "         ('new', 'south'): 444,\n",
       "         ('new', 'wales'): 423,\n",
       "         ('south', 'western'): 53,\n",
       "         ('south', 'new'): 444,\n",
       "         ('south', 'wales'): 434,\n",
       "         ('south', 'despite'): 2,\n",
       "         ('wales', 'new'): 423,\n",
       "         ('wales', 'south'): 434,\n",
       "         ('wales', 'despite'): 1,\n",
       "         ('wales', 'revelations'): 1,\n",
       "         ('despite', 'south'): 2,\n",
       "         ('despite', 'wales'): 1,\n",
       "         ('despite', 'revelations'): 1,\n",
       "         ('despite', 'cole'): 1,\n",
       "         ('revelations', 'wales'): 1,\n",
       "         ('revelations', 'despite'): 1,\n",
       "         ('revelations', 'cole'): 1,\n",
       "         ('revelations', 'inquiry'): 1,\n",
       "         ('cole', 'despite'): 1,\n",
       "         ('cole', 'revelations'): 1,\n",
       "         ('inquiry', 'revelations'): 1,\n",
       "         ('producers', 'broadly'): 1,\n",
       "         ('producers', 'support'): 2,\n",
       "         ('broadly', 'producers'): 1,\n",
       "         ('broadly', 'support'): 1,\n",
       "         ('broadly', 'awb'): 1,\n",
       "         ('support', 'producers'): 2,\n",
       "         ('support', 'broadly'): 1,\n",
       "         ('support', 's'): 8,\n",
       "         ('awb', 'broadly'): 1,\n",
       "         ('awb', 's'): 116,\n",
       "         ('awb', 'attempts'): 2,\n",
       "         ('s', 'support'): 8,\n",
       "         ('s', 'awb'): 116,\n",
       "         ('s', 'attempts'): 3,\n",
       "         ('s', 'best'): 24,\n",
       "         ('attempts', 'awb'): 2,\n",
       "         ('attempts', 's'): 3,\n",
       "         ('attempts', 'best'): 1,\n",
       "         ('attempts', 'prices'): 1,\n",
       "         ('best', 's'): 24,\n",
       "         ('best', 'attempts'): 1,\n",
       "         ('best', 'prices'): 4,\n",
       "         ('best', 'products'): 1,\n",
       "         ('prices', 'attempts'): 1,\n",
       "         ('prices', 'best'): 4,\n",
       "         ('prices', 'products'): 2,\n",
       "         ('products', 'best'): 1,\n",
       "         ('products', 'prices'): 2,\n",
       "         ('think', 's'): 121,\n",
       "         ('think', 'ploy'): 1,\n",
       "         ('s', 'think'): 121,\n",
       "         ('s', 'ploy'): 1,\n",
       "         ('s', 'overseas'): 2,\n",
       "         ('ploy', 'think'): 1,\n",
       "         ('ploy', 's'): 1,\n",
       "         ('ploy', 'overseas'): 1,\n",
       "         ('ploy', 'interests'): 1,\n",
       "         ('overseas', 's'): 2,\n",
       "         ('overseas', 'ploy'): 1,\n",
       "         ('overseas', 'interests'): 1,\n",
       "         ('overseas', 'try'): 1,\n",
       "         ('interests', 'ploy'): 1,\n",
       "         ('interests', 'overseas'): 1,\n",
       "         ('interests', 'try'): 1,\n",
       "         ('interests', 'single'): 1,\n",
       "         ('try', 'overseas'): 1,\n",
       "         ('try', 'interests'): 1,\n",
       "         ('try', 'single'): 1,\n",
       "         ('try', 'desk'): 1,\n",
       "         ('single', 'interests'): 1,\n",
       "         ('single', 'try'): 1,\n",
       "         ('single', 'desk'): 143,\n",
       "         ('single', 'aside'): 1,\n",
       "         ('desk', 'try'): 1,\n",
       "         ('desk', 'single'): 143,\n",
       "         ('desk', 'aside'): 1,\n",
       "         ('aside', 'single'): 1,\n",
       "         ('aside', 'desk'): 1,\n",
       "         ('stories', 'going'): 1,\n",
       "         ('stories', 'round'): 1,\n",
       "         ('going', 'stories'): 1,\n",
       "         ('going', 'round'): 2,\n",
       "         ('going', 'commission'): 1,\n",
       "         ('round', 'stories'): 1,\n",
       "         ('round', 'going'): 2,\n",
       "         ('round', 'commission'): 1,\n",
       "         ('round', 'think'): 1,\n",
       "         ('commission', 'going'): 1,\n",
       "         ('commission', 'round'): 1,\n",
       "         ('commission', 'think'): 1,\n",
       "         ('commission', 's'): 14,\n",
       "         ('think', 'round'): 1,\n",
       "         ('think', 'commission'): 1,\n",
       "         ('think', 'way'): 11,\n",
       "         ('s', 'commission'): 14,\n",
       "         ('s', 'way'): 60,\n",
       "         ('s', 'people'): 79,\n",
       "         ('way', 'think'): 11,\n",
       "         ('way', 's'): 60,\n",
       "         ('way', 'people'): 18,\n",
       "         ('way', 'got'): 3,\n",
       "         ('people', 's'): 79,\n",
       "         ('people', 'way'): 18,\n",
       "         ('people', 'got'): 13,\n",
       "         ('people', 'things'): 8,\n",
       "         ('got', 'way'): 3,\n",
       "         ('got', 'people'): 13,\n",
       "         ('got', 'things'): 2,\n",
       "         ('got', 'business'): 3,\n",
       "         ('things', 'people'): 8,\n",
       "         ('things', 'got'): 2,\n",
       "         ('things', 'business'): 2,\n",
       "         ('things', 'middle'): 1,\n",
       "         ('business', 'got'): 3,\n",
       "         ('business', 'things'): 2,\n",
       "         ('business', 'middle'): 1,\n",
       "         ('business', 'east'): 3,\n",
       "         ('middle', 'things'): 1,\n",
       "         ('middle', 'business'): 1,\n",
       "         ('middle', 'east'): 38,\n",
       "         ('middle', 'asian'): 1,\n",
       "         ('east', 'business'): 3,\n",
       "         ('east', 'middle'): 38,\n",
       "         ('east', 'asian'): 3,\n",
       "         ('east', 'countries'): 3,\n",
       "         ('asian', 'middle'): 1,\n",
       "         ('asian', 'east'): 3,\n",
       "         ('asian', 'countries'): 3,\n",
       "         ('asian', ',\"'): 1,\n",
       "         ('countries', 'east'): 3,\n",
       "         ('countries', 'asian'): 3,\n",
       "         ('countries', ',\"'): 7,\n",
       "         ('countries', 'producer'): 1,\n",
       "         (',\"', 'asian'): 1,\n",
       "         (',\"', 'countries'): 7,\n",
       "         (',\"', 'producer'): 3,\n",
       "         ('producer', 'countries'): 1,\n",
       "         ('producer', ',\"'): 3,\n",
       "         ('producer', 'said'): 4,\n",
       "         ('said', 'producer'): 4,\n",
       "         ('think', 'actually'): 5,\n",
       "         ('s', 'actually'): 26,\n",
       "         ('s', 'pretty'): 20,\n",
       "         ('actually', 'think'): 5,\n",
       "         ('actually', 's'): 26,\n",
       "         ('actually', 'pretty'): 4,\n",
       "         ('actually', 'reasonable'): 1,\n",
       "         ('pretty', 's'): 20,\n",
       "         ('pretty', 'actually'): 4,\n",
       "         ('pretty', 'reasonable'): 2,\n",
       "         ('pretty', 'system'): 1,\n",
       "         ('reasonable', 'actually'): 1,\n",
       "         ('reasonable', 'pretty'): 2,\n",
       "         ('reasonable', 'system'): 1,\n",
       "         ('reasonable', 'think'): 2,\n",
       "         ('system', 'pretty'): 1,\n",
       "         ('system', 'reasonable'): 1,\n",
       "         ('system', 'think'): 3,\n",
       "         ('system', 'actually'): 2,\n",
       "         ('think', 'reasonable'): 2,\n",
       "         ('think', 'system'): 3,\n",
       "         ('think', 'd'): 9,\n",
       "         ('actually', 'system'): 2,\n",
       "         ('actually', 'd'): 1,\n",
       "         ('d', 'think'): 9,\n",
       "         ('d', 'actually'): 1,\n",
       "         ('d', 'pretty'): 3,\n",
       "         ('d', 'fair'): 1,\n",
       "         ('pretty', 'd'): 3,\n",
       "         ('pretty', 'fair'): 2,\n",
       "         ('pretty', 'support'): 1,\n",
       "         ('fair', 'd'): 1,\n",
       "         ('fair', 'pretty'): 2,\n",
       "         ('fair', 'support'): 1,\n",
       "         ('fair', 'moment'): 1,\n",
       "         ('support', 'pretty'): 1,\n",
       "         ('support', 'fair'): 1,\n",
       "         ('support', 'moment'): 2,\n",
       "         ('moment', 'fair'): 1,\n",
       "         ('moment', 'support'): 2,\n",
       "         ('think', 'average'): 2,\n",
       "         ('think', 've'): 16,\n",
       "         ('average', 'think'): 2,\n",
       "         ('average', 've'): 2,\n",
       "         ('average', 'performed'): 1,\n",
       "         ('ve', 'think'): 16,\n",
       "         ('ve', 'average'): 2,\n",
       "         ('ve', 'performed'): 1,\n",
       "         ('ve', 'fairly'): 2,\n",
       "         ('performed', 'average'): 1,\n",
       "         ('performed', 've'): 1,\n",
       "         ('performed', 'fairly'): 1,\n",
       "         ('performed', ',\"'): 1,\n",
       "         ('fairly', 've'): 2,\n",
       "         ('fairly', 'performed'): 1,\n",
       "         ('fairly', ',\"'): 2,\n",
       "         ('fairly', 'producer'): 1,\n",
       "         (',\"', 'performed'): 1,\n",
       "         (',\"', 'fairly'): 2,\n",
       "         ('producer', 'fairly'): 1,\n",
       "         ('biggest', 'thing'): 3,\n",
       "         ('biggest', 'taking'): 1,\n",
       "         ('thing', 'biggest'): 3,\n",
       "         ('thing', 'taking'): 1,\n",
       "         ('thing', 'multinationals'): 1,\n",
       "         ('taking', 'biggest'): 1,\n",
       "         ('taking', 'thing'): 1,\n",
       "         ('taking', 'multinationals'): 1,\n",
       "         ('taking', 'foothold'): 1,\n",
       "         ('multinationals', 'thing'): 1,\n",
       "         ('multinationals', 'taking'): 1,\n",
       "         ('multinationals', 'foothold'): 1,\n",
       "         ('multinationals', 'advantage'): 1,\n",
       "         ('foothold', 'taking'): 1,\n",
       "         ('foothold', 'multinationals'): 1,\n",
       "         ('foothold', 'advantage'): 1,\n",
       "         ('foothold', '.\"'): 1,\n",
       "         ('advantage', 'multinationals'): 1,\n",
       "         ('advantage', 'foothold'): 1,\n",
       "         ('advantage', '.\"'): 2,\n",
       "         ('.\"', 'foothold'): 1,\n",
       "         ('.\"', 'advantage'): 2,\n",
       "         ('grain', 'prices'): 35,\n",
       "         ('grain', 'analyst'): 4,\n",
       "         ('prices', 'grain'): 35,\n",
       "         ('prices', 'analyst'): 2,\n",
       "         ('prices', 'predicts'): 2,\n",
       "         ('analyst', 'grain'): 4,\n",
       "         ('analyst', 'prices'): 2,\n",
       "         ('analyst', 'predicts'): 2,\n",
       "         ('predicts', 'prices'): 2,\n",
       "         ('predicts', 'analyst'): 2,\n",
       "         ('predicts', 'grain'): 1,\n",
       "         ('grain', 'predicts'): 1,\n",
       "         ('grain', 'drop'): 3,\n",
       "         ('prices', 'drop'): 12,\n",
       "         ('prices', '20'): 5,\n",
       "         ('drop', 'grain'): 3,\n",
       "         ('drop', 'prices'): 12,\n",
       "         ('drop', '20'): 3,\n",
       "         ('drop', 'tonne'): 2,\n",
       "         ('20', 'prices'): 5,\n",
       "         ('20', 'drop'): 3,\n",
       "         ('20', 'tonne'): 5,\n",
       "         ('20', 'inquiry'): 1,\n",
       "         ('tonne', 'drop'): 2,\n",
       "         ('tonne', '20'): 5,\n",
       "         ('tonne', 'inquiry'): 1,\n",
       "         ('tonne', 'awb'): 1,\n",
       "         ('inquiry', '20'): 1,\n",
       "         ('inquiry', 'tonne'): 1,\n",
       "         ('awb', 'tonne'): 1,\n",
       "         ('malcolm', 'bartholomaeus'): 7,\n",
       "         ('malcolm', 'says'): 15,\n",
       "         ('bartholomaeus', 'malcolm'): 7,\n",
       "         ('bartholomaeus', 'says'): 6,\n",
       "         ('bartholomaeus', 'pool'): 1,\n",
       "         ('says', 'malcolm'): 15,\n",
       "         ('says', 'bartholomaeus'): 6,\n",
       "         ('says', 'pool'): 3,\n",
       "         ('says', 'returns'): 7,\n",
       "         ('pool', 'bartholomaeus'): 1,\n",
       "         ('pool', 'says'): 3,\n",
       "         ('pool', 'returns'): 10,\n",
       "         ('pool', 'dropped'): 1,\n",
       "         ('returns', 'says'): 7,\n",
       "         ('returns', 'pool'): 10,\n",
       "         ('returns', 'dropped'): 1,\n",
       "         ('returns', '20'): 1,\n",
       "         ('dropped', 'pool'): 1,\n",
       "         ('dropped', 'returns'): 1,\n",
       "         ('dropped', '20'): 4,\n",
       "         ('dropped', 'tonne'): 1,\n",
       "         ('20', 'returns'): 1,\n",
       "         ('20', 'dropped'): 4,\n",
       "         ('20', 'year'): 10,\n",
       "         ('tonne', 'dropped'): 1,\n",
       "         ('tonne', 'year'): 4,\n",
       "         ('tonne', 'average'): 1,\n",
       "         ('year', '20'): 10,\n",
       "         ('year', 'tonne'): 4,\n",
       "         ('year', 'average'): 9,\n",
       "         ('year', 'price'): 4,\n",
       "         ('average', 'tonne'): 1,\n",
       "         ('average', 'year'): 9,\n",
       "         ('average', 'price'): 9,\n",
       "         ('average', 'past'): 2,\n",
       "         ('price', 'year'): 4,\n",
       "         ('price', 'average'): 9,\n",
       "         ('price', 'past'): 2,\n",
       "         ('price', 'years'): 2,\n",
       "         ('past', 'average'): 2,\n",
       "         ('past', 'price'): 2,\n",
       "         ('past', 'years'): 76,\n",
       "         ('years', 'price'): 2,\n",
       "         ('years', 'past'): 76,\n",
       "         ('says', 'premiums'): 1,\n",
       "         ('says', 'awb'): 33,\n",
       "         ('premiums', 'says'): 1,\n",
       "         ('premiums', 'awb'): 1,\n",
       "         ('premiums', 'achieving'): 1,\n",
       "         ('awb', 'says'): 33,\n",
       "         ('awb', 'premiums'): 1,\n",
       "         ('awb', 'achieving'): 1,\n",
       "         ('awb', 'wheat'): 66,\n",
       "         ('achieving', 'premiums'): 1,\n",
       "         ('achieving', 'awb'): 1,\n",
       "         ('achieving', 'wheat'): 1,\n",
       "         ('achieving', 'export'): 1,\n",
       "         ('wheat', 'awb'): 66,\n",
       "         ('wheat', 'achieving'): 1,\n",
       "         ('wheat', 'export'): 75,\n",
       "         ('wheat', 'monopoly'): 11,\n",
       "         ('export', 'achieving'): 1,\n",
       "         ('export', 'wheat'): 75,\n",
       "         ('export', 'monopoly'): 8,\n",
       "         ('export', 'severely'): 1,\n",
       "         ('monopoly', 'wheat'): 11,\n",
       "         ('monopoly', 'export'): 8,\n",
       "         ('monopoly', 'severely'): 1,\n",
       "         ('monopoly', 'eroded'): 1,\n",
       "         ('severely', 'export'): 1,\n",
       "         ('severely', 'monopoly'): 1,\n",
       "         ('severely', 'eroded'): 1,\n",
       "         ('eroded', 'monopoly'): 1,\n",
       "         ('eroded', 'severely'): 1,\n",
       "         ('sa', 'farmers'): 4,\n",
       "         ('sa', 'help'): 2,\n",
       "         ('farmers', 'sa'): 4,\n",
       "         ('farmers', 'help'): 42,\n",
       "         ('farmers', 'fire'): 3,\n",
       "         ('help', 'sa'): 2,\n",
       "         ('help', 'farmers'): 42,\n",
       "         ('help', 'fire'): 2,\n",
       "         ('help', 'ravaged'): 1,\n",
       "         ('fire', 'farmers'): 3,\n",
       "         ('fire', 'help'): 2,\n",
       "         ('fire', 'ravaged'): 3,\n",
       "         ('fire', 'neighbours'): 1,\n",
       "         ('ravaged', 'help'): 1,\n",
       "         ('ravaged', 'fire'): 3,\n",
       "         ('ravaged', 'neighbours'): 1,\n",
       "         ('ravaged', 'farmers'): 1,\n",
       "         ('neighbours', 'fire'): 1,\n",
       "         ('neighbours', 'ravaged'): 1,\n",
       "         ('neighbours', 'farmers'): 1,\n",
       "         ('neighbours', 'south'): 1,\n",
       "         ('farmers', 'ravaged'): 1,\n",
       "         ('farmers', 'neighbours'): 1,\n",
       "         ('farmers', 'south'): 44,\n",
       "         ('farmers', 'australia'): 35,\n",
       "         ('south', 'neighbours'): 1,\n",
       "         ('south', 'farmers'): 44,\n",
       "         ('south', 'australia'): 248,\n",
       "         ('south', 's'): 117,\n",
       "         ('australia', 'farmers'): 35,\n",
       "         ('australia', 'south'): 248,\n",
       "         ('s', 'south'): 117,\n",
       "         ('s', 'east'): 31,\n",
       "         ('south', 'east'): 85,\n",
       "         ('south', 'donating'): 1,\n",
       "         ('east', 's'): 31,\n",
       "         ('east', 'south'): 85,\n",
       "         ('east', 'donating'): 1,\n",
       "         ('east', 'truckloads'): 1,\n",
       "         ('donating', 'south'): 1,\n",
       "         ('donating', 'east'): 1,\n",
       "         ('donating', 'truckloads'): 1,\n",
       "         ('donating', 'hay'): 1,\n",
       "         ('truckloads', 'east'): 1,\n",
       "         ('truckloads', 'donating'): 1,\n",
       "         ('truckloads', 'hay'): 1,\n",
       "         ('truckloads', 'neighbours'): 1,\n",
       "         ('hay', 'donating'): 1,\n",
       "         ('hay', 'truckloads'): 1,\n",
       "         ('hay', 'neighbours'): 1,\n",
       "         ('hay', 'border'): 1,\n",
       "         ('neighbours', 'truckloads'): 1,\n",
       "         ('neighbours', 'hay'): 1,\n",
       "         ('neighbours', 'border'): 1,\n",
       "         ('neighbours', 'wake'): 1,\n",
       "         ('border', 'hay'): 1,\n",
       "         ('border', 'neighbours'): 1,\n",
       "         ('border', 'wake'): 1,\n",
       "         ('border', 'grampians'): 1,\n",
       "         ('wake', 'neighbours'): 1,\n",
       "         ('wake', 'border'): 1,\n",
       "         ('wake', 'grampians'): 1,\n",
       "         ('wake', 'bushfires'): 1,\n",
       "         ('grampians', 'border'): 1,\n",
       "         ('grampians', 'wake'): 1,\n",
       "         ('grampians', 'bushfires'): 2,\n",
       "         ('bushfires', 'wake'): 1,\n",
       "         ('bushfires', 'grampians'): 2,\n",
       "         ('days', 'farmers'): 2,\n",
       "         ('days', 'donated'): 1,\n",
       "         ('farmers', 'days'): 2,\n",
       "         ('farmers', 'donated'): 2,\n",
       "         ('farmers', '250'): 1,\n",
       "         ('donated', 'days'): 1,\n",
       "         ('donated', 'farmers'): 2,\n",
       "         ('donated', '250'): 1,\n",
       "         ('donated', 'tonnes'): 1,\n",
       "         ('250', 'farmers'): 1,\n",
       "         ('250', 'donated'): 1,\n",
       "         ('250', 'tonnes'): 1,\n",
       "         ('250', 'hay'): 1,\n",
       "         ('tonnes', 'donated'): 1,\n",
       "         ('tonnes', '250'): 1,\n",
       "         ('tonnes', 'hay'): 2,\n",
       "         ('tonnes', 'agistment'): 1,\n",
       "         ('hay', '250'): 1,\n",
       "         ('hay', 'tonnes'): 2,\n",
       "         ('hay', 'agistment'): 1,\n",
       "         ('hay', 'cattle'): 1,\n",
       "         ('agistment', 'tonnes'): 1,\n",
       "         ('agistment', 'hay'): 1,\n",
       "         ('agistment', 'cattle'): 1,\n",
       "         ('cattle', 'hay'): 1,\n",
       "         ('cattle', 'agistment'): 1,\n",
       "         ('fodder', 'drive'): 1,\n",
       "         ('fodder', 'coordinator'): 1,\n",
       "         ('drive', 'fodder'): 1,\n",
       "         ('drive', 'coordinator'): 1,\n",
       "         ('drive', 'peter'): 1,\n",
       "         ('coordinator', 'fodder'): 1,\n",
       "         ('coordinator', 'drive'): 1,\n",
       "         ('coordinator', 'peter'): 2,\n",
       "         ('coordinator', 'o'): 1,\n",
       "         ('peter', 'drive'): 1,\n",
       "         ('peter', 'coordinator'): 2,\n",
       "         ('peter', 'o'): 2,\n",
       "         ('peter', 'conner'): 1,\n",
       "         ('o', 'coordinator'): 1,\n",
       "         ('o', 'peter'): 2,\n",
       "         ('o', 'conner'): 1,\n",
       "         ('conner', 'peter'): 1,\n",
       "         ('conner', 'o'): 1,\n",
       "         ('conner', 'says'): 1,\n",
       "         ('conner', 'overwhelmed'): 1,\n",
       "         ('says', 'conner'): 1,\n",
       "         ('says', 'overwhelmed'): 1,\n",
       "         ('says', 'response'): 13,\n",
       "         ('overwhelmed', 'conner'): 1,\n",
       "         ('overwhelmed', 'says'): 1,\n",
       "         ('overwhelmed', 'response'): 1,\n",
       "         ('response', 'says'): 13,\n",
       "         ('response', 'overwhelmed'): 1,\n",
       "         ('hay', 's'): 5,\n",
       "         ('hay', 'going'): 2,\n",
       "         ('s', 'hay'): 5,\n",
       "         ('s', 'going'): 112,\n",
       "         ...})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count co-occurences in the skipgrams\n",
    "X_ik_skipgram = Counter(skip_grams)\n",
    "\n",
    "X_ik_skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(X_ik_skipgram[('prime', 'minister')])\n",
    "print(X_ik_skipgram[('prime', 'director')])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Weighting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the weighting function\n",
    "def weighting(w_i, w_j, X_ik):\n",
    "    \n",
    "    x_ij = X_ik[(w_i, w_j)]\n",
    "\n",
    "    #label smoothing if there is no co-occurence (i.e., x_ij is 0)\n",
    "    if x_ij == 0:\n",
    "        x_ij = 1\n",
    "        \n",
    "    #maximum co-occurrences is 100 according to the paper\n",
    "    x_max = 100\n",
    "    alpha = 0.75\n",
    "    \n",
    "    #if the co-occurrences does not exceed x_max, scale it down based on some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij/x_max) ** alpha\n",
    "    else:\n",
    "        result = 1 #the maximum probability\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9393790503782488\n"
     ]
    }
   ],
   "source": [
    "#test the weighting function\n",
    "w_i  = 'prime'\n",
    "w_j  = 'minister'\n",
    "\n",
    "print(weighting(w_i, w_j, X_ik_skipgram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03162277660168379\n",
      "0.03162277660168379\n"
     ]
    }
   ],
   "source": [
    "w_i  = 'prime'\n",
    "w_j  = 'director'\n",
    "\n",
    "print(weighting(w_i, w_j, X_ik_skipgram))\n",
    "print((1 / 100) ** 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply this weighting to all possible pairs\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {} #for keeping the co-occurrences\n",
    "weighting_dic = {} #for keeping all the probabilities after passing through the weighting function\n",
    "\n",
    "for bigram in combinations_with_replacement(vocabs, 2):  #we need to also think its reverse\n",
    "    #if this bigram exists in X_ik_skipgrams\n",
    "    #we gonna add this to our co-occurence matrix\n",
    "    if X_ik_skipgram.get(bigram) is not None:\n",
    "        cooc = X_ik_skipgram[bigram]  #get the co-occurrence\n",
    "        X_ik[bigram] = cooc + 1 #this is again basically label smoothing\n",
    "        X_ik[(bigram[1], bigram[0])] = cooc + 1  #trick to get all pairs\n",
    "    else: #otherwise, put 0\n",
    "        X_ik[bigram] = 0\n",
    "        X_ik[(bigram[1], bigram[0])] = 0\n",
    "\n",
    "    #apply the weighting function using this co-occurrence matrix thingy    \n",
    "    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the weighting function\n",
    "print(X_ik_skipgram[('managing', 'director')])\n",
    "print(X_ik_skipgram[('director', 'managing')])\n",
    "\n",
    "print(X_ik[('managing', 'director')])\n",
    "print(X_ik[('director', 'managing')])\n",
    "\n",
    "print(weighting_dic[('managing', 'director')])\n",
    "print(weighting_dic[('director', 'managing')])\n",
    "\n",
    "print((5 / 100) ** 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_ik_skipgram[('prime', 'director')])\n",
    "print(X_ik[('prime', 'director')])\n",
    "print(weighting_dic[('prime', 'director')])\n",
    "print((1 / 100) ** 0.75)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random batch for GloVe with generic batch size, corpus and skipgrams\n",
    "import math\n",
    "\n",
    "def random_batch_glove(batch_size, corpus, skip_grams, X_ik, weighting_dic):\n",
    "    \n",
    "    #change words in the skipgrams to idices\n",
    "    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    \n",
    "    #randomly pick \"batch_size\" indices\n",
    "    number_of_choices = len(skip_grams_id)\n",
    "    random_index = np.random.choice(number_of_choices, batch_size, replace=False) #no repeating indexes among these random indexes\n",
    "    \n",
    "    random_inputs = [] #xi, wi (in batches)\n",
    "    random_labels = [] #xj, wj (in batches)\n",
    "    random_coocs  = [] #Xij (in batches)\n",
    "    random_weightings = [] #weighting_dic(Xij) (in batches)\n",
    "    #for each of the sample in these indexes\n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams_id[i][0]])\n",
    "        random_labels.append([skip_grams_id[i][1]])\n",
    "        \n",
    "        #get cooc\n",
    "        #first check whether it exists...\n",
    "        pair = skip_grams[i]\n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1 #label smoothing\n",
    "            \n",
    "        random_coocs.append([math.log(cooc)])\n",
    "        #log according to the cost function equation\n",
    "        #bracket because neural network requires size ( , 1)\n",
    "        \n",
    "        #get weighting\n",
    "        weighting = weighting_dic[pair]\n",
    "        random_weightings.append(weighting)\n",
    "\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the method\n",
    "batch_size = 2\n",
    "inputs, targets, coocs, weightings = random_batch_glove(batch_size, corpus_tokenized, skip_grams, X_ik, weighting_dic)\n",
    "\n",
    "inputs, targets, coocs, weightings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size,embed_size):\n",
    "        super(GloVe,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, embed_size) # center embedding\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size) # context embedding\n",
    "        \n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "        \n",
    "    def forward(self, center_words, context_words, coocs, weightings):\n",
    "        center_embeds = self.embedding_v(center_words) # [batch_size, 1, emb_size]\n",
    "        context_embeds = self.embedding_u(context_words) # [batch_size, 1, emb_size]\n",
    "        \n",
    "        center_bias = self.v_bias(center_words).squeeze(1)\n",
    "        context_bias = self.u_bias(context_words).squeeze(1)\n",
    "        \n",
    "        inner_product = context_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "        \n",
    "        #note that coocs is already log\n",
    "        loss = weightings * torch.pow(inner_product + center_bias + context_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set training parameters\n",
    "batch_size  = 10\n",
    "vocab_size  = len(vocabs)\n",
    "emb_size    = 50\n",
    "model       = GloVe(vocab_size, emb_size)\n",
    "optimizer   = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate epoch time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time // 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "import time\n",
    "\n",
    "num_epochs = 500\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    inputs, targets, coocs, weightings = random_batch_glove(batch_size, corpus_tokenized, skip_grams, X_ik, weighting_dic)\n",
    "    input_batch     = torch.LongTensor(inputs)\n",
    "    target_batch    = torch.LongTensor(targets)\n",
    "    cooc_batch      = torch.FloatTensor(coocs)\n",
    "    weighting_batch = torch.FloatTensor(weightings)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(input_batch, target_batch, cooc_batch, weighting_batch)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        end = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6. Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the GloVe model with pickle\n",
    "import pickle\n",
    "\n",
    "pickle.dump(model, open('GloVe.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random batch for CBOW model with default window size and batch size of 1 each\n",
    "def random_batch_cbow(window_size=1, batch_size=1): \n",
    "    cbow = []\n",
    "    for sentence in corpus_tokenized:\n",
    "        #I include the first and last words\n",
    "        #so that every word could be a center word\n",
    "        for i in range(len(sentence)):\n",
    "            center = word2index[sentence[i]]\n",
    "            context = []\n",
    "            context_idx = []\n",
    "            #group the indices of the context words\n",
    "            for j in range(window_size):\n",
    "                context_idx.append(i - window_size + j)\n",
    "            for k in range(1, window_size + 1):\n",
    "                context_idx.append(i + k)\n",
    "            #append the context words based on their indices\n",
    "            #append <UNK> if there is no word at an index\n",
    "            for idx in context_idx:\n",
    "                if idx < 0:\n",
    "                    context.append(word2index['<UNK>'])\n",
    "                elif idx >= len(sentence):\n",
    "                    context.append(word2index['<UNK>'])\n",
    "                else:\n",
    "                    context.append(word2index[sentence[idx]])\n",
    "            cbow.append([context, center])\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(cbow)), batch_size, replace=False)\n",
    "    for i in random_index:\n",
    "        random_inputs.append(cbow[i][0])\n",
    "        random_labels.append([cbow[i][1]])\n",
    "            \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the CBOW method\n",
    "input_batch, target_batch = random_batch_cbow(2, 10)\n",
    "\n",
    "print(\"Input: \", input_batch)\n",
    "print(\"Target: \", target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CBOW model\n",
    "class CBOW(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(CBOW,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, emb_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, emb_size)\n",
    "    \n",
    "    def forward(self, context_words, center_words, all_vocabs):\n",
    "        center_embeds = self.embedding_v(center_words)  #[batch_size, 1, emb_size]\n",
    "        context_embeds = self.embedding_u(context_words)  #[batch_size, window_size, emb_size]\n",
    "        all_embeds    = self.embedding_v(all_vocabs)    #[batch_size, vocab_size, emb_size]\n",
    "        \n",
    "        scores      = center_embeds.bmm(context_embeds.transpose(1, 2)).squeeze(2)\n",
    "\n",
    "        norm_scores = all_embeds.bmm(context_embeds.transpose(1, 2)).squeeze(2)\n",
    "\n",
    "        nll = -torch.mean(torch.log(torch.exp(scores)/torch.sum(torch.exp(norm_scores), 1).unsqueeze(1)))\n",
    "            \n",
    "        return nll # negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set training parameters\n",
    "window_size = 2\n",
    "batch_size = 10\n",
    "vocab_size = len(vocabs)\n",
    "emb_size = 50\n",
    "model = CBOW(vocab_size, emb_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#convert all vocabs to tensors\n",
    "def prepare_sequence(vocabs, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], vocabs))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, vocab_size)\n",
    "\n",
    "#calculate epoch time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time // 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "#train the model\n",
    "import time\n",
    "\n",
    "num_epochs = 500\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    input_batch, target_batch = random_batch_cbow(window_size, batch_size)\n",
    "    input_batch  = torch.LongTensor(input_batch)  #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch) #[batch_size, 1]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(input_batch, target_batch, all_vocabs)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        end = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the CBOW model\n",
    "pickle.dump(model, open('CBOW.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random batch for skip-gram model with default window size and batch size of 1 each\n",
    "def random_batch_skip_gram(window_size=1, batch_size=1): \n",
    "    skip_grams = []\n",
    "    for sentence in corpus_tokenized:\n",
    "        #I include the first and last words\n",
    "        #so that every word could be a center word\n",
    "        for i in range(len(sentence)):\n",
    "            center = word2index[sentence[i]]\n",
    "            context = []\n",
    "            for j in range(window_size):\n",
    "                if (i - window_size + j) >= 0:\n",
    "                    context.append(word2index[sentence[i - window_size + j]])\n",
    "            for k in range(1, window_size + 1):\n",
    "                if (i + k) < len(sentence):\n",
    "                    context.append(word2index[sentence[i + k]])\n",
    "            for w in context:\n",
    "                skip_grams.append([center, w])\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False)\n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams[i][0]])\n",
    "        random_labels.append([skip_grams[i][1]])\n",
    "            \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the skip-gram method\n",
    "input_batch, target_batch = random_batch_skip_gram(2, 10)\n",
    "\n",
    "print(\"Input: \", input_batch)\n",
    "print(\"Target: \", target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip-gram model\n",
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(Skipgram,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, emb_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, emb_size)\n",
    "    \n",
    "    def forward(self, center_words, context_words, all_vocabs):\n",
    "        center_embeds = self.embedding_v(center_words)  #[batch_size, 1, emb_size]\n",
    "        context_embeds = self.embedding_u(context_words)  #[batch_size, 1, emb_size]\n",
    "        all_embeds    = self.embedding_u(all_vocabs)    #[batch_size, vocab_size, emb_size]\n",
    "        \n",
    "        scores      = context_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "\n",
    "        norm_scores = all_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, vocab_size, emb_size] @ [batch_size, emb_size, 1] = [batch_size, vocab_size, 1] = [batch_size, vocab_size]\n",
    "\n",
    "        nll = -torch.mean(torch.log(torch.exp(scores)/torch.sum(torch.exp(norm_scores), 1).unsqueeze(1))) # log-softmax\n",
    "        # scalar (loss must be scalar)    \n",
    "            \n",
    "        return nll # negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set training parameters\n",
    "window_size = 2\n",
    "batch_size = 10\n",
    "vocab_size = len(vocabs)\n",
    "emb_size = 50\n",
    "model = Skipgram(vocab_size, emb_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#convert all vocabs to tensors\n",
    "def prepare_sequence(vocabs, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], vocabs))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, vocab_size)\n",
    "\n",
    "#calculate epoch time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time // 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "#train the model\n",
    "import time\n",
    "\n",
    "num_epochs = 500\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    input_batch, target_batch = random_batch_skip_gram(window_size, batch_size)\n",
    "    input_batch  = torch.LongTensor(input_batch)  #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch) #[batch_size, 1]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(input_batch, target_batch, all_vocabs)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        end = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the skip-gram model\n",
    "pickle.dump(model, open('Skipgram.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Skip-gram with Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count all the occurrences of vocabs\n",
    "from collections import Counter\n",
    "word_count = Counter(flatten(corpus_tokenized))\n",
    "\n",
    "#count the number of total words\n",
    "num_total_words = sum([c for w, c in word_count.items()])\n",
    "\n",
    "#create the scaled-up unigram distribution table for vocabs\n",
    "z = 0.001 #the scaler\n",
    "unigram_table = []\n",
    "for v in vocabs:\n",
    "    unigram_table.extend([v] * int(((word_count[v]/num_total_words)**0.75)/z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert word indices to tensors\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "#generate random negative samples\n",
    "import random\n",
    "\n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.size(0)\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):\n",
    "        nsample = []\n",
    "        target_index = targets[i].item()\n",
    "        while len(nsample) < k:\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).view(1, -1))\n",
    "    \n",
    "    return torch.cat(neg_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the negative sampling method\n",
    "input_batch, target_batch = random_batch_skip_gram(2, 10)\n",
    "input_batch = torch.LongTensor(input_batch)\n",
    "target_batch = torch.LongTensor(target_batch)\n",
    "\n",
    "num_neg = 5 #number of negative samples for each target word\n",
    "\n",
    "neg_samples = negative_sampling(target_batch, unigram_table, num_neg)\n",
    "\n",
    "neg_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip-gram with negative sampling model\n",
    "class SkipgramNegSampling(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(SkipgramNegSampling, self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, emb_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, emb_size)\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "                    \n",
    "    def forward(self, center_words, context_words, neg_samples):\n",
    "        center_embeds  = self.embedding_v(center_words) # [batch_size, 1, emb_size]\n",
    "        context_embeds = self.embedding_u(context_words) # [batch_size, 1, emb_size]\n",
    "        neg_embeds     = self.embedding_u(neg_samples) # [batch_size, num_neg, emb_size]\n",
    "        \n",
    "        positive_score = context_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "        \n",
    "        negative_score = -neg_embeds.bmm(center_embeds.transpose(1, 2))\n",
    "        #[batch_size, k, emb_size] @ [batch_size, emb_size, 1] = [batch_size, num_neg, 1]\n",
    "        \n",
    "        loss = -torch.mean(self.logsigmoid(positive_score) + torch.sum(self.logsigmoid(negative_score), 1))\n",
    "                \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set parameters\n",
    "window_size = 2\n",
    "batch_size  = 10\n",
    "vocab_size  = len(vocabs)\n",
    "emb_size    = 50\n",
    "model       = SkipgramNegSampling(vocab_size, emb_size)\n",
    "num_neg     = 10\n",
    "optimizer   = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#calculate epoch time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time // 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "#train the model\n",
    "import time\n",
    "\n",
    "num_epochs = 500\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):    \n",
    "    input_batch, target_batch = random_batch_skip_gram(window_size, batch_size)\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    target_batch = torch.LongTensor(target_batch)\n",
    "    neg_samples = negative_sampling(target_batch, unigram_table, num_neg)\n",
    "    \n",
    "    optimizer.zero_grad()   \n",
    "    loss = model(input_batch, target_batch, neg_samples)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        end = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the skip-gram with negative sampling model\n",
    "pickle.dump(model, open('SkipgramNegSampling.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. Preparing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset for testing\n",
    "file_path = \"C:/Users/MARC/Downloads/Datasets/questions-words.txt\"\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    contents = f.read()\n",
    "    data = contents.split('\\n')\n",
    "\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore the dataset\n",
    "for idx, sent in enumerate(data):\n",
    "    if sent[0] == ':':\n",
    "        print(idx, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the corpora for testing\n",
    "family = data[8368:8874]\n",
    "family_tokenized = [sent.split(' ') for sent in family]\n",
    "print(family_tokenized[:5])\n",
    "\n",
    "plural = data[17355:18687]\n",
    "plural_tokenized = [sent.split(' ') for sent in plural]\n",
    "print(plural_tokenized[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get unique words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "family_vocabs  = list(set(flatten(family_tokenized)))\n",
    "plural_vocabs  = list(set(flatten(plural_tokenized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numericalize the vocabs\n",
    "family_word2index = {w: i for i, w in enumerate(family_vocabs)}\n",
    "plural_word2index = {w: i for i, w in enumerate(plural_vocabs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append <UNK>\n",
    "family_vocabs.append('<UNK>')\n",
    "family_word2index['<UNK>'] = len(family_word2index)\n",
    "\n",
    "plural_vocabs.append('<UNK>')\n",
    "plural_word2index['<UNK>'] = len(plural_word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare index2word\n",
    "family_index2word = {i:w for w, i in family_word2index.items()}\n",
    "plural_index2word = {i:w for w, i in plural_word2index.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. Syntactic Test\n",
    "\n",
    "##### The 'plural' corpus will be used for syntactic analogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Semantic Test\n",
    "\n",
    "The 'family' corpus will be used for semantic analogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3. Similarity Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7ca07084f99cae884d00a2401f5a915152405bd446c47609c53897335e04337"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
