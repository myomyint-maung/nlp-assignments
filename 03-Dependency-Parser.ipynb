{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 26 Jan - Dependency Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parsing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class for parsing\n",
    "class Parsing(object):\n",
    "    def __init__(self, sentence):\n",
    "        self.sentence = sentence\n",
    "        self.stack = ['ROOT']\n",
    "        self.buffer = sentence[:]\n",
    "        self.dep = []\n",
    "\n",
    "    def parse_step(self, transition):\n",
    "        if transition == 'S':\n",
    "            buffer_head = self.buffer.pop(0)\n",
    "            self.stack.append(buffer_head)\n",
    "\n",
    "        elif transition == 'LA':\n",
    "            dependent = self.stack.pop(-2)\n",
    "            self.dep.append((self.stack[-1], dependent))\n",
    "        \n",
    "        elif transition == 'RA':\n",
    "            dependent = self.stack.pop()\n",
    "            self.dep.append((self.stack[-1], dependent))\n",
    "        else:\n",
    "            print(f\"Unknown transition: {transition}\")\n",
    "    \n",
    "    def parse(self, transitions):\n",
    "        for transition in transitions:\n",
    "            self.parse_step(transition)\n",
    "        return self.dep\n",
    "    \n",
    "    def is_completed(self):\n",
    "        return (len(self.buffer) == 0) and (len(self.stack) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ROOT', 'has'],\n",
       " ['.'],\n",
       " [('has', 'He'), ('control', 'good'), ('has', 'control')])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the parsing class\n",
    "parsing = Parsing(['He', 'has', 'good', 'control', '.'])\n",
    "parsing.parse([\"S\",\"S\", \"LA\", \"S\", \"S\", \"LA\", \"RA\"])\n",
    "parsing.stack, parsing.buffer, parsing.dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a minibatch parsing function\n",
    "def minibatch_parse(sentences, model, batch_size):\n",
    "    dep = []\n",
    "    partial_parses = [Parsing(sentence) for sentence in sentences]\n",
    "    unfinished_parses = partial_parses[:]\n",
    "\n",
    "    while unfinished_parses:\n",
    "        minibatch = unfinished_parses[:batch_size]\n",
    "        transitions = model.predict(minibatch)\n",
    "\n",
    "        for transition, partial_parse in zip(transitions, minibatch):\n",
    "            partial_parse.parse_step(transition)\n",
    "\n",
    "        unfinished_parses[:] = [p for p in unfinished_parses if not p.is_completed()]\n",
    "    \n",
    "    dep = [parse.dep for parse in partial_parses]\n",
    "\n",
    "    return dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy model to predict transitions\n",
    "class DummyModel(object):\n",
    "    def predict(self, partial_parses):\n",
    "        return [(\"RA\" if pp.stack[1] == \"right\" else \"LA\")\n",
    "                if len(pp.buffer) == 0 else \"S\"\n",
    "                for pp in partial_parses] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('arcs', 'only'), ('right', 'arcs'), ('ROOT', 'right')],\n",
       " [('only', 'again'), ('arcs', 'only'), ('right', 'arcs'), ('ROOT', 'right')],\n",
       " [('only', 'arcs'), ('only', 'left'), ('only', 'ROOT')],\n",
       " [('again', 'only'), ('again', 'arcs'), ('again', 'left'), ('again', 'ROOT')]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the minibatch parsing function\n",
    "sentences = [[\"right\", \"arcs\", \"only\"],\n",
    "             [\"right\", \"arcs\", \"only\", \"again\"],\n",
    "             [\"left\", \"arcs\", \"only\"],\n",
    "             [\"left\", \"arcs\", \"only\", \"again\"]]\n",
    "\n",
    "minibatch_parse(sentences, DummyModel(), 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to read files in CoNLL format\n",
    "def read_conll(filename):\n",
    "    \n",
    "    examples = []\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        i = 0\n",
    "        word, pos, head, dep = [], [], [], []\n",
    "        for line in f.readlines():\n",
    "            i = i+1\n",
    "            wa = line.strip().split('\\t')\n",
    "\n",
    "            if len(wa) == 10:\n",
    "                word.append(wa[1].lower())\n",
    "                pos.append(wa[4])\n",
    "                head.append(int(wa[6]))\n",
    "                dep.append(wa[7])\n",
    "            \n",
    "            elif len(word) > 0:\n",
    "                examples.append({'word': word, 'pos': pos, 'head': head, 'dep': dep})\n",
    "                word, pos, head, dep = [], [], [], []\n",
    "        \n",
    "        if len(word) > 0:\n",
    "            examples.append({'word': word, 'pos': pos, 'head': head, 'dep': dep})\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to load the English Penn Treebank dataset\n",
    "def load_data():\n",
    "    print(\"1. Loading data\")\n",
    "    train_set = read_conll(\"data/train.conll\")\n",
    "    dev_set   = read_conll(\"data/dev.conll\")\n",
    "    test_set   = read_conll(\"data/test.conll\")\n",
    "    \n",
    "    #make my dataset smaller because my laptop cannot handle it\n",
    "    train_set = train_set[:1000]\n",
    "    dev_set   = dev_set[:500]\n",
    "    test_set  = test_set[:500]\n",
    "    \n",
    "    return train_set, dev_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 500, 500)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the load function\n",
    "train_set, dev_set, test_set = load_data()\n",
    "\n",
    "len(train_set), len(dev_set), len(test_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a parser based on this paper: https://aclanthology.org/D14-1082.pdf\n",
    "P_PREFIX = '<p>:' #indicating pos tags\n",
    "D_PREFIX = '<d>:' #indicating dependency tags\n",
    "UNK      = '<UNK>'\n",
    "NULL     = '<NULL>'\n",
    "ROOT     = '<ROOT>'\n",
    "\n",
    "class Parser(object):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        #set the root dep\n",
    "        self.root_dep = 'root'\n",
    "                \n",
    "        #get all the dep of the dataset as list, e.g., ['root', 'acl', 'nmod', 'nmod:npmod']\n",
    "        all_dep = [self.root_dep] + list(set([w for ex in dataset\n",
    "                                               for w in ex['dep']\n",
    "                                               if w != self.root_dep]))\n",
    "        \n",
    "        #1. put dep into tok2id lookup table, with D_PREFIX so we know it is dependency\n",
    "        #{'D_PREFIX:root': 0, 'D_PREFIX:acl': 1, 'D_PREFIX:nmod': 2, ..., 'D_PREFIX:<NULL>': 30}\n",
    "        tok2id = {D_PREFIX + l: i for (i, l) in enumerate(all_dep)}\n",
    "        tok2id[D_PREFIX + NULL] = self.D_NULL = len(tok2id)\n",
    "        \n",
    "        #we are using \"unlabeled\" where we do not label with the dependency\n",
    "        #thus the number of dependency relation is 1\n",
    "        trans = ['L', 'R', 'S']\n",
    "        self.n_deprel = 1\n",
    "        \n",
    "        #create a simple lookup table mapping action and id\n",
    "        #e.g., tran2id: {'L': 0, 'R': 1, 'S': 2}\n",
    "        #e.g., id2tran: {0: 'L', 1: 'R', 2: 'S'}\n",
    "        self.n_trans = len(trans)\n",
    "        self.tran2id = {t: i for (i, t) in enumerate(trans)}\n",
    "        self.id2tran = {i: t for (i, t) in enumerate(trans)}\n",
    "\n",
    "        #2. put pos tags into tok2id lookup table, with P_PREFIX so we know it is pos\n",
    "        tok2id.update(build_dict([P_PREFIX + w for ex in dataset for w in ex['pos']],\n",
    "                                  offset=len(tok2id)))\n",
    "        tok2id[P_PREFIX + UNK]  = self.P_UNK  = len(tok2id)  #also remember the pos tags of unknown\n",
    "        tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)\n",
    "        tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)\n",
    "        \n",
    "        #now tok2id:  {'P_PREFIX:root': 0, 'P_PREFIX:acl': 1, ..., 'P_PREFIX:JJR': 62, 'P_PREFIX:<UNK>': 63, 'P_PREFIX:<NULL>': 64, 'P_PREFIX:<ROOT>': 65}\n",
    "        \n",
    "        #3. put word into tok2id lookup table\n",
    "        tok2id.update(build_dict([w for ex in dataset for w in ex['word']],\n",
    "                                  offset=len(tok2id)))\n",
    "        tok2id[UNK]  = self.UNK = len(tok2id)\n",
    "        tok2id[NULL] = self.NULL = len(tok2id)\n",
    "        tok2id[ROOT] = self.ROOT = len(tok2id)\n",
    "        \n",
    "        #now tok2id: {'D_PREFIX:root': 0, 'D_PREFIX:acl': 1, 'D_PREFIX:nmod': 2, ..., 'memory': 340, 'mr.': 341, '<UNK>': 342, '<NULL>': 343, '<ROOT>': 344}\n",
    "        \n",
    "        #create id2tok\n",
    "        self.tok2id = tok2id\n",
    "        self.id2tok = {v: k for (k, v) in tok2id.items()}\n",
    "        \n",
    "        #why 18 normal features + 18 (pos) + 12 (dep)\n",
    "        #18 features - top 3 words on buffer, top 3 words on stack, \n",
    "        # the first and second left most/rightmost children of the top two words on the stack\n",
    "        # the leftmost of leftmost/rightmost of rightmost children of the top two words on the stack\n",
    "        #18 pos - basically corresponding POS tags\n",
    "        #12 dep - corresponding ARC, excluding 6 words on hte stack/buffer..\n",
    "        self.n_features = 18 + 18 + 12\n",
    "        self.n_tokens = len(tok2id)\n",
    "    \n",
    "    #function to turn train set with words to train set with id instead using tok2id\n",
    "    def numericalize(self, examples):\n",
    "        numer_examples = []\n",
    "        for ex in examples:\n",
    "            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id\n",
    "                                  else self.UNK for w in ex['word']]\n",
    "            pos  = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id\n",
    "                                   else self.P_UNK for w in ex['pos']]\n",
    "            head = [-1] + ex['head']\n",
    "            dep  = [-1] + [self.tok2id[D_PREFIX + w] if D_PREFIX + w in self.tok2id\n",
    "                            else -1 for w in ex['dep']]\n",
    "            numer_examples.append({'word': word, 'pos': pos,\n",
    "                                 'head': head, 'dep': dep})\n",
    "        return numer_examples\n",
    "\n",
    "    #function to extract features to form a feature embedding matrix\n",
    "    def extract_features(self, stack, buf, arcs, ex):\n",
    "             \n",
    "        #ex['word']:  [55, 32, 33, 34, 35, 30], i.e., ['root', 'ms.', 'haag', 'plays', 'elianti', '.']\n",
    "        #ex['pos']:   [29, 14, 14, 16, 14, 17], i.e., ['NNP', 'NNP', 'VBZ', 'NNP', '.']\n",
    "        #ex['head']:  [-1, 2, 3, 0, 3, 3]  or ['root', 'compound', 'nsubj', 'root', 'dobj', 'punct']}\n",
    "        #ex['dep']:   [-1, 1, 2, 0, 6, 12] or ['compound', 'nsubj', 'root', 'dobj', 'punct']\n",
    "\n",
    "        #stack     :  [0]\n",
    "        #buffer    :  [1, 2, 3, 4, 5]\n",
    "        \n",
    "        if stack[0] == \"ROOT\":\n",
    "            stack[0] = 0  #start the stack with [ROOT]\n",
    "\n",
    "        #get leftmost children based on the dependency arcs\n",
    "        def get_lc(k):\n",
    "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] < k])\n",
    "\n",
    "        #get right most children based on the dependency arcs\n",
    "        def get_rc(k):\n",
    "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] > k],\n",
    "                          reverse=True)\n",
    "\n",
    "        p_features = [] #pos features (2a, 2b, 2c) - 18\n",
    "        d_features = [] #dep features (3b, 3c) - 12\n",
    "        \n",
    "        #last 3 things on the stack as features\n",
    "        #if the stack is less than 3, then we simply append NULL from the left\n",
    "        features = [self.NULL] * (3 - len(stack)) + [ex['word'][x] for x in stack[-3:]]\n",
    "        \n",
    "        # next 3 things on the buffer as features\n",
    "        #if the buffer is less than 3, simply append NULL\n",
    "        #the reason why NULL is appended on end because buffer is read left to right\n",
    "        features += [ex['word'][x] for x in buf[:3]] + [self.NULL] * (3 - len(buf))\n",
    "        \n",
    "        #corresponding pos tags\n",
    "        p_features = [self.P_NULL] * (3 - len(stack)) + [ex['pos'][x] for x in stack[-3:]]\n",
    "        p_features += [ex['pos'][x] for x in buf[:3]] + [self.P_NULL] * (3 - len(buf))\n",
    "        \n",
    "        #get the leftmost and rightmost children of the top two words, thus we loop 2 times\n",
    "        for i in range(2):\n",
    "            if i < len(stack):\n",
    "                k = stack[-i-1] #-1, -2 last two in the stack\n",
    "                \n",
    "                #the first and second lefmost/rightmost children of the top two words (i=1, 2) on the stack\n",
    "                lc = get_lc(k)  \n",
    "                rc = get_rc(k)\n",
    "                \n",
    "                #the leftmost of leftmost/rightmost of rightmost children of the top two words on the stack:\n",
    "                llc = get_lc(lc[0]) if len(lc) > 0 else []\n",
    "                rrc = get_rc(rc[0]) if len(rc) > 0 else []\n",
    "\n",
    "                #(leftmost of first word on stack, rightmost of first word, \n",
    "                # leftmost of the second word on stack, rightmost of second, \n",
    "                # leftmost of leftmost, rightmost of rightmost\n",
    "                features.append(ex['word'][lc[0]] if len(lc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][rc[0]] if len(rc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][lc[1]] if len(lc) > 1 else self.NULL)\n",
    "                features.append(ex['word'][rc[1]] if len(rc) > 1 else self.NULL)\n",
    "                features.append(ex['word'][llc[0]] if len(llc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][rrc[0]] if len(rrc) > 0 else self.NULL)\n",
    "\n",
    "                #corresponding pos\n",
    "                p_features.append(ex['pos'][lc[0]] if len(lc) > 0 else self.P_NULL)\n",
    "                p_features.append(ex['pos'][rc[0]] if len(rc) > 0 else self.P_NULL)\n",
    "                p_features.append(ex['pos'][lc[1]] if len(lc) > 1 else self.P_NULL)\n",
    "                p_features.append(ex['pos'][rc[1]] if len(rc) > 1 else self.P_NULL)\n",
    "                p_features.append(ex['pos'][llc[0]] if len(llc) > 0 else self.P_NULL)\n",
    "                p_features.append(ex['pos'][rrc[0]] if len(rrc) > 0 else self.P_NULL)\n",
    "            \n",
    "                #corresponding dep\n",
    "                d_features.append(ex['dep'][lc[0]] if len(lc) > 0 else self.D_NULL)\n",
    "                d_features.append(ex['dep'][rc[0]] if len(rc) > 0 else self.D_NULL)\n",
    "                d_features.append(ex['dep'][lc[1]] if len(lc) > 1 else self.D_NULL)\n",
    "                d_features.append(ex['dep'][rc[1]] if len(rc) > 1 else self.D_NULL)\n",
    "                d_features.append(ex['dep'][llc[0]] if len(llc) > 0 else self.D_NULL)\n",
    "                d_features.append(ex['dep'][rrc[0]] if len(rrc) > 0 else self.D_NULL)\n",
    "                \n",
    "            else:\n",
    "                #attach NULL when they don't exist\n",
    "                features += [self.NULL] * 6\n",
    "                p_features += [self.P_NULL] * 6\n",
    "                d_features += [self.D_NULL] * 6\n",
    "\n",
    "        features += p_features + d_features\n",
    "        assert len(features) == self.n_features  #assert they are 18 + 18 + 12\n",
    "        return features\n",
    "\n",
    "    #decide whether to shift, leftarc, or rightarc, based on gold parse trees\n",
    "    #this is needed to create training examples which contain samples and ground truth\n",
    "    def get_oracle(self, stack, buf, ex):\n",
    "\n",
    "        #leave if the stack is only 1, thus nothing to predict....\n",
    "        if len(stack) < 2:\n",
    "            return self.n_trans - 1\n",
    "\n",
    "        #predict based on the last two words on the stack\n",
    "        i0 = stack[-1]\n",
    "        i1 = stack[-2]\n",
    "\n",
    "        #get the head and dependency\n",
    "        h0 = ex['head'][i0]\n",
    "        h1 = ex['head'][i1]\n",
    "        d0 = ex['dep'][i0]\n",
    "        d1 = ex['dep'][i1]\n",
    "\n",
    "        #either shift, left arc or right arc\n",
    "        #\"Shift\" = 2; \"LA\" = 0; \"RA\" = 1\n",
    "        #if head of the second last word is the last word, then leftarc\n",
    "        if (i1 > 0) and (h1 == i0):\n",
    "            return 0\n",
    "        #if head of the last word is the second last word, then rightarc\n",
    "        #make sure nothing in the buffer has head with the last word on the stack\n",
    "        #otherwise, we lose the last word.....\n",
    "        elif (i1 >= 0) and (h0 == i1) and \\\n",
    "                (not any([x for x in buf if ex['head'][x] == i0])):\n",
    "            return 1\n",
    "        #otherwise shift, if something is left in buffer, otherwise, do nothing....\n",
    "        else:\n",
    "            return None if len(buf) == 0 else 2\n",
    "\n",
    "    #generate training examples\n",
    "    #from the training sentences and their gold parse trees \n",
    "    def create_instances(self, examples):\n",
    "        all_instances = []\n",
    "        \n",
    "        for i, ex in enumerate(examples):\n",
    "            #e.g., ex['word]: [344, 163, 99, 164, 165, 68]\n",
    "            n_words = len(ex['word']) - 1  #excluding the root\n",
    "\n",
    "            #arcs = {(head, tail, dependency label)}\n",
    "            stack = [0]\n",
    "            buf = [i + 1 for i in range(n_words)]  #[1, 2, 3, 4, 5]\n",
    "            arcs = []\n",
    "            instances = []\n",
    "            \n",
    "            #because that's the maximum number of shift, leftarcs, rightarcs you can have\n",
    "            #this will determine the sample size of each training example\n",
    "            #if given five words, we will get a sample of (10, 48) where 10 comes from 5 * 2, and 48 is n_features\n",
    "            #but this for loop can be break if there is nothing left....\n",
    "            for i in range(n_words * 2):\n",
    "\n",
    "                #get the gold transition based on the parse trees\n",
    "                #gold_t can be either shift(2), leftarc(0), or rightarc(1)\n",
    "                gold_t = self.get_oracle(stack, buf, ex)\n",
    "                \n",
    "                #if gold_t is None, no need to extract features.....\n",
    "                if gold_t is None:\n",
    "                    break\n",
    "                \n",
    "                #make sure when the model predicts, we inform the current state of stack and buffer, so\n",
    "                #the model is not allowed to make any illegal action, e.g., buffer is empty but trying to pop\n",
    "                legal_labels = self.legal_labels(stack, buf)                \n",
    "                assert legal_labels[gold_t] == 1\n",
    "               \n",
    "                #extract all the 48 features \n",
    "                features = self.extract_features(stack, buf, arcs, ex)\n",
    "                instances.append((features, legal_labels, gold_t))\n",
    "            \n",
    "                #shift \n",
    "                if gold_t == 2:\n",
    "                    stack.append(buf[0])\n",
    "                    buf = buf[1:]\n",
    "                #left arc \n",
    "                elif gold_t == 0:\n",
    "                    arcs.append((stack[-1], stack[-2], gold_t))\n",
    "                    stack = stack[:-2] + [stack[-1]]\n",
    "                #right arc\n",
    "                else:\n",
    "                    arcs.append((stack[-2], stack[-1], gold_t - self.n_deprel))\n",
    "                    stack = stack[:-1]\n",
    "            else:\n",
    "                all_instances += instances\n",
    "\n",
    "        return all_instances\n",
    "\n",
    "    #provide an one hot encoding of the labels\n",
    "    def legal_labels(self, stack, buf):\n",
    "        labels =  ([1] if len(stack) > 2  else [0]) * self.n_deprel   #left arc   But cannot ROOT <----He thus 3\n",
    "        labels += ([1] if len(stack) >= 2 else [0]) * self.n_deprel   #right arc  ROOT--->He\n",
    "        labels += [1] if len(buf) > 0 else [0]   #shift\n",
    "        return labels\n",
    "    \n",
    "    #a simple function to check punctuation POS tags\n",
    "    def punct(self, pos):\n",
    "        return pos in [\"''\", \",\", \".\", \":\", \"``\", \"-LRB-\", \"-RRB-\"]\n",
    "\n",
    "    def parse(self, dataset, eval_batch_size=5000):\n",
    "        sentences = []\n",
    "        sentence_id_to_idx = {}\n",
    "                \n",
    "        for i, example in enumerate(dataset):\n",
    "            \n",
    "            #example['word']=[188, 186, 186, ..., 59]\n",
    "            #n_words=37\n",
    "            #sentence=[1, 2, 3, 4, 5,.., 37]\n",
    "                        \n",
    "            n_words = len(example['word']) - 1\n",
    "            sentence = [j + 1 for j in range(n_words)]            \n",
    "            sentences.append(sentence)\n",
    "            \n",
    "            #mapping the object unique id to the i            \n",
    "            #The id is the object's memory address\n",
    "            sentence_id_to_idx[id(sentence)] = i\n",
    "            \n",
    "        model = ModelWrapper(self, dataset, sentence_id_to_idx)\n",
    "        dependencies = minibatch_parse(sentences, model, eval_batch_size)\n",
    "                \n",
    "        UAS = all_tokens = 0.0\n",
    "        with tqdm(total=len(dataset)) as prog:\n",
    "            for i, ex in enumerate(dataset):\n",
    "                head = [-1] * len(ex['word'])\n",
    "                for h, t, in dependencies[i]:\n",
    "                    head[t] = h\n",
    "                for pred_h, gold_h, gold_l, pos in \\\n",
    "                        zip(head[1:], ex['head'][1:], ex['dep'][1:], ex['pos'][1:]):\n",
    "                        assert self.id2tok[pos].startswith(P_PREFIX)\n",
    "                        pos_str = self.id2tok[pos][len(P_PREFIX):]\n",
    "                        if (not self.punct(pos_str)):\n",
    "                            UAS += 1 if pred_h == gold_h else 0\n",
    "                            all_tokens += 1\n",
    "                prog.update(i + 1)\n",
    "        UAS /= all_tokens\n",
    "        return UAS, dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model wrapper\n",
    "class ModelWrapper(object):\n",
    "    def __init__(self, parser, dataset, sentence_id_to_idx):\n",
    "        self.parser = parser\n",
    "        self.dataset = dataset\n",
    "        self.sentence_id_to_idx = sentence_id_to_idx\n",
    "\n",
    "    def predict(self, partial_parses):\n",
    "        mb_x = [self.parser.extract_features(p.stack, p.buffer, p.dep,\n",
    "                                             self.dataset[self.sentence_id_to_idx[id(p.sentence)]])\n",
    "                for p in partial_parses]\n",
    "        mb_x = np.array(mb_x).astype('int32')\n",
    "        mb_x = torch.from_numpy(mb_x).long()\n",
    "        mb_l = [self.parser.legal_labels(p.stack, p.buffer) for p in partial_parses]\n",
    "\n",
    "        pred = self.parser.model(mb_x)\n",
    "        pred = pred.detach().numpy()\n",
    "        \n",
    "        #we need to multiply 10000 with legal labels, to force the model not to make any impossible prediction\n",
    "        #other, when we parse sequentially, sometimes there is nothing in the buffer or stack, thus error....        \n",
    "        pred = np.argmax(pred + 10000 * np.array(mb_l).astype('float32'), 1)\n",
    "        pred = [\"S\" if p == 2 else (\"LA\" if p == 0 else \"RA\") for p in pred]\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple function to create ids\n",
    "def build_dict(keys, offset=0):\n",
    "    count = Counter()\n",
    "    for key in keys:\n",
    "        count[key] += 1\n",
    "    \n",
    "    mc = count.most_common()\n",
    "    \n",
    "    return {w[0]: index + offset for (index, w) in enumerate(mc)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Building parser...\n",
      "took 0.04 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test the parser\n",
    "print(\"2. Building parser...\",)\n",
    "start = time.time()\n",
    "parser = Parser(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:  ['ms.', 'haag', 'plays', 'elianti', '.']\n",
      "Pos:   ['NNP', 'NNP', 'VBZ', 'NNP', '.']\n",
      "Head:  [2, 3, 0, 3, 3]\n",
      "Dep:   ['compound', 'nsubj', 'root', 'dobj', 'punct']\n"
     ]
    }
   ],
   "source": [
    "# Print the unnumericalized train set\n",
    "print(\"Word: \",  train_set[1]['word'])\n",
    "print(\"Pos:  \",  train_set[1]['pos'])\n",
    "print(\"Head: \",  train_set[1]['head'])\n",
    "print(\"Dep:  \",  train_set[1]['dep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Numericalizing data...\n",
      "took 0.07 seconds\n"
     ]
    }
   ],
   "source": [
    "# Test the numericalize function\n",
    "print(\"3. Numericalizing data...\",)\n",
    "start = time.time()\n",
    "train_set = parser.numericalize(train_set)\n",
    "dev_set   = parser.numericalize(dev_set)\n",
    "test_set  = parser.numericalize(test_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5156, 304, 1364, 1002, 2144, 87]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the word ids in the train set\n",
    "train_set[1]['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ROOT>\n",
      "ms.\n",
      "haag\n",
      "plays\n",
      "elianti\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Print the corresponding words\n",
    "for i in train_set[1]['word']:\n",
    "    print(parser.id2tok[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[84, 42, 42, 55, 42, 46]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the pos ids in the train set\n",
    "train_set[1]['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>:<ROOT>\n",
      "<p>:NNP\n",
      "<p>:NNP\n",
      "<p>:VBZ\n",
      "<p>:NNP\n",
      "<p>:.\n"
     ]
    }
   ],
   "source": [
    "# Print the corresponding pos\n",
    "for i in train_set[1]['pos']:\n",
    "    print(parser.id2tok[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 2, 3, 0, 3, 3]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the head ids in the train set\n",
    "train_set[1]['head']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 37, 30, 0, 23, 28]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the dependency ids in the train set\n",
    "train_set[1]['dep']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Loading pretrained embeddings...\n",
      "Embedding matrix shape (vocab, emb size):  (5157, 50)\n",
      "took 2.44 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained word embeddings\n",
    "print(\"4. Loading pretrained embeddings...\",)\n",
    "start = time.time()\n",
    "word_vectors = {}\n",
    "for line in open(\"data/en-cw.txt\").readlines():\n",
    "    we = line.strip().split() #we = word embeddings - first column: word;  the rest is embedding\n",
    "    word_vectors[we[0]] = [float(x) for x in we[1:]] #{word: [list of 50 numbers], nextword: [another list], so on...}\n",
    "    \n",
    "#create an empty embedding matrix holding the embedding lookup table (vocab size, embed dim)\n",
    "#we use random.normal instead of zeros, to keep the embedding matrix arbitrary in case word vectors don't exist....\n",
    "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
    "\n",
    "for token in parser.tok2id:\n",
    "        i = parser.tok2id[token]\n",
    "        if token in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token]\n",
    "        elif token.lower() in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "print(\"Embedding matrix shape (vocab, emb size): \", embeddings_matrix.shape)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Preprocessing training data...\n",
      "took 1.36 seconds\n"
     ]
    }
   ],
   "source": [
    "# Do preprocessing of the tranining data\n",
    "print(\"5. Preprocessing training data...\",)\n",
    "start = time.time()\n",
    "train_examples = parser.create_instances(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Minibatch loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions to get minibatches\n",
    "def get_minibatches(data, minibatch_size, shuffle=True):\n",
    "    data_size = len(data[0])\n",
    "    indices = np.arange(data_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for minibatch_start in np.arange(0, data_size, minibatch_size):\n",
    "        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n",
    "        yield [_minibatch(d, minibatch_indices) for d in data]\n",
    "\n",
    "def _minibatch(data, minibatch_idx):\n",
    "    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]\n",
    "\n",
    "def minibatches(data, batch_size):\n",
    "    x = np.array([d[0] for d in data])\n",
    "    y = np.array([d[2] for d in data])\n",
    "    one_hot = np.zeros((y.size, 3))\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return get_minibatches([x, one_hot], batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the neural parser model\n",
    "class ParserModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddings, n_features=48,\n",
    "                 hidden_size=400, n_classes=3, dropout_prob=0.5):\n",
    "\n",
    "        super(ParserModel, self).__init__()\n",
    "        self.n_features   = n_features\n",
    "        self.n_classes    = n_classes\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.embed_size   = embeddings.shape[1]\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.pretrained_embeddings = nn.Embedding(embeddings.shape[0], self.embed_size)\n",
    "        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))\n",
    "\n",
    "        self.embed_to_hidden = nn.Linear(n_features * self.embed_size, hidden_size)\n",
    "        nn.init.xavier_uniform_(self.embed_to_hidden.weight, gain=1.)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.hidden_to_logits = nn.Linear(hidden_size, n_classes)\n",
    "        nn.init.xavier_uniform_(self.hidden_to_logits.weight)\n",
    "\n",
    "    def embedding_lookup(self, t):\n",
    "        #t:  batch_size, n_features\n",
    "        batch_size = t.size()[0]\n",
    "                    \n",
    "        x = self.pretrained_embeddings(t)        \n",
    "        x = x.reshape(-1, self.n_features * self.embed_size)\n",
    "        # x = (1024, 48 * 50)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, t):\n",
    "        # t: (1024, 48)\n",
    "        embeddings = self.embedding_lookup(t)  \n",
    "    \n",
    "        # embeddings: (1024, 48 * 50)\n",
    "        hidden = self.embed_to_hidden(embeddings)\n",
    "    \n",
    "        # hidden: (1024, 200)\n",
    "        hidden_activations = F.relu(hidden)\n",
    "        # hidden_activations: (1024, 200)\n",
    "        thin_net = self.dropout(hidden_activations)\n",
    "        # thin_net: (1024, 200)\n",
    "        logits = self.hidden_to_logits(thin_net)\n",
    "        # logits: (1024, 3)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crate a class to get the average\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions for training\n",
    "def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005):\n",
    "    \n",
    "    best_dev_UAS = 0\n",
    "    \n",
    "    optimizer = optim.Adam(parser.model.parameters(), lr=0.001)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
    "        dev_UAS = train_for_epoch(\n",
    "            parser, train_data, dev_data, optimizer, loss_func, batch_size)\n",
    "        if dev_UAS > best_dev_UAS:\n",
    "            best_dev_UAS = dev_UAS\n",
    "            print(\"New best dev UAS! Saving model.\")\n",
    "            torch.save(parser.model.state_dict(), output_path)\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size):\n",
    "    \n",
    "    parser.model.train()  # Places model in \"train\" mode, i.e. apply dropout layer\n",
    "    n_minibatches = math.ceil(len(train_data) / batch_size)\n",
    "    loss_meter = AverageMeter()\n",
    "\n",
    "    with tqdm(total=(n_minibatches)) as prog:\n",
    "        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n",
    "            \n",
    "            #train_x:  batch_size, n_features\n",
    "            #train_y:  batch_size, target(=3)\n",
    "            \n",
    "            optimizer.zero_grad() \n",
    "            loss = 0.\n",
    "            train_x = torch.from_numpy(train_x).long()  #long() for int so embedding works....\n",
    "            train_y = torch.from_numpy(train_y.nonzero()[1]).long()  #get the index with 1 because torch expects label to be single integer\n",
    "\n",
    "            # Forward pass: compute predicted logits.\n",
    "            logits = parser.model(train_x)\n",
    "            # Compute loss\n",
    "            loss = loss_func(logits, train_y)\n",
    "            # Compute gradients of the loss w.r.t model parameters.\n",
    "            loss.backward()\n",
    "            # Take step with optimizer.\n",
    "            optimizer.step()\n",
    "\n",
    "            prog.update(1)\n",
    "            loss_meter.update(loss.item())\n",
    "\n",
    "    print(\"Average Train Loss: {}\".format(loss_meter.avg))\n",
    "    print(\"Evaluating on dev set\",)\n",
    "    parser.model.eval()  # Places model in \"eval\" mode, i.e. don't apply dropout layer\n",
    "        \n",
    "    dev_UAS, _ = parser.parse(dev_data)\n",
    "    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
    "    return dev_UAS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 1.0659268535673618\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7309030.62it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 53.97\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.3523068067928155\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7327994.20it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 61.78\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.27751495875418186\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7649604.31it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 66.14\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.23797372821718454\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6594072.60it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 69.30\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.2099525068576137\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6515398.44it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 71.72\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.18636098535110554\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 5953024.76it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 72.07\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.16948170773684978\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 8230116.65it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 74.27\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.15627218472460905\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7195504.34it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 74.38\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.14298919743547836\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7061355.13it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 75.30\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 10 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.1328008707302312\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6221565.84it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 75.30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create directory if it does not exist for saving the weights...\n",
    "output_dir = \"output/all_features/\"\n",
    "output_path = output_dir + \"model.weights\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "print(80 * \"=\")\n",
    "print(\"TRAINING\")\n",
    "print(80 * \"=\")\n",
    "    \n",
    "model = ParserModel(embeddings_matrix)\n",
    "parser.model = model\n",
    "\n",
    "start = time.time()\n",
    "train(parser, train_examples, dev_set, output_path,\n",
    "      batch_size=1024, n_epochs=10, lr=0.0005)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING\n",
      "================================================================================\n",
      "Restoring the best model weights found on the dev set\n",
      "Final evaluation on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 8056197.40it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- test UAS: 76.13\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(80 * \"=\")\n",
    "print(\"TESTING\")\n",
    "print(80 * \"=\")\n",
    "\n",
    "print(\"Restoring the best model weights found on the dev set\")\n",
    "parser.model.load_state_dict(torch.load(output_path))\n",
    "print(\"Final evaluation on test set\",)\n",
    "parser.model.eval()\n",
    "UAS, dependencies = parser.parse(test_set)\n",
    "print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the original parser for the ablation study\n",
    "# by adding options to include dep and pos as features\n",
    "class NewParser(object):\n",
    "\n",
    "    def __init__(self, dataset, dep_in=True, pos_in=True):\n",
    "        \n",
    "        #set the root dep\n",
    "        self.root_dep = 'root'\n",
    "                \n",
    "        #get all the dep of the dataset as list, e.g., ['root', 'acl', 'nmod', 'nmod:npmod']\n",
    "        all_dep = [self.root_dep] + list(set([w for ex in dataset\n",
    "                                               for w in ex['dep']\n",
    "                                               if w != self.root_dep]))\n",
    "        \n",
    "        #1. put dep into tok2id lookup table, with D_PREFIX so we know it is dependency\n",
    "        #{'D_PREFIX:root': 0, 'D_PREFIX:acl': 1, 'D_PREFIX:nmod': 2, ..., 'D_PREFIX:<NULL>': 30}\n",
    "        tok2id = {D_PREFIX + l: i for (i, l) in enumerate(all_dep)}\n",
    "        tok2id[D_PREFIX + NULL] = self.D_NULL = len(tok2id)\n",
    "        \n",
    "        #we are using \"unlabeled\" where we do not label with the dependency\n",
    "        #thus the number of dependency relation is 1\n",
    "        trans = ['L', 'R', 'S']\n",
    "        self.n_deprel = 1\n",
    "        \n",
    "        #create a simple lookup table mapping action and id\n",
    "        #e.g., tran2id: {'L': 0, 'R': 1, 'S': 2}\n",
    "        #e.g., id2tran: {0: 'L', 1: 'R', 2: 'S'}\n",
    "        self.n_trans = len(trans)\n",
    "        self.tran2id = {t: i for (i, t) in enumerate(trans)}\n",
    "        self.id2tran = {i: t for (i, t) in enumerate(trans)}\n",
    "\n",
    "        #2. put pos tags into tok2id lookup table, with P_PREFIX so we know it is pos\n",
    "        tok2id.update(build_dict([P_PREFIX + w for ex in dataset for w in ex['pos']],\n",
    "                                  offset=len(tok2id)))\n",
    "        tok2id[P_PREFIX + UNK]  = self.P_UNK  = len(tok2id)  #also remember the pos tags of unknown\n",
    "        tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)\n",
    "        tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)\n",
    "        \n",
    "        #now tok2id:  {'P_PREFIX:root': 0, 'P_PREFIX:acl': 1, ..., 'P_PREFIX:JJR': 62, 'P_PREFIX:<UNK>': 63, 'P_PREFIX:<NULL>': 64, 'P_PREFIX:<ROOT>': 65}\n",
    "        \n",
    "        #3. put word into tok2id lookup table\n",
    "        tok2id.update(build_dict([w for ex in dataset for w in ex['word']],\n",
    "                                  offset=len(tok2id)))\n",
    "        tok2id[UNK]  = self.UNK = len(tok2id)\n",
    "        tok2id[NULL] = self.NULL = len(tok2id)\n",
    "        tok2id[ROOT] = self.ROOT = len(tok2id)\n",
    "        \n",
    "        #now tok2id: {'D_PREFIX:root': 0, 'D_PREFIX:acl': 1, 'D_PREFIX:nmod': 2, ..., 'memory': 340, 'mr.': 341, '<UNK>': 342, '<NULL>': 343, '<ROOT>': 344}\n",
    "        \n",
    "        #create id2tok\n",
    "        self.tok2id = tok2id\n",
    "        self.id2tok = {v: k for (k, v) in tok2id.items()}\n",
    "        \n",
    "        #why 18 normal features + 18 (pos) + 12 (dep)\n",
    "        #18 features - top 3 words on buffer, top 3 words on stack, \n",
    "        # the first and second left most/rightmost children of the top two words on the stack\n",
    "        # the leftmost of leftmost/rightmost of rightmost children of the top two words on the stack\n",
    "        #18 pos - basically corresponding POS tags\n",
    "        #12 dep - corresponding ARC, excluding 6 words on hte stack/buffer..\n",
    "        self.n_features = 18\n",
    "\n",
    "        # modification for the ablation test 1\n",
    "        self.dep_in = dep_in\n",
    "\n",
    "        if self.dep_in == True:\n",
    "            self.n_features += 12\n",
    "        \n",
    "        # modification for the ablation test 2\n",
    "        self.pos_in = pos_in\n",
    "        \n",
    "        if self.pos_in == True:\n",
    "            self.n_features += 18\n",
    "\n",
    "        self.n_tokens = len(tok2id)\n",
    "    \n",
    "    #function to turn train set with words to train set with id instead using tok2id\n",
    "    def numericalize(self, examples):\n",
    "        numer_examples = []\n",
    "        for ex in examples:\n",
    "            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id\n",
    "                                  else self.UNK for w in ex['word']]\n",
    "            pos  = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id\n",
    "                                   else self.P_UNK for w in ex['pos']]\n",
    "            head = [-1] + ex['head']\n",
    "            dep  = [-1] + [self.tok2id[D_PREFIX + w] if D_PREFIX + w in self.tok2id\n",
    "                            else -1 for w in ex['dep']]\n",
    "            numer_examples.append({'word': word, 'pos': pos,\n",
    "                                 'head': head, 'dep': dep})\n",
    "        return numer_examples\n",
    "\n",
    "    #function to extract features to form a feature embedding matrix\n",
    "    def extract_features(self, stack, buf, arcs, ex):\n",
    "             \n",
    "        #ex['word']:  [55, 32, 33, 34, 35, 30], i.e., ['root', 'ms.', 'haag', 'plays', 'elianti', '.']\n",
    "        #ex['pos']:   [29, 14, 14, 16, 14, 17], i.e., ['NNP', 'NNP', 'VBZ', 'NNP', '.']\n",
    "        #ex['head']:  [-1, 2, 3, 0, 3, 3]  or ['root', 'compound', 'nsubj', 'root', 'dobj', 'punct']}\n",
    "        #ex['dep']:   [-1, 1, 2, 0, 6, 12] or ['compound', 'nsubj', 'root', 'dobj', 'punct']\n",
    "\n",
    "        #stack     :  [0]\n",
    "        #buffer    :  [1, 2, 3, 4, 5]\n",
    "        \n",
    "        if stack[0] == \"ROOT\":\n",
    "            stack[0] = 0  #start the stack with [ROOT]\n",
    "\n",
    "        #get leftmost children based on the dependency arcs\n",
    "        def get_lc(k):\n",
    "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] < k])\n",
    "\n",
    "        #get right most children based on the dependency arcs\n",
    "        def get_rc(k):\n",
    "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] > k],\n",
    "                          reverse=True)\n",
    "\n",
    "        p_features = [] #pos features (2a, 2b, 2c) - 18\n",
    "        d_features = [] #dep features (3b, 3c) - 12\n",
    "        \n",
    "        #last 3 things on the stack as features\n",
    "        #if the stack is less than 3, then we simply append NULL from the left\n",
    "        features = [self.NULL] * (3 - len(stack)) + [ex['word'][x] for x in stack[-3:]]\n",
    "        \n",
    "        # next 3 things on the buffer as features\n",
    "        #if the buffer is less than 3, simply append NULL\n",
    "        #the reason why NULL is appended on end because buffer is read left to right\n",
    "        features += [ex['word'][x] for x in buf[:3]] + [self.NULL] * (3 - len(buf))\n",
    "        \n",
    "        #corresponding pos tags\n",
    "        p_features = [self.P_NULL] * (3 - len(stack)) + [ex['pos'][x] for x in stack[-3:]]\n",
    "        p_features += [ex['pos'][x] for x in buf[:3]] + [self.P_NULL] * (3 - len(buf))\n",
    "        \n",
    "        #get the leftmost and rightmost children of the top two words, thus we loop 2 times\n",
    "        for i in range(2):\n",
    "            if i < len(stack):\n",
    "                k = stack[-i-1] #-1, -2 last two in the stack\n",
    "                \n",
    "                #the first and second lefmost/rightmost children of the top two words (i=1, 2) on the stack\n",
    "                lc = get_lc(k)  \n",
    "                rc = get_rc(k)\n",
    "                \n",
    "                #the leftmost of leftmost/rightmost of rightmost children of the top two words on the stack:\n",
    "                llc = get_lc(lc[0]) if len(lc) > 0 else []\n",
    "                rrc = get_rc(rc[0]) if len(rc) > 0 else []\n",
    "\n",
    "                #(leftmost of first word on stack, rightmost of first word, \n",
    "                # leftmost of the second word on stack, rightmost of second, \n",
    "                # leftmost of leftmost, rightmost of rightmost\n",
    "                features.append(ex['word'][lc[0]] if len(lc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][rc[0]] if len(rc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][lc[1]] if len(lc) > 1 else self.NULL)\n",
    "                features.append(ex['word'][rc[1]] if len(rc) > 1 else self.NULL)\n",
    "                features.append(ex['word'][llc[0]] if len(llc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][rrc[0]] if len(rrc) > 0 else self.NULL)\n",
    "\n",
    "                #corresponding pos\n",
    "                p_features.append(ex['pos'][lc[0]] if len(lc) > 0 else self.P_NULL)\n",
    "                p_features.append(ex['pos'][rc[0]] if len(rc) > 0 else self.P_NULL)\n",
    "                p_features.append(ex['pos'][lc[1]] if len(lc) > 1 else self.P_NULL)\n",
    "                p_features.append(ex['pos'][rc[1]] if len(rc) > 1 else self.P_NULL)\n",
    "                p_features.append(ex['pos'][llc[0]] if len(llc) > 0 else self.P_NULL)\n",
    "                p_features.append(ex['pos'][rrc[0]] if len(rrc) > 0 else self.P_NULL)\n",
    "            \n",
    "                #corresponding dep\n",
    "                d_features.append(ex['dep'][lc[0]] if len(lc) > 0 else self.D_NULL)\n",
    "                d_features.append(ex['dep'][rc[0]] if len(rc) > 0 else self.D_NULL)\n",
    "                d_features.append(ex['dep'][lc[1]] if len(lc) > 1 else self.D_NULL)\n",
    "                d_features.append(ex['dep'][rc[1]] if len(rc) > 1 else self.D_NULL)\n",
    "                d_features.append(ex['dep'][llc[0]] if len(llc) > 0 else self.D_NULL)\n",
    "                d_features.append(ex['dep'][rrc[0]] if len(rrc) > 0 else self.D_NULL)\n",
    "                \n",
    "            else:\n",
    "                #attach NULL when they don't exist\n",
    "                features += [self.NULL] * 6\n",
    "                p_features += [self.P_NULL] * 6\n",
    "                d_features += [self.D_NULL] * 6\n",
    "\n",
    "        if self.dep_in == True:     #modification for the ablation test 1\n",
    "            features += d_features\n",
    "        \n",
    "        if self.pos_in == True:     #modification for the ablation test 2\n",
    "            features += p_features\n",
    "        \n",
    "        assert len(features) == self.n_features\n",
    "        return features\n",
    "\n",
    "    #decide whether to shift, leftarc, or rightarc, based on gold parse trees\n",
    "    #this is needed to create training examples which contain samples and ground truth\n",
    "    def get_oracle(self, stack, buf, ex):\n",
    "\n",
    "        #leave if the stack is only 1, thus nothing to predict....\n",
    "        if len(stack) < 2:\n",
    "            return self.n_trans - 1\n",
    "\n",
    "        #predict based on the last two words on the stack\n",
    "        i0 = stack[-1]\n",
    "        i1 = stack[-2]\n",
    "\n",
    "        #get the head and dependency\n",
    "        h0 = ex['head'][i0]\n",
    "        h1 = ex['head'][i1]\n",
    "        d0 = ex['dep'][i0]\n",
    "        d1 = ex['dep'][i1]\n",
    "\n",
    "        #either shift, left arc or right arc\n",
    "        #\"Shift\" = 2; \"LA\" = 0; \"RA\" = 1\n",
    "        #if head of the second last word is the last word, then leftarc\n",
    "        if (i1 > 0) and (h1 == i0):\n",
    "            return 0\n",
    "        #if head of the last word is the second last word, then rightarc\n",
    "        #make sure nothing in the buffer has head with the last word on the stack\n",
    "        #otherwise, we lose the last word.....\n",
    "        elif (i1 >= 0) and (h0 == i1) and \\\n",
    "                (not any([x for x in buf if ex['head'][x] == i0])):\n",
    "            return 1\n",
    "        #otherwise shift, if something is left in buffer, otherwise, do nothing....\n",
    "        else:\n",
    "            return None if len(buf) == 0 else 2\n",
    "\n",
    "    #generate training examples\n",
    "    #from the training sentences and their gold parse trees \n",
    "    def create_instances(self, examples):\n",
    "        all_instances = []\n",
    "        \n",
    "        for i, ex in enumerate(examples):\n",
    "            #e.g., ex['word]: [344, 163, 99, 164, 165, 68]\n",
    "            n_words = len(ex['word']) - 1  #excluding the root\n",
    "\n",
    "            #arcs = {(head, tail, dependency label)}\n",
    "            stack = [0]\n",
    "            buf = [i + 1 for i in range(n_words)]  #[1, 2, 3, 4, 5]\n",
    "            arcs = []\n",
    "            instances = []\n",
    "            \n",
    "            #because that's the maximum number of shift, leftarcs, rightarcs you can have\n",
    "            #this will determine the sample size of each training example\n",
    "            #if given five words, we will get a sample of (10, 48) where 10 comes from 5 * 2, and 48 is n_features\n",
    "            #but this for loop can be break if there is nothing left....\n",
    "            for i in range(n_words * 2):\n",
    "\n",
    "                #get the gold transition based on the parse trees\n",
    "                #gold_t can be either shift(2), leftarc(0), or rightarc(1)\n",
    "                gold_t = self.get_oracle(stack, buf, ex)\n",
    "                \n",
    "                #if gold_t is None, no need to extract features.....\n",
    "                if gold_t is None:\n",
    "                    break\n",
    "                \n",
    "                #make sure when the model predicts, we inform the current state of stack and buffer, so\n",
    "                #the model is not allowed to make any illegal action, e.g., buffer is empty but trying to pop\n",
    "                legal_labels = self.legal_labels(stack, buf)                \n",
    "                assert legal_labels[gold_t] == 1\n",
    "               \n",
    "                #extract all the 48 features \n",
    "                features = self.extract_features(stack, buf, arcs, ex)\n",
    "                instances.append((features, legal_labels, gold_t))\n",
    "            \n",
    "                #shift \n",
    "                if gold_t == 2:\n",
    "                    stack.append(buf[0])\n",
    "                    buf = buf[1:]\n",
    "                #left arc \n",
    "                elif gold_t == 0:\n",
    "                    arcs.append((stack[-1], stack[-2], gold_t))\n",
    "                    stack = stack[:-2] + [stack[-1]]\n",
    "                #right arc\n",
    "                else:\n",
    "                    arcs.append((stack[-2], stack[-1], gold_t - self.n_deprel))\n",
    "                    stack = stack[:-1]\n",
    "            else:\n",
    "                all_instances += instances\n",
    "\n",
    "        return all_instances\n",
    "\n",
    "    #provide an one hot encoding of the labels\n",
    "    def legal_labels(self, stack, buf):\n",
    "        labels =  ([1] if len(stack) > 2  else [0]) * self.n_deprel   #left arc   But cannot ROOT <----He thus 3\n",
    "        labels += ([1] if len(stack) >= 2 else [0]) * self.n_deprel   #right arc  ROOT--->He\n",
    "        labels += [1] if len(buf) > 0 else [0]   #shift\n",
    "        return labels\n",
    "    \n",
    "    #a simple function to check punctuation POS tags\n",
    "    def punct(self, pos):\n",
    "        return pos in [\"''\", \",\", \".\", \":\", \"``\", \"-LRB-\", \"-RRB-\"]\n",
    "\n",
    "    def parse(self, dataset, eval_batch_size=5000):\n",
    "        sentences = []\n",
    "        sentence_id_to_idx = {}\n",
    "                \n",
    "        for i, example in enumerate(dataset):\n",
    "            \n",
    "            #example['word']=[188, 186, 186, ..., 59]\n",
    "            #n_words=37\n",
    "            #sentence=[1, 2, 3, 4, 5,.., 37]\n",
    "                        \n",
    "            n_words = len(example['word']) - 1\n",
    "            sentence = [j + 1 for j in range(n_words)]            \n",
    "            sentences.append(sentence)\n",
    "            \n",
    "            #mapping the object unique id to the i            \n",
    "            #The id is the object's memory address\n",
    "            sentence_id_to_idx[id(sentence)] = i\n",
    "            \n",
    "        model = ModelWrapper(self, dataset, sentence_id_to_idx)\n",
    "        dependencies = minibatch_parse(sentences, model, eval_batch_size)\n",
    "                \n",
    "        UAS = all_tokens = 0.0\n",
    "        with tqdm(total=len(dataset)) as prog:\n",
    "            for i, ex in enumerate(dataset):\n",
    "                head = [-1] * len(ex['word'])\n",
    "                for h, t, in dependencies[i]:\n",
    "                    head[t] = h\n",
    "                for pred_h, gold_h, gold_l, pos in \\\n",
    "                        zip(head[1:], ex['head'][1:], ex['dep'][1:], ex['pos'][1:]):\n",
    "                        assert self.id2tok[pos].startswith(P_PREFIX)\n",
    "                        pos_str = self.id2tok[pos][len(P_PREFIX):]\n",
    "                        if (not self.punct(pos_str)):\n",
    "                            UAS += 1 if pred_h == gold_h else 0\n",
    "                            all_tokens += 1\n",
    "                prog.update(i + 1)\n",
    "        UAS /= all_tokens\n",
    "        return UAS, dependencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.1. Test 1 (Without Dep Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading data\n",
      "2. Building parser...\n",
      "took 0.03 seconds\n",
      "3. Numericalizing data...\n",
      "took 0.06 seconds\n",
      "4. Loading pretrained embeddings...\n",
      "Embedding matrix shape (vocab, emb size):  (5157, 50)\n",
      "took 2.47 seconds\n",
      "5. Preprocessing training data...\n",
      "took 1.70 seconds\n",
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:04<00:00, 10.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.8118986872335275\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7587731.29it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 52.27\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:04<00:00, 11.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.3421574408809344\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7128910.94it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 60.54\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:04<00:00, 10.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.265701597246031\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6610252.24it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 64.81\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:04<00:00, 10.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.2282435530796647\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6105654.00it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 67.51\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:04<00:00, 10.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.19723631255328655\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7481402.13it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 69.27\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.1745234007636706\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6304972.05it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 70.73\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  9.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.1584206932845215\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6321131.25it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 72.06\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  9.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.14116762625053525\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6588201.21it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 72.31\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:04<00:00, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.12918514323731264\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6124017.30it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 72.16\n",
      "\n",
      "Epoch 10 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:04<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.11910351381326716\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6157899.64it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 73.63\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "================================================================================\n",
      "TESTING\n",
      "================================================================================\n",
      "Restoring the best model weights found on the dev set\n",
      "Final evaluation on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6545923.89it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- test UAS: 75.34\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_set, dev_set, test_set = load_data()\n",
    "\n",
    "# Build the parser\n",
    "print(\"2. Building parser...\",)\n",
    "start = time.time()\n",
    "parser = NewParser(train_set, dep_in=False)                    # ***WITHOUT DEP!!!***\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "# Numericalize the data\n",
    "print(\"3. Numericalizing data...\",)\n",
    "start = time.time()\n",
    "train_set = parser.numericalize(train_set)\n",
    "dev_set   = parser.numericalize(dev_set)\n",
    "test_set  = parser.numericalize(test_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "# Load pretrained word embeddings\n",
    "print(\"4. Loading pretrained embeddings...\",)\n",
    "start = time.time()\n",
    "word_vectors = {}\n",
    "for line in open(\"data/en-cw.txt\").readlines():\n",
    "    we = line.strip().split() #we = word embeddings - first column: word;  the rest is embedding\n",
    "    word_vectors[we[0]] = [float(x) for x in we[1:]] #{word: [list of 50 numbers], nextword: [another list], so on...}\n",
    "    \n",
    "#create an empty embedding matrix holding the embedding lookup table (vocab size, embed dim)\n",
    "#we use random.normal instead of zeros, to keep the embedding matrix arbitrary in case word vectors don't exist....\n",
    "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
    "\n",
    "for token in parser.tok2id:\n",
    "        i = parser.tok2id[token]\n",
    "        if token in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token]\n",
    "        elif token.lower() in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "print(\"Embedding matrix shape (vocab, emb size): \", embeddings_matrix.shape)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "# Do preprocessing of the tranining data\n",
    "print(\"5. Preprocessing training data...\",)\n",
    "start = time.time()\n",
    "train_examples = parser.create_instances(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "# Training\n",
    "output_dir = \"output/without_dep/\"\n",
    "output_path = output_dir + \"model.weights\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(80 * \"=\")\n",
    "print(\"TRAINING\")\n",
    "print(80 * \"=\")\n",
    "    \n",
    "model = ParserModel(embeddings_matrix, n_features=36)       # ***WITHOUT DEP!!!***\n",
    "parser.model = model\n",
    "\n",
    "start = time.time()\n",
    "train(parser, train_examples, dev_set, output_path,\n",
    "      batch_size=1024, n_epochs=10, lr=0.0005)\n",
    "\n",
    "# Testing\n",
    "print(80 * \"=\")\n",
    "print(\"TESTING\")\n",
    "print(80 * \"=\")\n",
    "\n",
    "print(\"Restoring the best model weights found on the dev set\")\n",
    "parser.model.load_state_dict(torch.load(output_path))\n",
    "print(\"Final evaluation on test set\",)\n",
    "parser.model.eval()\n",
    "UAS, dependencies = parser.parse(test_set)\n",
    "print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.2. Test 2 (Without POS Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading data\n",
      "2. Building parser...\n",
      "took 0.03 seconds\n",
      "3. Numericalizing data...\n",
      "took 0.05 seconds\n",
      "4. Loading pretrained embeddings...\n",
      "Embedding matrix shape (vocab, emb size):  (5157, 50)\n",
      "took 2.31 seconds\n",
      "5. Preprocessing training data...\n",
      "took 1.39 seconds\n",
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:03<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.852983079229792\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6235967.10it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 47.78\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:03<00:00, 12.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.3903315644711256\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7361057.30it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 54.54\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:04<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.31724644017716247\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6252443.75it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 57.49\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:04<00:00, 11.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.26963266264647245\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6657161.38it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 59.67\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:03<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.23663212669392428\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 5799185.06it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 61.56\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:04<00:00, 11.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.21107783789436022\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7151230.94it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 64.21\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:03<00:00, 12.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.19108616715917984\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6428651.90it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 63.82\n",
      "\n",
      "Epoch 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:04<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.17482889629900455\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6785716.19it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 65.66\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:04<00:00, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.15816679767643413\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7760689.24it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 65.61\n",
      "\n",
      "Epoch 10 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:03<00:00, 12.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.1458960835201045\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 5213379.14it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 65.53\n",
      "\n",
      "================================================================================\n",
      "TESTING\n",
      "================================================================================\n",
      "Restoring the best model weights found on the dev set\n",
      "Final evaluation on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7813439.07it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- test UAS: 67.07\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_set, dev_set, test_set = load_data()\n",
    "\n",
    "# Build the parser\n",
    "print(\"2. Building parser...\",)\n",
    "start = time.time()\n",
    "parser = NewParser(train_set, pos_in=False)                    # ***WITHOUT POS!!!***\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "# Numericalize the data\n",
    "print(\"3. Numericalizing data...\",)\n",
    "start = time.time()\n",
    "train_set = parser.numericalize(train_set)\n",
    "dev_set   = parser.numericalize(dev_set)\n",
    "test_set  = parser.numericalize(test_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "# Load pretrained word embeddings\n",
    "print(\"4. Loading pretrained embeddings...\",)\n",
    "start = time.time()\n",
    "word_vectors = {}\n",
    "for line in open(\"data/en-cw.txt\").readlines():\n",
    "    we = line.strip().split() #we = word embeddings - first column: word;  the rest is embedding\n",
    "    word_vectors[we[0]] = [float(x) for x in we[1:]] #{word: [list of 50 numbers], nextword: [another list], so on...}\n",
    "    \n",
    "#create an empty embedding matrix holding the embedding lookup table (vocab size, embed dim)\n",
    "#we use random.normal instead of zeros, to keep the embedding matrix arbitrary in case word vectors don't exist....\n",
    "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
    "\n",
    "for token in parser.tok2id:\n",
    "        i = parser.tok2id[token]\n",
    "        if token in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token]\n",
    "        elif token.lower() in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "print(\"Embedding matrix shape (vocab, emb size): \", embeddings_matrix.shape)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "# Do preprocessing of the tranining data\n",
    "print(\"5. Preprocessing training data...\",)\n",
    "start = time.time()\n",
    "train_examples = parser.create_instances(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "# Training\n",
    "output_dir = \"output/without_pos/\"\n",
    "output_path = output_dir + \"model.weights\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(80 * \"=\")\n",
    "print(\"TRAINING\")\n",
    "print(80 * \"=\")\n",
    "    \n",
    "model = ParserModel(embeddings_matrix, n_features=30)       # ***WITHOUT POS!!!***\n",
    "parser.model = model\n",
    "\n",
    "start = time.time()\n",
    "train(parser, train_examples, dev_set, output_path,\n",
    "      batch_size=1024, n_epochs=10, lr=0.0005)\n",
    "\n",
    "# Testing\n",
    "print(80 * \"=\")\n",
    "print(\"TESTING\")\n",
    "print(80 * \"=\")\n",
    "\n",
    "print(\"Restoring the best model weights found on the dev set\")\n",
    "parser.model.load_state_dict(torch.load(output_path))\n",
    "print(\"Final evaluation on test set\",)\n",
    "parser.model.eval()\n",
    "UAS, dependencies = parser.parse(test_set)\n",
    "print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Embedding Comparison"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.1. Pretrained GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading data\n",
      "2. Building parser...\n",
      "took 0.03 seconds\n",
      "3. Numericalizing data...\n",
      "took 0.05 seconds\n",
      "4. Loading GloVe embeddings...\n",
      "Embedding matrix shape (vocab, emb size):  (5157, 50)\n",
      "took 24.12 seconds\n",
      "5. Preprocessing training data...\n",
      "took 1.45 seconds\n",
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.8925228640437126\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7733840.39it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 51.26\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.3586074939618508\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6508940.35it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 59.85\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.288917042935888\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6480836.12it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 64.36\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.2500016773119569\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7656628.23it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 65.29\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.2203830253953735\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7475014.95it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 68.52\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.19789679317424694\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7887936.58it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 69.60\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.17898793332278728\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 10011559.77it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 70.77\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.16705883666872978\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7066484.30it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 72.19\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.1503782531556984\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7175454.85it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 71.87\n",
      "\n",
      "Epoch 10 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.13739651177699366\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6126874.22it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 72.80\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "================================================================================\n",
      "TESTING\n",
      "================================================================================\n",
      "Restoring the best model weights found on the dev set\n",
      "Final evaluation on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6724566.40it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- test UAS: 72.81\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_set, dev_set, test_set = load_data()\n",
    "\n",
    "# Build the parser\n",
    "print(\"2. Building parser...\",)\n",
    "start = time.time()\n",
    "parser = Parser(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "# Numericalize the data\n",
    "print(\"3. Numericalizing data...\",)\n",
    "start = time.time()\n",
    "train_set = parser.numericalize(train_set)\n",
    "dev_set   = parser.numericalize(dev_set)\n",
    "test_set  = parser.numericalize(test_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "# Load GloVe embeddings with Gensim\n",
    "print(\"4. Loading GloVe embeddings...\",)\n",
    "start = time.time()\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "#you have to put this file in some python/gensim directory; just run it and it will inform where to put....\n",
    "glove_file = datapath('glove.6B.50d.txt')\n",
    "glove_model = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)\n",
    "\n",
    "#create an empty embedding matrix holding the embedding lookup table (vocab size, embed dim)\n",
    "#we use random.normal instead of zeros, to keep the embedding matrix arbitrary in case word vectors don't exist....\n",
    "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
    "\n",
    "for token in parser.tok2id:\n",
    "        i = parser.tok2id[token]\n",
    "        if token in glove_model.index_to_key:\n",
    "            embeddings_matrix[i] = glove_model[token]\n",
    "        elif token.lower() in glove_model.index_to_key:\n",
    "            embeddings_matrix[i] = glove_model[token.lower()]\n",
    "print(\"Embedding matrix shape (vocab, emb size): \", embeddings_matrix.shape)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "# Do preprocessing of the tranining data\n",
    "print(\"5. Preprocessing training data...\",)\n",
    "start = time.time()\n",
    "train_examples = parser.create_instances(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "# Training\n",
    "output_dir = \"output/glove/\"\n",
    "output_path = output_dir + \"model.weights\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(80 * \"=\")\n",
    "print(\"TRAINING\")\n",
    "print(80 * \"=\")\n",
    "    \n",
    "model = ParserModel(embeddings_matrix)\n",
    "parser.model = model\n",
    "\n",
    "start = time.time()\n",
    "train(parser, train_examples, dev_set, output_path,\n",
    "      batch_size=1024, n_epochs=10, lr=0.0005)\n",
    "\n",
    "# Testing\n",
    "print(80 * \"=\")\n",
    "print(\"TESTING\")\n",
    "print(80 * \"=\")\n",
    "\n",
    "print(\"Restoring the best model weights found on the dev set\")\n",
    "parser.model.load_state_dict(torch.load(output_path))\n",
    "print(\"Final evaluation on test set\",)\n",
    "parser.model.eval()\n",
    "UAS, dependencies = parser.parse(test_set)\n",
    "print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2. Skip-gram from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Load the Brown Corpus from NLTK\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "corpus = nltk.corpus.brown.sents()\n",
    "\n",
    "print(corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', \"atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['the', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'city', 'executive', 'committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'city', 'of', 'atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ['the', 'september-october', 'term', 'jury', 'had', 'been', 'charged', 'by', 'fulton', 'superior', 'court', 'judge', 'durwood', 'pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'mayor-nominate', 'ivan', 'allen', 'jr.', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Convert the words in the corpus into lower case\n",
    "corpus = [[word.lower() for word in sent] for sent in corpus]\n",
    "\n",
    "print(corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investigation', \"atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'evidence', \"''\", 'irregularities', 'took', 'place', '.'], ['jury', 'said', 'term-end', 'presentments', 'city', 'executive', 'committee', ',', 'over-all', 'charge', 'election', ',', '``', 'deserves', 'praise', 'thanks', 'city', 'atlanta', \"''\", 'manner', 'election', 'conducted', '.'], ['september-october', 'term', 'jury', 'charged', 'fulton', 'superior', 'court', 'judge', 'durwood', 'pye', 'investigate', 'reports', 'possible', '``', 'irregularities', \"''\", 'hard-fought', 'primary', 'won', 'mayor-nominate', 'ivan', 'allen', 'jr.', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words with SpaCy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "for sentence in corpus:\n",
    "    for word in sentence[:]:\n",
    "        if word in stopwords:\n",
    "            sentence.remove(word)\n",
    "\n",
    "print(corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investigation', \"atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'evidence', \"''\", 'irregularities', 'took', 'place'], ['jury', 'said', 'term-end', 'presentments', 'city', 'executive', 'committee', 'over-all', 'charge', 'election', '``', 'deserves', 'praise', 'thanks', 'city', 'atlanta', \"''\", 'manner', 'election', 'conducted'], ['september-october', 'term', 'jury', 'charged', 'fulton', 'superior', 'court', 'judge', 'durwood', 'pye', 'investigate', 'reports', 'possible', '``', 'irregularities', \"''\", 'hard-fought', 'primary', 'won', 'mayor-nominate', 'ivan', 'allen', 'jr.']]\n"
     ]
    }
   ],
   "source": [
    "# Remove punctutations with String\n",
    "import string\n",
    "punctutations = string.punctuation\n",
    "\n",
    "for sentence in corpus:\n",
    "    for word in sentence[:]:\n",
    "        if word in punctutations:\n",
    "            sentence.remove(word)\n",
    "\n",
    "print(corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investigation', \"atlanta's\", 'recent', 'primary', 'election', 'produced', 'evidence', 'irregularities', 'took', 'place'], ['jury', 'said', 'term-end', 'presentments', 'city', 'executive', 'committee', 'over-all', 'charge', 'election', 'deserves', 'praise', 'thanks', 'city', 'atlanta', 'manner', 'election', 'conducted'], ['september-october', 'term', 'jury', 'charged', 'fulton', 'superior', 'court', 'judge', 'durwood', 'pye', 'investigate', 'reports', 'possible', 'irregularities', 'hard-fought', 'primary', 'won', 'mayor-nominate', 'ivan', 'allen', 'jr.']]\n"
     ]
    }
   ],
   "source": [
    "# Remove '``' and \"''\"\n",
    "for sentence in corpus:\n",
    "    for word in sentence[:]:\n",
    "        if word == '``':\n",
    "            sentence.remove(word)\n",
    "        elif word == \"''\":\n",
    "            sentence.remove(word)\n",
    "\n",
    "print(corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique words in the corpus\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabs = list(set(flatten(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word2index dictionary\n",
    "vocabs.append('<UNK>')\n",
    "word2index = {'<UNK>': 0}\n",
    "\n",
    "for v in vocabs:\n",
    "    if word2index.get(v) is None:\n",
    "        word2index[v] = len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to generate random batch of skipgrams\n",
    "def random_batch(corpus, window_size, batch_size): \n",
    "    skip_grams = []\n",
    "    for sentence in corpus:\n",
    "        #I include the first and last words\n",
    "        #so that every word could be a center word\n",
    "        for i in range(len(sentence)):\n",
    "            center = word2index[sentence[i]]\n",
    "            context = []\n",
    "            for j in range(window_size):\n",
    "                if (i - window_size + j) >= 0:\n",
    "                    context.append(word2index[sentence[i - window_size + j]])\n",
    "            for k in range(1, window_size + 1):\n",
    "                if (i + k) < len(sentence):\n",
    "                    context.append(word2index[sentence[i + k]])\n",
    "            for w in context:\n",
    "                skip_grams.append([center, w])\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False)\n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams[i][0]])\n",
    "        random_labels.append([skip_grams[i][1]])\n",
    "            \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [[31023]\n",
      " [22592]\n",
      " [37854]\n",
      " [ 2271]]\n",
      "Target:  [[40713]\n",
      " [ 4005]\n",
      " [28021]\n",
      " [43496]]\n"
     ]
    }
   ],
   "source": [
    "# Test the random batch function\n",
    "input_batch, target_batch = random_batch(corpus, 2, 4)\n",
    "\n",
    "print(\"Input: \", input_batch)\n",
    "print(\"Target: \", target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Skip-gram model\n",
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(Skipgram,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, emb_size)\n",
    "        self.embedding_u = nn.Embedding(vocab_size, emb_size)\n",
    "    \n",
    "    def forward(self, center_words, context_words, all_vocabs):\n",
    "        center_embeds = self.embedding_v(center_words)  #[batch_size, 1, emb_size]\n",
    "        context_embeds = self.embedding_u(context_words)  #[batch_size, 1, emb_size]\n",
    "        all_embeds    = self.embedding_u(all_vocabs)    #[batch_size, vocab_size, emb_size]\n",
    "        \n",
    "        scores      = context_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "\n",
    "        norm_scores = all_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, vocab_size, emb_size] @ [batch_size, emb_size, 1] = [batch_size, vocab_size, 1] = [batch_size, vocab_size]\n",
    "\n",
    "        nll = -torch.mean(torch.log(torch.exp(scores)/torch.sum(torch.exp(norm_scores), 1).unsqueeze(1))) # log-softmax\n",
    "        # scalar (loss must be scalar)    \n",
    "            \n",
    "        return nll # negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to convert indices to tensors\n",
    "def prepare_sequence(vocabs, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], vocabs))\n",
    "    return torch.LongTensor(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to calculate training epoch time\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time // 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "window_size = 2\n",
    "batch_size = 10\n",
    "vocab_size = len(vocabs)\n",
    "emb_size = 50\n",
    "model = Skipgram(vocab_size, emb_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the indices of all words to tensors\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | cost: 34.287952 | time: 7m 39s\n",
      "Epoch: 200 | cost: 27.399923 | time: 15m 15s\n",
      "Epoch: 300 | cost: 32.196110 | time: 23m 2s\n",
      "Epoch: 400 | cost: 29.706690 | time: 30m 40s\n",
      "Epoch: 500 | cost: 28.325581 | time: 38m 16s\n"
     ]
    }
   ],
   "source": [
    "# Train the Skip-gram model for embedding\n",
    "import time\n",
    "\n",
    "num_epochs = 500\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    input_batch, target_batch = random_batch(corpus, window_size, batch_size)\n",
    "    input_batch  = torch.LongTensor(input_batch)\n",
    "    target_batch = torch.LongTensor(target_batch)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(input_batch, target_batch, all_vocabs)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        end = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to get embeddings of words\n",
    "def get_embed(word):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except:\n",
    "        index = word2index['<UNK>']\n",
    "    \n",
    "    word = torch.LongTensor([index])\n",
    "\n",
    "    center_embed  = model.embedding_v(word)\n",
    "    context_embed = model.embedding_u(word)\n",
    "    \n",
    "    embed = (center_embed + context_embed) / 2\n",
    "    \n",
    "    return  embed[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading data\n",
      "2. Building parser...\n",
      "took 0.05 seconds\n",
      "3. Numericalizing data...\n",
      "took 0.06 seconds\n",
      "4. Loading Skip-gram embeddings...\n",
      "Embedding matrix shape (vocab, emb size):  (5157, 50)\n",
      "took 18.63 seconds\n",
      "5. Preprocessing training data...\n",
      "took 1.61 seconds\n",
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.9576916539420685\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 8629900.71it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 48.82\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.37226903935273487\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 8017590.40it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 59.15\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.30115020585556823\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 3960350.82it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 62.45\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.25547634344547987\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 8026042.35it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 65.13\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.22628249631573757\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 8019303.85it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 67.57\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.20265763780723015\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6609836.38it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 68.77\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.18359983650346598\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 13341542.46it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 68.55\n",
      "\n",
      "Epoch 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.16783282533288002\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 4041299.28it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 70.62\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.1548461395626267\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7427876.65it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 71.77\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 10 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.1405178957308332\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 8015877.69it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 72.55\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "================================================================================\n",
      "TESTING\n",
      "================================================================================\n",
      "Restoring the best model weights found on the dev set\n",
      "Final evaluation on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7773320.94it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- test UAS: 73.81\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_set, dev_set, test_set = load_data()\n",
    "\n",
    "# Build the parser\n",
    "print(\"2. Building parser...\",)\n",
    "start = time.time()\n",
    "parser = Parser(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "# Numericalize the data\n",
    "print(\"3. Numericalizing data...\",)\n",
    "start = time.time()\n",
    "train_set = parser.numericalize(train_set)\n",
    "dev_set   = parser.numericalize(dev_set)\n",
    "test_set  = parser.numericalize(test_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "# Load Skip-gram embeddings\n",
    "print(\"4. Loading Skip-gram embeddings...\",)\n",
    "start = time.time() \n",
    "\n",
    "#create an empty embedding matrix holding the embedding lookup table (vocab size, embed dim)\n",
    "#we use random.normal instead of zeros, to keep the embedding matrix arbitrary in case word vectors don't exist....\n",
    "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
    "\n",
    "for token in parser.tok2id:\n",
    "        i = parser.tok2id[token]\n",
    "        if token in vocabs:\n",
    "            embeddings_matrix[i] = get_embed(token)\n",
    "        elif token.lower() in vocabs:\n",
    "            embeddings_matrix[i] = get_embed(token.lower())\n",
    "print(\"Embedding matrix shape (vocab, emb size): \", embeddings_matrix.shape)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "# Do preprocessing of the tranining data\n",
    "print(\"5. Preprocessing training data...\",)\n",
    "start = time.time()\n",
    "train_examples = parser.create_instances(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "# Training\n",
    "output_dir = \"output/skipgram/\"\n",
    "output_path = output_dir + \"model.weights\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(80 * \"=\")\n",
    "print(\"TRAINING\")\n",
    "print(80 * \"=\")\n",
    "    \n",
    "model = ParserModel(embeddings_matrix)\n",
    "parser.model = model\n",
    "\n",
    "start = time.time()\n",
    "train(parser, train_examples, dev_set, output_path,\n",
    "      batch_size=1024, n_epochs=10, lr=0.0005)\n",
    "\n",
    "# Testing\n",
    "print(80 * \"=\")\n",
    "print(\"TESTING\")\n",
    "print(80 * \"=\")\n",
    "\n",
    "print(\"Restoring the best model weights found on the dev set\")\n",
    "parser.model.load_state_dict(torch.load(output_path))\n",
    "print(\"Final evaluation on test set\",)\n",
    "parser.model.eval()\n",
    "UAS, dependencies = parser.parse(test_set)\n",
    "print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Dependency Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.1. SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading data\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_set, dev_set, test_set = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': ['the', 'market', 'crumbled', '.'], 'pos': ['DT', 'NN', 'VBD', '.'], 'head': [2, 3, 0, 3], 'dep': ['det', 'nsubj', 'root', 'punct']}, {'word': ['these', 'stocks', 'eventually', 'reopened', '.'], 'pos': ['DT', 'NNS', 'RB', 'VBD', '.'], 'head': [2, 4, 4, 0, 4], 'dep': ['det', 'nsubj', 'advmod', 'root', 'punct']}, {'word': ['but', 'stocks', 'kept', 'falling', '.'], 'pos': ['CC', 'NNS', 'VBD', 'VBG', '.'], 'head': [3, 3, 0, 3, 3], 'dep': ['cc', 'nsubj', 'root', 'xcomp', 'punct']}]\n"
     ]
    }
   ],
   "source": [
    "# Choose 3 short and simple sentences from the test set for testing\n",
    "# Choose sentences with less than 10 words and without '``'\n",
    "word_limit = 5\n",
    "test_sentences = []\n",
    "\n",
    "for sent in test_set:\n",
    "    if len(sent['word']) <= word_limit and '``' not in sent['word']:\n",
    "        test_sentences.append(sent)\n",
    "    \n",
    "    if len(test_sentences) == 3:\n",
    "        break\n",
    "\n",
    "print(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the market crumbled .', 'these stocks eventually reopened .', 'but stocks kept falling .']\n"
     ]
    }
   ],
   "source": [
    "# Format the test sentences for SpaCy\n",
    "formatted_sentences = [' '.join(sent['word']) for sent in test_sentences]\n",
    "\n",
    "print(formatted_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"cf7939dd09e642dabf51f69009a44e96-0\" class=\"displacy\" width=\"750\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">market</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">crumbled</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PUNCT</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cf7939dd09e642dabf51f69009a44e96-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cf7939dd09e642dabf51f69009a44e96-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cf7939dd09e642dabf51f69009a44e96-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cf7939dd09e642dabf51f69009a44e96-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,91.5 L237,79.5 253,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-cf7939dd09e642dabf51f69009a44e96-0-2\" stroke-width=\"2px\" d=\"M420,89.5 C420,2.0 575.0,2.0 575.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-cf7939dd09e642dabf51f69009a44e96-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,91.5 L583.0,79.5 567.0,79.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"e8bea018ffdd4522ab2f492613946349-0\" class=\"displacy\" width=\"925\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">these</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">stocks</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">eventually</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">reopened</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PUNCT</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e8bea018ffdd4522ab2f492613946349-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e8bea018ffdd4522ab2f492613946349-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e8bea018ffdd4522ab2f492613946349-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e8bea018ffdd4522ab2f492613946349-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e8bea018ffdd4522ab2f492613946349-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e8bea018ffdd4522ab2f492613946349-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-e8bea018ffdd4522ab2f492613946349-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-e8bea018ffdd4522ab2f492613946349-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"c94cc03d7aa441c7a9b7845ccdbd6ffb-0\" class=\"displacy\" width=\"925\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">but</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">stocks</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">kept</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">falling</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PUNCT</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c94cc03d7aa441c7a9b7845ccdbd6ffb-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,2.0 400.0,2.0 400.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c94cc03d7aa441c7a9b7845ccdbd6ffb-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c94cc03d7aa441c7a9b7845ccdbd6ffb-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c94cc03d7aa441c7a9b7845ccdbd6ffb-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c94cc03d7aa441c7a9b7845ccdbd6ffb-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c94cc03d7aa441c7a9b7845ccdbd6ffb-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c94cc03d7aa441c7a9b7845ccdbd6ffb-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c94cc03d7aa441c7a9b7845ccdbd6ffb-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">punct</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use SpaCy to visualize dependency trees of the chosen sentences\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "options = {\"collapse_punct\": False}\n",
    "\n",
    "for sent in formatted_sentences:\n",
    "    displacy.render(nlp(sent), options = options, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.2. Chaky's Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5156, 85, 174, 5154, 87], [5156, 239, 668, 537, 5154, 87], [5156, 124, 668, 1905, 5154, 87]]\n"
     ]
    }
   ],
   "source": [
    "# Numericalize the test sentences for Chaky's Model\n",
    "parser = Parser(train_set)\n",
    "\n",
    "numericalized_sentences = [sent['word'] for sent in parser.numericalize(test_sentences)]\n",
    "\n",
    "print(numericalized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall Chaky's Model\n",
    "word_vectors = {}\n",
    "for line in open(\"data/en-cw.txt\").readlines():\n",
    "    we = line.strip().split()\n",
    "    word_vectors[we[0]] = [float(x) for x in we[1:]]\n",
    "\n",
    "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
    "\n",
    "for token in parser.tok2id:\n",
    "        i = parser.tok2id[token]\n",
    "        if token in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token]\n",
    "        elif token.lower() in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "\n",
    "model = ParserModel(embeddings_matrix)\n",
    "parser.model = model"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "My_Virtual_Environment",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "f7ca07084f99cae884d00a2401f5a915152405bd446c47609c53897335e04337"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
