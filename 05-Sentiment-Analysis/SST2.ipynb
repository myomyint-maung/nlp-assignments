{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4N4uheCsFx6P9vQC8xN1/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myomyint-maung/nlp-assignments/blob/main/05-Sentiment-Analysis/SST2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch, torchtext, torchdata\n",
        "from torch import nn\n",
        "import time\n",
        "\n",
        "# Choose computing device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "# Set SEED for reproducibility\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpooZWOwGEf0",
        "outputId": "0c2553d8-4e63-472c-d322-7450cb9dc050"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SST2\n",
        "from torchtext.datasets import SST2\n",
        "train = SST2(split='train')\n",
        "val = SST2(split='dev')"
      ],
      "metadata": {
        "id": "vMZlCqR6GR_5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check a train sample\n",
        "for text, sentiment in train:\n",
        "    print(text, sentiment)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKQrYKDyGVYh",
        "outputId": "a9598df8-7283-4a47-b5ed-5d749c880a65"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hide new secretions from the parental units 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check a val sample\n",
        "for text, sentiment in val:\n",
        "    print(text, sentiment)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5cTwSzEIOAk",
        "outputId": "65fd4c59-e339-4a04-d2b1-49bfd604c661"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it 's a charming and often affecting journey . 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the training and validation data\n",
        "datasets    = [train, val]\n",
        "train_data  = []\n",
        "val_data    = []\n",
        "data        = [train_data, val_data]\n",
        "\n",
        "for i in range(len(data)):\n",
        "    dataset = datasets[i]\n",
        "    for text, sentiment in dataset:\n",
        "        data[i].append((text, sentiment))\n",
        "\n",
        "len(train_data), len(val_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6Nq15gGGm6r",
        "outputId": "fabe9f0f-d4b2-4ab3-e0a8-b0816a53d932"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(67349, 872)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the datasets into dataframes\n",
        "import pandas as pd\n",
        "\n",
        "train_df  = pd.DataFrame(train_data, columns=[\"Text\", \"Sentiment\"])\n",
        "val_df    = pd.DataFrame(val_data, columns=[\"Text\", \"Sentiment\"])"
      ],
      "metadata": {
        "id": "KGxRLpE6IAPu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the dataframes\n",
        "print(train_df.head())\n",
        "print(val_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FizjEihSJRZM",
        "outputId": "e9174797-bb9b-43e1-fdee-5659b58e66ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                Text  Sentiment\n",
            "0        hide new secretions from the parental units          0\n",
            "1                contains no wit , only labored gags          0\n",
            "2  that loves its characters and communicates som...          1\n",
            "3  remains utterly satisfied to remain the same t...          0\n",
            "4  on the worst revenge-of-the-nerds clich√©s the ...          0\n",
            "                                                Text  Sentiment\n",
            "0     it 's a charming and often affecting journey .          1\n",
            "1                  unflinchingly bleak and desperate          0\n",
            "2  allows us to hope that nolan is poised to emba...          1\n",
            "3  the acting , costumes , music , cinematography...          1\n",
            "4                   it 's slow -- very , very slow .          0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the class labels in the dataframes\n",
        "print(train_df.Sentiment.value_counts())\n",
        "print(val_df.Sentiment.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqca74IVJUZV",
        "outputId": "31f42afb-8975-4250-c541-df16ce5f0bc3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1    37569\n",
            "0    29780\n",
            "Name: Sentiment, dtype: int64\n",
            "1    444\n",
            "0    428\n",
            "Name: Sentiment, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: 0 means negative and 1 means positive.**"
      ],
      "metadata": {
        "id": "UYsLtyiKJqvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tokenizer\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
      ],
      "metadata": {
        "id": "vNLGioM0JY0W"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vocab out of the training set\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for text, _ in data_iter:\n",
        "        yield tokenizer(text)\n",
        "        \n",
        "vocab = build_vocab_from_iterator(yield_tokens(train),\n",
        "                                  specials=['<unk>','<pad>','<bos>','<eos>'])\n",
        "\n",
        "vocab(['<unk>', '<pad>', '<bos>', '<eos>'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrPzsGKrJ4sz",
        "outputId": "8e5dd309-11fa-445c-8efa-fca07c46eadf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set <unk> as the default index of the vocab\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "vocab['hahaha']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JxyLVtDKL-l",
        "outputId": "96d4b8bb-87dd-40fe-8713-f5dc713799c6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create idex2word dictionary\n",
        "idx2word = vocab.get_itos()\n",
        "\n",
        "idx2word[0:4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXxxoBuFKQb9",
        "outputId": "92179b29-cc96-4255-8154-37a80f0f6148"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>', '<pad>', '<bos>', '<eos>']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the vocab size\n",
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACe3yzNAKTSh",
        "outputId": "86634764-63e0-4d9c-ca45-8056e9284e28"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13891"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load FastText embeddings\n",
        "from torchtext.vocab import FastText\n",
        "\n",
        "fast_vectors = FastText(language='simple')"
      ],
      "metadata": {
        "id": "fro5G5bYKVZ1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select FastText embeddings for the vocab\n",
        "fast_embedding = fast_vectors.get_vecs_by_tokens(vocab.get_itos()).to(device)\n",
        "\n",
        "fast_embedding.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tj_vU8XDKYpT",
        "outputId": "e0dbf7c9-0677-405c-a89a-a61d28ec45fa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([13891, 300])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to collate batches\n",
        "from torch.utils.data   import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "text_pipeline  = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1\n",
        "\n",
        "pad_idx = vocab['<pad>']\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, length_list = [], [], []\n",
        "    for (_text, _label) in batch:\n",
        "        label_list.append(_label)\n",
        "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        length_list.append(processed_text.size(0))\n",
        "        \n",
        "    return torch.tensor(label_list, dtype=torch.int64), \\\n",
        "        pad_sequence(text_list, padding_value=pad_idx, batch_first=True), \\\n",
        "        torch.tensor(length_list, dtype=torch.int64)"
      ],
      "metadata": {
        "id": "Bx14DBSQKkBx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data loaders\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size = batch_size,\n",
        "                          shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "val_loader   = DataLoader(val_data, batch_size = batch_size,\n",
        "                          shuffle=True, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "ZwDvFiA3LBh8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check a sample batch from the train_loader\n",
        "for label, text, length in train_loader:\n",
        "  break\n",
        "\n",
        "label, text, length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBRahxVLLJcm",
        "outputId": "df155bc8-5e2a-46a2-f2e8-ae4945b0ca8a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
              "         0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
              "         1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1]),\n",
              " tensor([[  420,    51,     8,  ...,     1,     1,     1],\n",
              "         [ 3043,     1,     1,  ...,     1,     1,     1],\n",
              "         [   19, 12666,   672,  ...,     1,     1,     1],\n",
              "         ...,\n",
              "         [ 9460,     1,     1,  ...,     1,     1,     1],\n",
              "         [   32,    36,  4124,  ...,     1,     1,     1],\n",
              "         [   13,     6,   121,  ...,     1,     1,     1]]),\n",
              " tensor([ 8,  1, 13,  1,  1, 16,  4, 13,  5, 14, 30, 37,  2, 37, 29, 17, 15,  4,\n",
              "          8, 15, 24,  2, 12,  6, 36, 19, 25,  5, 29, 10,  4,  2, 21,  7,  5,  9,\n",
              "         19, 20,  9,  8,  3,  1, 18,  3,  4,  3,  9, 21,  9,  5,  8,  6, 15, 10,\n",
              "         13,  9, 17,  9,  1,  3,  5,  1, 10, 17]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label.shape, text.shape, length.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHoiqVE4LSJa",
        "outputId": "18251e5e-1c18-4e37-b92c-d219f19d62a0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64]), torch.Size([64, 37]), torch.Size([64]))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the lengths of the data loaders\n",
        "train_loader_length = len(list(iter(train_loader)))\n",
        "val_loader_length   = len(list(iter(val_loader)))\n",
        "\n",
        "train_loader_length, val_loader_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHbb4mqLLXzE",
        "outputId": "88a304c6-2596-4e0f-a59c-dc7a296ff8ca"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1053, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the LSTM model\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout):\n",
        "        super().__init__()\n",
        "        #put padding_idx so asking the embedding layer to ignore padding\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(emb_dim, \n",
        "                           hid_dim, \n",
        "                           num_layers=num_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True)\n",
        "        self.fc = nn.Linear(hid_dim * 2, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        #text = [batch size, seq len]\n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        #++ pack sequence ++\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), enforce_sorted=False, batch_first=True)\n",
        "        \n",
        "        #embedded = [batch size, seq len, embed dim]\n",
        "        packed_output, (hn, cn) = self.lstm(packed_embedded)  #if no h0, all zeroes\n",
        "        \n",
        "        #++ unpack in case we need to use it ++\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "        \n",
        "        #output = [batch size, seq len, hidden dim * num directions]\n",
        "        #output over padding tokens are zero tensors\n",
        "        \n",
        "        #hidden = [num layers * num directions, batch size, hid dim]\n",
        "        #cell = [num layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
        "        #hn = [batch size, hidden dim * num directions]\n",
        "        \n",
        "        return self.fc(hn)"
      ],
      "metadata": {
        "id": "rdtfqITILhTW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#explicitly initialize weights for better learning\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "        nn.init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.LSTM):\n",
        "        for name, param in m.named_parameters():\n",
        "            if 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "            elif 'weight' in name:\n",
        "                nn.init.orthogonal_(param)"
      ],
      "metadata": {
        "id": "oDUq8VC0LoD-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the parameters for the LSTM model\n",
        "input_dim  = len(vocab)\n",
        "hid_dim    = 256\n",
        "emb_dim    = 300\n",
        "output_dim = 2\n",
        "\n",
        "#for biLSTM\n",
        "num_layers = 2\n",
        "bidirectional = True\n",
        "dropout = 0.5\n",
        "\n",
        "model = LSTM(input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout).to(device)\n",
        "model.apply(initialize_weights)\n",
        "model.embedding.weight.data = fast_embedding #**<------applied the fast text embedding as the initial weights"
      ],
      "metadata": {
        "id": "o7gCRWL4Lq8D"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model's parameters\n",
        "def count_parameters(model):\n",
        "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
        "    for item in params:\n",
        "        print(f'{item:>6}')\n",
        "    print(f'______\\n{sum(params):>6}')\n",
        "    \n",
        "count_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnPgQvOSLutx",
        "outputId": "de418972-c994-46e3-a1d3-37e615c30a93"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4167300\n",
            "307200\n",
            "262144\n",
            "  1024\n",
            "  1024\n",
            "307200\n",
            "262144\n",
            "  1024\n",
            "  1024\n",
            "524288\n",
            "262144\n",
            "  1024\n",
            "  1024\n",
            "524288\n",
            "262144\n",
            "  1024\n",
            "  1024\n",
            "  1024\n",
            "     2\n",
            "______\n",
            "6888070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the hyperparameters for training\n",
        "import torch.optim as optim\n",
        "\n",
        "lr=1e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Zu6H1O3nLyzH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to calculate prediction accuracy\n",
        "def accuracy(preds, y):\n",
        "    \n",
        "    predicted = torch.max(preds.data, 1)[1]\n",
        "    batch_corr = (predicted == y).sum()\n",
        "    acc = batch_corr / len(y)\n",
        "    \n",
        "    return acc"
      ],
      "metadata": {
        "id": "tidkqQYGL2Pc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to train the model\n",
        "def train(model, loader, optimizer, criterion, loader_length):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train() #useful for batchnorm and dropout\n",
        "    \n",
        "    for i, (label, text, text_length) in enumerate(loader): \n",
        "        label = label.to(device) #(batch_size, )\n",
        "        text = text.to(device) #(batch_size, seq len)\n",
        "                \n",
        "        #predict\n",
        "        predictions = model(text, text_length).squeeze(1) #output by the fc is (batch_size, 1), thus need to remove this 1\n",
        "        \n",
        "        #calculate loss\n",
        "        loss = criterion(predictions, label)\n",
        "        acc  = accuracy(predictions, label)\n",
        "        \n",
        "        #backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "                        \n",
        "    return epoch_loss / loader_length, epoch_acc / loader_length"
      ],
      "metadata": {
        "id": "c7vFh7YsL4aj"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to evaluate the model\n",
        "def evaluate(model, loader, criterion, loader_length):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, (label, text, text_length) in enumerate(loader): \n",
        "            label = label.to(device) #(batch_size, )\n",
        "            text  = text.to(device)  #(seq len, batch_size)\n",
        "\n",
        "            predictions = model(text, text_length).squeeze(1) \n",
        "            \n",
        "            loss = criterion(predictions, label)\n",
        "            acc  = accuracy(predictions, label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / loader_length, epoch_acc / loader_length"
      ],
      "metadata": {
        "id": "1DFnzjWuL6Ja"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to calculate training time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "3RW7_56TL88y"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the folder to save models\n",
        "import os\n",
        "from os import path\n",
        "\n",
        "if path.exists('./models') == False:\n",
        "  os.mkdir('./models')"
      ],
      "metadata": {
        "id": "ecxNynEVL_H4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and save the model\n",
        "best_val_loss = float('inf')\n",
        "num_epochs      = 2\n",
        "\n",
        "save_path = f'models/{model.__class__.__name__}_SST2.pt'\n",
        "\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, train_loader_length)\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, val_loader_length)\n",
        "    \n",
        "    #for plotting\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\t Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')   \n",
        "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_acc*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Rpp7YC7MBX_",
        "outputId": "b9501463-5a93-48b6-922e-7ac94b77b3b8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 0m 17s\n",
            "\t Train Loss: 0.307 | Train Acc: 86.95%\n",
            "\t Val. Loss: 0.475 |  Val. Acc: 83.64%\n",
            "Epoch: 02 | Time: 0m 17s\n",
            "\t Train Loss: 0.153 | Train Acc: 94.27%\n",
            "\t Val. Loss: 0.392 |  Val. Acc: 85.11%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "model.load_state_dict(torch.load(save_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mxlwJ-rMHO4",
        "outputId": "fce6cdf7-6549-46ea-c248-614a672240a6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test on some random text\n",
        "test_str = \"Google is now falling nonstop.  The price is really bad now.\"\n",
        "text = torch.tensor(text_pipeline(test_str)).to(device)\n",
        "text = text.reshape(1, -1)  #because batch_size is 1\n",
        "text_length = torch.tensor([text.size(1)]).to(dtype=torch.int64)\n",
        "\n",
        "def predict(text, text_length):\n",
        "    with torch.no_grad():\n",
        "        output = model(text, text_length).squeeze(1)\n",
        "        predicted = torch.max(output.data, 1)[1]\n",
        "        return predicted\n",
        "\n",
        "predict(text, text_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPOGb9NwNmoi",
        "outputId": "173627f8-8ce4-4265-daf8-52a465a5b394"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test on some negating positive sentence\n",
        "test_str = \"He is hardly intelligent.\"\n",
        "text = torch.tensor(text_pipeline(test_str)).to(device)\n",
        "text = text.reshape(1, -1)  #because batch_size is 1\n",
        "text_length = torch.tensor([text.size(1)]).to(dtype=torch.int64)\n",
        "\n",
        "predict(text, text_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lhA-WbaNxk5",
        "outputId": "3c9c9694-580a-4f02-9233-3dae39f588e2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test on another negating positive sentence\n",
        "test_str = \"You will be wrong to think that he is smart.\"\n",
        "text = torch.tensor(text_pipeline(test_str)).to(device)\n",
        "text = text.reshape(1, -1)  #because batch_size is 1\n",
        "text_length = torch.tensor([text.size(1)]).to(dtype=torch.int64)\n",
        "\n",
        "predict(text, text_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPUbJp0DN1a0",
        "outputId": "2dea069f-34c2-4f22-ca48-5ef514810580"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test on some negating negative sentence\n",
        "test_str = \"The reviews for the movie were not bad.\"\n",
        "text = torch.tensor(text_pipeline(test_str)).to(device)\n",
        "text = text.reshape(1, -1)  #because batch_size is 1\n",
        "text_length = torch.tensor([text.size(1)]).to(dtype=torch.int64)\n",
        "\n",
        "predict(text, text_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a96AF9GVN59s",
        "outputId": "5199b72c-d35a-48d6-b818-d052c6b92535"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test on another negating negative sentence\n",
        "test_str = \"There is nothing to dislike about the party.\"\n",
        "text = torch.tensor(text_pipeline(test_str)).to(device)\n",
        "text = text.reshape(1, -1)  #because batch_size is 1\n",
        "text_length = torch.tensor([text.size(1)]).to(dtype=torch.int64)\n",
        "\n",
        "predict(text, text_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAF_dfz7N8uQ",
        "outputId": "a24537f1-96cc-46d1-e54a-44fae4603ee1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion"
      ],
      "metadata": {
        "id": "1fg-qVqOQBGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The SST2 model achieved over 94% training accuracy, which is comparable to the accuracy achieved by the SST model, and over 85% validation accuracy, which is better than that of the SST model. However, like the SST model, the SST2 model has a shortcoming in classifying negating negative sentences.**"
      ],
      "metadata": {
        "id": "FASmt0o_Ofq4"
      }
    }
  ]
}