{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myomyint-maung/nlp-assignments/blob/main/05-Sentiment-Analysis/05-Sentiment-Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg6EIYVIk5CR"
      },
      "source": [
        "## 08 Feb - Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MOFVNFmpF6N1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch, torchdata, torchtext\n",
        "from torch import nn\n",
        "import time\n",
        "\n",
        "# Choose computing device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "# Set SEED for reproducibility\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl2U86yqLgNf"
      },
      "source": [
        "### 1. ETL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hBMLNy7UFNWo"
      },
      "outputs": [],
      "source": [
        "# Load the Stanford Sentiment Treebank dataset\n",
        "import pytreebank\n",
        "sst = pytreebank.load_sst()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPIgusSUOtq6",
        "outputId": "3f6391a9-a1bd-4700-b70e-aab31eaa90df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8544, 1101, 2210)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Extract the training, validation and test sets\n",
        "train  = sst['train']\n",
        "val    = sst['dev']\n",
        "test   = sst['test']\n",
        "\n",
        "len(train), len(val), len(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KladKjbdNEuj"
      },
      "source": [
        "### 2. EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCYKmm1KcHnP",
        "outputId": "b0ba8c5c-b0b4-4c56-affd-31c6ddd3f6d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3 The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\n"
          ]
        }
      ],
      "source": [
        "# Check a data sample\n",
        "for sentiment, text in train[0].to_labeled_lines():\n",
        "    print(sentiment, text)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmrCui4ZXPcL",
        "outputId": "9a4d9831-21ae-4cd9-9d96-fa363f8b5b23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(318582, 41447, 82600)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Extract the training, validation and test data\n",
        "datasets    = [train, val, test]\n",
        "train_data  = []\n",
        "val_data    = []\n",
        "test_data   = []\n",
        "data        = [train_data, val_data, test_data]\n",
        "\n",
        "for i in range(len(data)):\n",
        "  dataset = datasets[i]\n",
        "  for example in dataset:\n",
        "    for sentiment, text in example.to_labeled_lines():\n",
        "      data[i].append((sentiment, text))\n",
        "\n",
        "len(train_data), len(val_data), len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "60WgAubFMBT7"
      },
      "outputs": [],
      "source": [
        "# Convert the datasets into dataframes\n",
        "import pandas as pd\n",
        "\n",
        "train_df  = pd.DataFrame(train_data, columns=[\"Sentiment\", \"Text\"])\n",
        "val_df    = pd.DataFrame(val_data, columns=[\"Sentiment\", \"Text\"])\n",
        "test_df   = pd.DataFrame(test_data, columns=[\"Sentiment\", \"Text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tqf28wncOwu",
        "outputId": "941f1a51-9c3d-4ee0-ab8c-94b5f3df6445"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Sentiment                                               Text\n",
            "0          3  The Rock is destined to be the 21st Century 's...\n",
            "1          2                                           The Rock\n",
            "2          2                                                The\n",
            "3          2                                               Rock\n",
            "4          4  is destined to be the 21st Century 's new `` C...\n",
            "   Sentiment                                               Text\n",
            "0          3  It 's a lovely film with lovely performances b...\n",
            "1          2                                                 It\n",
            "2          4  's a lovely film with lovely performances by B...\n",
            "3          4  's a lovely film with lovely performances by B...\n",
            "4          2                                                 's\n",
            "   Sentiment                            Text\n",
            "0          2  Effective but too-tepid biopic\n",
            "1          3                   Effective but\n",
            "2          3                       Effective\n",
            "3          2                             but\n",
            "4          1                too-tepid biopic\n"
          ]
        }
      ],
      "source": [
        "# Check the dataframes\n",
        "print(train_df.head())\n",
        "print(val_df.head())\n",
        "print(test_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnwZ8cYjN7va",
        "outputId": "a66c841e-dfbb-42cd-b88c-9dde3b5f573a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2    219788\n",
            "3     44194\n",
            "1     34362\n",
            "4     11993\n",
            "0      8245\n",
            "Name: Sentiment, dtype: int64\n",
            "2    28305\n",
            "3     5781\n",
            "1     4613\n",
            "4     1678\n",
            "0     1070\n",
            "Name: Sentiment, dtype: int64\n",
            "2    56548\n",
            "3    10998\n",
            "1     9255\n",
            "4     3791\n",
            "0     2008\n",
            "Name: Sentiment, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Check the class labels in the dataframes\n",
        "print(train_df.Sentiment.value_counts())\n",
        "print(val_df.Sentiment.value_counts())\n",
        "print(test_df.Sentiment.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU2pG4DJQ-Nj"
      },
      "source": [
        "### 3. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eyU-wyPRRMG"
      },
      "source": [
        "#### 3.1. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "iT3PUyx3Qyy5"
      },
      "outputs": [],
      "source": [
        "# Create a tokenizer\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sudE8LGmS-4S"
      },
      "source": [
        "#### 3.2. Numericalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1gV8jvYTxwh",
        "outputId": "95be100a-c031-4661-9077-9517001dcb3c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 1, 2, 3]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create vocab out of the training set\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for data in data_iter:\n",
        "        for _, text in data.to_labeled_lines():\n",
        "            yield tokenizer(text)\n",
        "        \n",
        "vocab = build_vocab_from_iterator(yield_tokens(train),\n",
        "                                  specials=['<unk>','<pad>','<bos>','<eos>'])\n",
        "\n",
        "vocab(['<unk>', '<pad>', '<bos>', '<eos>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "vcFX9uTQUQcU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set <unk> as the default index of the vocab\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "vocab['hahaha']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqwG37kEU7mq",
        "outputId": "7df1591d-baaf-4903-f76f-ee1e2e75d517"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<unk>', '<pad>', '<bos>', '<eos>']"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create idex2word dictionary\n",
        "idx2word = vocab.get_itos()\n",
        "\n",
        "idx2word[0:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KF7k_l_kXIOB",
        "outputId": "78bae0cb-6779-4bcb-c69e-2cefd94a585e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17136"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the vocab size\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7fP_vs2Ybju"
      },
      "source": [
        "### 4. FastText Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "BC_0cVtHY50i"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ".vector_cache\\wiki.simple.vec:   3%|â–Ž         | 7.41M/293M [00:09<06:03, 786kB/s]    \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [32], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Load FastText embeddings\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvocab\u001b[39;00m \u001b[39mimport\u001b[39;00m FastText\n\u001b[1;32m----> 4\u001b[0m fast_vectors \u001b[39m=\u001b[39m FastText(language\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msimple\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\Users\\MARC\\anaconda3\\envs\\my_env\\lib\\site-packages\\torchtext\\vocab\\vectors.py:230\u001b[0m, in \u001b[0;36mFastText.__init__\u001b[1;34m(self, language, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl_base\u001b[39m.\u001b[39mformat(language)\n\u001b[0;32m    229\u001b[0m name \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(url)\n\u001b[1;32m--> 230\u001b[0m \u001b[39msuper\u001b[39m(FastText, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(name, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\MARC\\anaconda3\\envs\\my_env\\lib\\site-packages\\torchtext\\vocab\\vectors.py:59\u001b[0m, in \u001b[0;36mVectors.__init__\u001b[1;34m(self, name, cache, url, unk_init, max_vectors)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munk_init \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor\u001b[39m.\u001b[39mzero_ \u001b[39mif\u001b[39;00m unk_init \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m unk_init\n\u001b[1;32m---> 59\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache(name, cache, url\u001b[39m=\u001b[39;49murl, max_vectors\u001b[39m=\u001b[39;49mmax_vectors)\n",
            "File \u001b[1;32mc:\\Users\\MARC\\anaconda3\\envs\\my_env\\lib\\site-packages\\torchtext\\vocab\\vectors.py:98\u001b[0m, in \u001b[0;36mVectors.cache\u001b[1;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# remove the partial zip file\u001b[39;00m\n\u001b[0;32m     97\u001b[0m             os\u001b[39m.\u001b[39mremove(dest)\n\u001b[1;32m---> 98\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     99\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mExtracting vectors into \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(cache))\n\u001b[0;32m    100\u001b[0m ext \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplitext(dest)[\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m:]\n",
            "File \u001b[1;32mc:\\Users\\MARC\\anaconda3\\envs\\my_env\\lib\\site-packages\\torchtext\\vocab\\vectors.py:95\u001b[0m, in \u001b[0;36mVectors.cache\u001b[1;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m, unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, miniters\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, desc\u001b[39m=\u001b[39mdest) \u001b[39mas\u001b[39;00m t:\n\u001b[0;32m     94\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m         urlretrieve(url, dest, reporthook\u001b[39m=\u001b[39;49mreporthook(t))\n\u001b[0;32m     96\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# remove the partial zip file\u001b[39;00m\n\u001b[0;32m     97\u001b[0m         os\u001b[39m.\u001b[39mremove(dest)\n",
            "File \u001b[1;32mc:\\Users\\MARC\\anaconda3\\envs\\my_env\\lib\\urllib\\request.py:270\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    267\u001b[0m     reporthook(blocknum, bs, size)\n\u001b[0;32m    269\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 270\u001b[0m     block \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mread(bs)\n\u001b[0;32m    271\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m block:\n\u001b[0;32m    272\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\MARC\\anaconda3\\envs\\my_env\\lib\\http\\client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[0;32m    463\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[1;32m--> 465\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[0;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[0;32m    467\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    468\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
            "File \u001b[1;32mc:\\Users\\MARC\\anaconda3\\envs\\my_env\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\MARC\\anaconda3\\envs\\my_env\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[1;32mc:\\Users\\MARC\\anaconda3\\envs\\my_env\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Load FastText embeddings\n",
        "from torchtext.vocab import FastText\n",
        "\n",
        "fast_vectors = FastText(language='simple')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZs--mSiZmOi",
        "outputId": "1b865883-3f96-405b-9114-9d1a1496bb49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([17136, 300])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Select FastText embeddings for the vocab\n",
        "fast_embedding = fast_vectors.get_vecs_by_tokens(vocab.get_itos()).to(device)\n",
        "\n",
        "fast_embedding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqYWWAIXcn0t"
      },
      "source": [
        "### 5. Preparing Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Q0gx3OdYd67E"
      },
      "outputs": [],
      "source": [
        "# Create a function to collate batches\n",
        "from torch.utils.data   import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "text_pipeline  = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1\n",
        "\n",
        "pad_idx = vocab['<pad>']\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, length_list = [], [], []\n",
        "    for (_label, _text) in batch:\n",
        "        label_list.append(_label)\n",
        "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        length_list.append(processed_text.size(0))\n",
        "        \n",
        "    return torch.tensor(label_list, dtype=torch.int64), \\\n",
        "        pad_sequence(text_list, padding_value=pad_idx, batch_first=True), \\\n",
        "        torch.tensor(length_list, dtype=torch.int64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "WHBhhzK0hVRl"
      },
      "outputs": [],
      "source": [
        "# Prepare dataloaders\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size = batch_size,\n",
        "                          shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "val_loader   = DataLoader(val_data, batch_size = batch_size,\n",
        "                          shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "test_loader  = DataLoader(test_data, batch_size = batch_size,\n",
        "                          shuffle=True, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHKsUqnWlQRf",
        "outputId": "838f159c-5f82-44e6-c705-028ebe2ed532"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([4, 2, 3, 2, 2, 3, 1, 4, 2, 4, 2, 0, 2, 2, 4, 3, 2, 2, 2, 2, 2, 1, 2, 2,\n",
              "         2, 2, 1, 2, 4, 2, 2, 2, 2, 4, 1, 2, 3, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1,\n",
              "         1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 2, 2, 2, 2]),\n",
              " tensor([[  163,     1,     1,  ...,     1,     1,     1],\n",
              "         [   24,     1,     1,  ...,     1,     1,     1],\n",
              "         [   16,  4723,     1,  ...,     1,     1,     1],\n",
              "         ...,\n",
              "         [  346,   797,     8,  ...,     1,     1,     1],\n",
              "         [ 1344,     1,     1,  ...,     1,     1,     1],\n",
              "         [    7, 14579,   594,  ...,     1,     1,     1]]),\n",
              " tensor([ 1,  1,  2,  3,  1,  6,  1,  1,  1,  3,  1, 12,  5, 13,  5,  1,  1,  1,\n",
              "          3,  1,  1, 11,  1,  1,  2,  1,  2,  1, 18,  2,  1,  1,  2, 18,  1,  9,\n",
              "          2,  1, 15,  2,  3,  1,  1,  2,  4,  1,  1,  9, 26,  1,  1,  1,  1,  3,\n",
              "          1,  1, 14,  1,  4,  5,  3,  4,  1,  6]))"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check a sample batch from the train_loader\n",
        "for label, text, length in train_loader:\n",
        "  break\n",
        "\n",
        "label, text, length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYf-NpizljGz",
        "outputId": "a5cf5e36-21f8-4a06-eff3-cb58cd3d54bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([64]), torch.Size([64, 26]), torch.Size([64]))"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label.shape, text.shape, length.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqdkCVLt8CDj",
        "outputId": "52b86676-6dfd-4ac8-b926-e8802bfeecd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4978, 648, 1291)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the lengths of data loaders\n",
        "train_loader_length = len(list(iter(train_loader)))\n",
        "val_loader_length   = len(list(iter(val_loader)))\n",
        "test_loader_length  = len(list(iter(test_loader)))\n",
        "\n",
        "train_loader_length, val_loader_length, test_loader_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UpDB8mJnfmN"
      },
      "source": [
        "### 6. Designing the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ukYLTp61mz7K"
      },
      "outputs": [],
      "source": [
        "# Create the LSTM model\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout):\n",
        "        super().__init__()\n",
        "        #put padding_idx so asking the embedding layer to ignore padding\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(emb_dim, \n",
        "                           hid_dim, \n",
        "                           num_layers=num_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True)\n",
        "        self.fc = nn.Linear(hid_dim * 2, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        #text = [batch size, seq len]\n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        #++ pack sequence ++\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), enforce_sorted=False, batch_first=True)\n",
        "        \n",
        "        #embedded = [batch size, seq len, embed dim]\n",
        "        packed_output, (hn, cn) = self.lstm(packed_embedded)  #if no h0, all zeroes\n",
        "        \n",
        "        #++ unpack in case we need to use it ++\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "        \n",
        "        #output = [batch size, seq len, hidden dim * num directions]\n",
        "        #output over padding tokens are zero tensors\n",
        "        \n",
        "        #hidden = [num layers * num directions, batch size, hid dim]\n",
        "        #cell = [num layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
        "        #hn = [batch size, hidden dim * num directions]\n",
        "        \n",
        "        return self.fc(hn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07qjuyyElNf3"
      },
      "source": [
        "### 7. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "KLJ_HN5MlP9R"
      },
      "outputs": [],
      "source": [
        "#explicitly initialize weights for better learning\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "        nn.init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.LSTM):\n",
        "        for name, param in m.named_parameters():\n",
        "            if 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "            elif 'weight' in name:\n",
        "                nn.init.orthogonal_(param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Kpbt11fOl66t"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'fast_embedding' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [39], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m model \u001b[39m=\u001b[39m LSTM(input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     13\u001b[0m model\u001b[39m.\u001b[39mapply(initialize_weights)\n\u001b[1;32m---> 14\u001b[0m model\u001b[39m.\u001b[39membedding\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m fast_embedding \u001b[39m#**<------applied the fast text embedding as the initial weights\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'fast_embedding' is not defined"
          ]
        }
      ],
      "source": [
        "# Set the parameters for the LSTM model\n",
        "input_dim  = len(vocab)\n",
        "hid_dim    = 256\n",
        "emb_dim    = 300\n",
        "output_dim = 5\n",
        "\n",
        "#for biLSTM\n",
        "num_layers = 2\n",
        "bidirectional = True\n",
        "dropout = 0.5\n",
        "\n",
        "model = LSTM(input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout).to(device)\n",
        "model.apply(initialize_weights)\n",
        "model.embedding.weight.data = fast_embedding #**<------applied the fast text embedding as the initial weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dtn6ak9fzysR",
        "outputId": "d8b63735-dddd-4287-8e58-8457a328f5d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5140800\n",
            "307200\n",
            "262144\n",
            "  1024\n",
            "  1024\n",
            "307200\n",
            "262144\n",
            "  1024\n",
            "  1024\n",
            "524288\n",
            "262144\n",
            "  1024\n",
            "  1024\n",
            "524288\n",
            "262144\n",
            "  1024\n",
            "  1024\n",
            "  2560\n",
            "     5\n",
            "______\n",
            "7863109\n"
          ]
        }
      ],
      "source": [
        "# Print the model's parameters\n",
        "def count_parameters(model):\n",
        "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
        "    for item in params:\n",
        "        print(f'{item:>6}')\n",
        "    print(f'______\\n{sum(params):>6}')\n",
        "    \n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "DnKReaX-m1qf"
      },
      "outputs": [],
      "source": [
        "# Set the hyperparameters for training\n",
        "import torch.optim as optim\n",
        "\n",
        "lr=1e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "zdrxQVWbnS2M"
      },
      "outputs": [],
      "source": [
        "# Create a function to calculate prediction accuracy\n",
        "def accuracy(preds, y):\n",
        "    \n",
        "    predicted = torch.max(preds.data, 1)[1]\n",
        "    batch_corr = (predicted == y).sum()\n",
        "    acc = batch_corr / len(y)\n",
        "    \n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "rTcFpM3SnyfP"
      },
      "outputs": [],
      "source": [
        "# Create a function to train the model\n",
        "def train(model, loader, optimizer, criterion, loader_length):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train() #useful for batchnorm and dropout\n",
        "    \n",
        "    for i, (label, text, text_length) in enumerate(loader): \n",
        "        label = label.to(device) #(batch_size, )\n",
        "        text = text.to(device) #(batch_size, seq len)\n",
        "                \n",
        "        #predict\n",
        "        predictions = model(text, text_length).squeeze(1) #output by the fc is (batch_size, 1), thus need to remove this 1\n",
        "        \n",
        "        #calculate loss\n",
        "        loss = criterion(predictions, label)\n",
        "        acc  = accuracy(predictions, label)\n",
        "        \n",
        "        #backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "                        \n",
        "    return epoch_loss / loader_length, epoch_acc / loader_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "MUFiJ9ahoYvZ"
      },
      "outputs": [],
      "source": [
        "# Create a function to evaluate the model\n",
        "def evaluate(model, loader, criterion, loader_length):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, (label, text, text_length) in enumerate(loader): \n",
        "            label = label.to(device) #(batch_size, )\n",
        "            text  = text.to(device)  #(seq len, batch_size)\n",
        "\n",
        "            predictions = model(text, text_length).squeeze(1) \n",
        "            \n",
        "            loss = criterion(predictions, label)\n",
        "            acc  = accuracy(predictions, label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / loader_length, epoch_acc / loader_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "vlViLxowp6TP"
      },
      "outputs": [],
      "source": [
        "# Create a function to calculate training time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "KhHAy6xFqGCb",
        "outputId": "f816c4cb-b932-46e8-8910-f6b51480aea5"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-ff41c4a50ecb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-27eb019f629e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion, loader_length)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "best_valid_loss = float('inf')\n",
        "num_epochs      = 5\n",
        "\n",
        "save_path = f'models/{model.__class__.__name__}.pt'\n",
        "\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, train_loader_length)\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, val_loader_length)\n",
        "    \n",
        "    #for plotting\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\t Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')   \n",
        "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_acc*100:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNhHmSD4bN7zoDkNVK7BCHo",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "my_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "f7ca07084f99cae884d00a2401f5a915152405bd446c47609c53897335e04337"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
