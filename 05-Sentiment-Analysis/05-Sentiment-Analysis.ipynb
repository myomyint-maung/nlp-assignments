{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPXZG9mJYUpRf6eHR/Zrrs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myomyint-maung/nlp-assignments/blob/main/05-Sentiment-Analysis/05-Sentiment-Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 08 Feb - Sentiment Analysis"
      ],
      "metadata": {
        "id": "wg6EIYVIk5CR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch, torchdata, torchtext\n",
        "from torch import nn\n",
        "import time"
      ],
      "metadata": {
        "id": "MOFVNFmpF6N1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose computing device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCENv7LmGvmM",
        "outputId": "ac3e1f20-2452-47a3-f104-e17873bfd5f3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set SEED for reproducibility\n",
        "SEED = 786\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "aVpZ0-BAHUOh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. ETL"
      ],
      "metadata": {
        "id": "Nl2U86yqLgNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Stanford Sentiment Treebank dataset\n",
        "import pytreebank\n",
        "sst = pytreebank.load_sst()"
      ],
      "metadata": {
        "id": "hBMLNy7UFNWo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the training, validation and test sets\n",
        "train  = sst['train']\n",
        "val    = sst['dev']\n",
        "test   = sst['test']\n",
        "\n",
        "len(train), len(val), len(test)"
      ],
      "metadata": {
        "id": "EPIgusSUOtq6",
        "outputId": "2c2c5465-367d-4506-eb50-c8ab3c6f02d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8544, 1101, 2210)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. EDA"
      ],
      "metadata": {
        "id": "KladKjbdNEuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check a data sample\n",
        "train[0].to_labeled_lines()"
      ],
      "metadata": {
        "id": "eCYKmm1KcHnP",
        "outputId": "ad643cd3-e793-4723-b268-193359aadcb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3,\n",
              "  \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\"),\n",
              " (2, 'The Rock'),\n",
              " (2, 'The'),\n",
              " (2, 'Rock'),\n",
              " (4,\n",
              "  \"is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\"),\n",
              " (3,\n",
              "  \"is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal\"),\n",
              " (2, 'is'),\n",
              " (4,\n",
              "  \"destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal\"),\n",
              " (2, 'destined'),\n",
              " (2,\n",
              "  \"to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal\"),\n",
              " (2, \"to be the 21st Century 's new `` Conan '' and\"),\n",
              " (2, \"to be the 21st Century 's new `` Conan ''\"),\n",
              " (2, \"to be the 21st Century 's new `` Conan\"),\n",
              " (2, 'to'),\n",
              " (2, \"be the 21st Century 's new `` Conan\"),\n",
              " (2, 'be'),\n",
              " (2, \"the 21st Century 's new `` Conan\"),\n",
              " (2, 'the'),\n",
              " (2, \"21st Century 's new `` Conan\"),\n",
              " (2, '21st'),\n",
              " (2, \"Century 's new `` Conan\"),\n",
              " (2, \"Century 's\"),\n",
              " (2, 'Century'),\n",
              " (2, \"'s\"),\n",
              " (2, 'new `` Conan'),\n",
              " (3, 'new'),\n",
              " (2, '`` Conan'),\n",
              " (2, '``'),\n",
              " (2, 'Conan'),\n",
              " (2, \"''\"),\n",
              " (2, 'and'),\n",
              " (3,\n",
              "  \"that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal\"),\n",
              " (2, 'that'),\n",
              " (3,\n",
              "  \"he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal\"),\n",
              " (2, 'he'),\n",
              " (3,\n",
              "  \"'s going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal\"),\n",
              " (2, \"'s\"),\n",
              " (3,\n",
              "  'going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal'),\n",
              " (2, 'going'),\n",
              " (3,\n",
              "  'to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal'),\n",
              " (2, 'to'),\n",
              " (4,\n",
              "  'make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal'),\n",
              " (3, 'make a splash even greater'),\n",
              " (2, 'make'),\n",
              " (3, 'a splash even greater'),\n",
              " (3, 'a splash'),\n",
              " (2, 'a'),\n",
              " (3, 'splash'),\n",
              " (2, 'even greater'),\n",
              " (2, 'even'),\n",
              " (3, 'greater'),\n",
              " (2, 'than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal'),\n",
              " (2, 'than'),\n",
              " (2, 'Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal'),\n",
              " (2, 'Arnold Schwarzenegger , Jean-Claud Van Damme or'),\n",
              " (2, 'Arnold Schwarzenegger , Jean-Claud Van Damme'),\n",
              " (2, 'Arnold Schwarzenegger ,'),\n",
              " (1, 'Arnold Schwarzenegger'),\n",
              " (2, 'Arnold'),\n",
              " (2, 'Schwarzenegger'),\n",
              " (2, ','),\n",
              " (2, 'Jean-Claud Van Damme'),\n",
              " (2, 'Jean-Claud'),\n",
              " (2, 'Van Damme'),\n",
              " (2, 'Van'),\n",
              " (2, 'Damme'),\n",
              " (2, 'or'),\n",
              " (2, 'Steven Segal'),\n",
              " (2, 'Steven'),\n",
              " (2, 'Segal'),\n",
              " (2, '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the training, validation and test data\n",
        "datasets    = [train, val, test]\n",
        "train_data  = []\n",
        "val_data    = []\n",
        "test_data   = []\n",
        "data        = [train_data, val_data, test_data]\n",
        "\n",
        "for i in range(len(data)):\n",
        "  dataset = datasets[i]\n",
        "  for example in dataset:\n",
        "    for sentiment, text in example.to_labeled_lines():\n",
        "      data[i].append((sentiment, text))\n",
        "\n",
        "len(train_data), len(val_data), len(test_data)"
      ],
      "metadata": {
        "id": "WmrCui4ZXPcL",
        "outputId": "39323aea-fa45-4685-a359-695fceab8c69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(318582, 41447, 82600)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the datasets into dataframes\n",
        "import pandas as pd\n",
        "\n",
        "train_df  = pd.DataFrame(train_data, columns=[\"Sentiment\", \"Text\"])\n",
        "val_df    = pd.DataFrame(val_data, columns=[\"Sentiment\", \"Text\"])\n",
        "test_df   = pd.DataFrame(test_data, columns=[\"Sentiment\", \"Text\"])"
      ],
      "metadata": {
        "id": "60WgAubFMBT7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the dataframes\n",
        "print(train_df.head())\n",
        "print(val_df.head())\n",
        "print(test_df.head())"
      ],
      "metadata": {
        "id": "2tqf28wncOwu",
        "outputId": "ca46b2ef-6270-4fb7-a1c8-f99113fe57d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Sentiment                                               Text\n",
            "0          3  The Rock is destined to be the 21st Century 's...\n",
            "1          2                                           The Rock\n",
            "2          2                                                The\n",
            "3          2                                               Rock\n",
            "4          4  is destined to be the 21st Century 's new `` C...\n",
            "   Sentiment                                               Text\n",
            "0          3  It 's a lovely film with lovely performances b...\n",
            "1          2                                                 It\n",
            "2          4  's a lovely film with lovely performances by B...\n",
            "3          4  's a lovely film with lovely performances by B...\n",
            "4          2                                                 's\n",
            "   Sentiment                            Text\n",
            "0          2  Effective but too-tepid biopic\n",
            "1          3                   Effective but\n",
            "2          3                       Effective\n",
            "3          2                             but\n",
            "4          1                too-tepid biopic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the class labels in the dataframes\n",
        "print(train_df.Sentiment.value_counts())\n",
        "print(val_df.Sentiment.value_counts())\n",
        "print(test_df.Sentiment.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnwZ8cYjN7va",
        "outputId": "83282f8c-cd1e-475e-abf5-13f48658d7c6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2    219788\n",
            "3     44194\n",
            "1     34362\n",
            "4     11993\n",
            "0      8245\n",
            "Name: Sentiment, dtype: int64\n",
            "2    28305\n",
            "3     5781\n",
            "1     4613\n",
            "4     1678\n",
            "0     1070\n",
            "Name: Sentiment, dtype: int64\n",
            "2    56548\n",
            "3    10998\n",
            "1     9255\n",
            "4     3791\n",
            "0     2008\n",
            "Name: Sentiment, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Preprocessing"
      ],
      "metadata": {
        "id": "gU2pG4DJQ-Nj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1. Tokenization"
      ],
      "metadata": {
        "id": "8eyU-wyPRRMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tokenizer\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
      ],
      "metadata": {
        "id": "iT3PUyx3Qyy5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to get tokens out of datasets\n",
        "def yield_tokens(data):\n",
        "  for _, text in data:\n",
        "    yield tokenizer(text)"
      ],
      "metadata": {
        "id": "75zJ6_KATiAq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2. Numericalization"
      ],
      "metadata": {
        "id": "sudE8LGmS-4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vocab out of the training set\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_data),\n",
        "                                  specials=['<unk>','<pad>','<bos>','<eos>'],\n",
        "                                  special_first=True)\n",
        "\n",
        "vocab(['<unk>', '<pad>', '<bos>', '<eos>'])"
      ],
      "metadata": {
        "id": "r1gV8jvYTxwh",
        "outputId": "706ff8f7-7e22-419e-82a0-dc122a7ce4d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set <unk> as the default index of the vocab\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ],
      "metadata": {
        "id": "vcFX9uTQUQcU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make idex2word dictionary\n",
        "index2word = vocab.get_itos()\n",
        "\n",
        "index2word[0:4]"
      ],
      "metadata": {
        "id": "VqwG37kEU7mq",
        "outputId": "2ca1e206-6c4b-478d-8b8d-27fcf9062a1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>', '<pad>', '<bos>', '<eos>']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab), len(index2word)"
      ],
      "metadata": {
        "id": "KF7k_l_kXIOB",
        "outputId": "a37d2422-edc6-4eb5-c135-44ef3800a7c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17136, 17136)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. FastText Embedding"
      ],
      "metadata": {
        "id": "k7fP_vs2Ybju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load FastText embeddings\n",
        "from torchtext.vocab import FastText\n",
        "\n",
        "fast_vectors = FastText(language='simple')"
      ],
      "metadata": {
        "id": "BC_0cVtHY50i"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select FastText embeddings for the vocab\n",
        "fast_embeddings = fast_vectors.get_vecs_by_tokens(index2word).to(device)\n",
        "\n",
        "fast_embeddings.shape"
      ],
      "metadata": {
        "id": "ZZs--mSiZmOi",
        "outputId": "81a1d0a9-38f9-4333-a722-d732af1c35a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([17136, 300])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Preparing Dataloader"
      ],
      "metadata": {
        "id": "sqYWWAIXcn0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to collate batches\n",
        "from torch.utils.data   import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "text_pipeline  = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1 #turn {1, 2, 3, 4} to {0, 1, 2, 3} for pytorch training \n",
        "\n",
        "pad_idx = vocab['<pad>'] #++<----making sure our embedding layer ignores pad\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, length_list = [], [], []\n",
        "    for (_label, _text) in batch:\n",
        "        label_list.append(label_pipeline(_label))\n",
        "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        length_list.append(processed_text.size(0))  #++<-----packed padded sequences require \n",
        "    #criterion expects float labels\n",
        "    return torch.tensor(label_list, dtype=torch.int64), pad_sequence(text_list,  padding_value=pad_idx, batch_first=True), torch.tensor(length_list, dtype=torch.int64)"
      ],
      "metadata": {
        "id": "Q0gx3OdYd67E"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataloaders\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader  = DataLoader(test_data, batch_size=batch_size,\n",
        "                             shuffle=True, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "WHBhhzK0hVRl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check a sample batch from the train_loader\n",
        "for label, text, length in train_loader:\n",
        "  break\n",
        "\n",
        "label, text, length"
      ],
      "metadata": {
        "id": "fHKsUqnWlQRf",
        "outputId": "b345e9b7-3afe-4d5f-eae7-c268d4b70924",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 1,  1,  2,  1,  1,  1,  1,  2,  1,  1,  2,  1,  1,  3,  1,  0,  1,  1,\n",
              "          1,  1,  2,  1,  1,  1, -1,  2,  1,  1,  1,  1,  1,  1,  1,  3,  1,  3,\n",
              "          0,  2,  1,  1,  1,  1,  2,  1,  1,  1,  1,  2,  1,  1,  1,  1,  0,  2,\n",
              "          1,  1,  1,  1,  1,  1,  1,  1,  1,  1]),\n",
              " tensor([[   10,     1,     1,  ...,     1,     1,     1],\n",
              "         [    4,     1,     1,  ...,     1,     1,     1],\n",
              "         [ 3349,    14,   708,  ...,     1,     1,     1],\n",
              "         ...,\n",
              "         [   14,     1,     1,  ...,     1,     1,     1],\n",
              "         [   19,   849, 12768,  ...,     1,     1,     1],\n",
              "         [  159,     1,     1,  ...,     1,     1,     1]]),\n",
              " tensor([ 1,  1,  4,  8,  1,  5,  1,  2, 17,  1,  1,  1,  1,  2,  8,  1,  2,  3,\n",
              "          1,  1,  3,  1,  1,  1, 28,  5,  1,  1,  5,  1,  3,  1, 15, 18,  1,  7,\n",
              "         17,  3,  2,  1,  1,  1,  8,  1,  1,  1,  1, 13,  1,  1,  1,  2, 14,  1,\n",
              "         23,  2,  1,  1,  1,  2,  1,  1,  3,  1]))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label.shape, text.shape, length.shape"
      ],
      "metadata": {
        "id": "UYf-NpizljGz",
        "outputId": "eb77a8c2-d0bc-45d9-fdd8-74fb384b7714",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64]), torch.Size([64, 28]), torch.Size([64]))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the lengths of data loaders\n",
        "train_loader_length = len(list(iter(train_loader)))\n",
        "val_loader_length   = len(list(iter(val_loader)))\n",
        "test_loader_length  = len(list(iter(test_loader)))\n",
        "\n",
        "train_loader_length, val_loader_length, test_loader_length"
      ],
      "metadata": {
        "id": "bqdkCVLt8CDj",
        "outputId": "ccc5ab13-5956-4f6c-901d-05a3853928df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4978, 648, 1291)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Designing the Model"
      ],
      "metadata": {
        "id": "1UpDB8mJnfmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout):\n",
        "        super().__init__()\n",
        "        #put padding_idx so asking the embedding layer to ignore padding\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(emb_dim, \n",
        "                           hid_dim, \n",
        "                           num_layers=num_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True)\n",
        "        self.fc = nn.Linear(hid_dim * 2, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        #text = [batch size, seq len]\n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        #++ pack sequence ++\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), enforce_sorted=False, batch_first=True)\n",
        "        \n",
        "        #embedded = [batch size, seq len, embed dim]\n",
        "        packed_output, (hn, cn) = self.lstm(packed_embedded)  #if no h0, all zeroes\n",
        "        \n",
        "        #++ unpack in case we need to use it ++\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "        \n",
        "        #output = [batch size, seq len, hidden dim * num directions]\n",
        "        #output over padding tokens are zero tensors\n",
        "        \n",
        "        #hidden = [num layers * num directions, batch size, hid dim]\n",
        "        #cell = [num layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
        "        #hn = [batch size, hidden dim * num directions]\n",
        "        \n",
        "        return self.fc(hn)"
      ],
      "metadata": {
        "id": "ukYLTp61mz7K"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Training"
      ],
      "metadata": {
        "id": "07qjuyyElNf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#explicitly initialize weights for better learning\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "        nn.init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.LSTM):\n",
        "        for name, param in m.named_parameters():\n",
        "            if 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "            elif 'weight' in name:\n",
        "                nn.init.orthogonal_(param)"
      ],
      "metadata": {
        "id": "KLJ_HN5MlP9R"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the parameters for the LSTM model\n",
        "input_dim  = len(vocab)\n",
        "hid_dim    = 256\n",
        "emb_dim    = 300\n",
        "output_dim = 5\n",
        "\n",
        "#for biLSTM\n",
        "num_layers = 2\n",
        "bidirectional = True\n",
        "dropout = 0.5\n",
        "\n",
        "model = LSTM(input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout).to(device)\n",
        "model.apply(initialize_weights)\n",
        "model.embedding.weight.data = fast_embeddings #**<------applied the fast text embedding as the initial weights"
      ],
      "metadata": {
        "id": "Kpbt11fOl66t"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model's parameters\n",
        "def count_parameters(model):\n",
        "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
        "    for item in params:\n",
        "        print(f'{item:>6}')\n",
        "    print(f'______\\n{sum(params):>6}')\n",
        "    \n",
        "count_parameters(model)"
      ],
      "metadata": {
        "id": "Dtn6ak9fzysR",
        "outputId": "0c91d7ca-5aa1-4551-d200-292334e25ea1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5140800\n",
            "307200\n",
            "262144\n",
            "  1024\n",
            "  1024\n",
            "307200\n",
            "262144\n",
            "  1024\n",
            "  1024\n",
            "524288\n",
            "262144\n",
            "  1024\n",
            "  1024\n",
            "524288\n",
            "262144\n",
            "  1024\n",
            "  1024\n",
            "  2560\n",
            "     5\n",
            "______\n",
            "7863109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the hyperparameters for training\n",
        "import torch.optim as optim\n",
        "\n",
        "lr=1e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "DnKReaX-m1qf"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to calculate prediction accuracy\n",
        "def accuracy(preds, y):\n",
        "    \n",
        "    predicted = torch.max(preds.data, 1)[1]\n",
        "    batch_corr = (predicted == y).sum()\n",
        "    acc = batch_corr / len(y)\n",
        "    \n",
        "    return acc"
      ],
      "metadata": {
        "id": "zdrxQVWbnS2M"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to train the model\n",
        "def train(model, loader, optimizer, criterion, loader_length):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train() #useful for batchnorm and dropout\n",
        "    \n",
        "    for i, (label, text, text_length) in enumerate(loader): \n",
        "        label = label.to(device) #(batch_size, )\n",
        "        text = text.to(device) #(batch_size, seq len)\n",
        "                \n",
        "        #predict\n",
        "        predictions = model(text, text_length).squeeze(1) #output by the fc is (batch_size, 1), thus need to remove this 1\n",
        "        \n",
        "        #calculate loss\n",
        "        loss = criterion(predictions, label)\n",
        "        acc  = accuracy(predictions, label)\n",
        "        \n",
        "        #backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "                        \n",
        "    return epoch_loss / loader_length, epoch_acc / loader_length"
      ],
      "metadata": {
        "id": "rTcFpM3SnyfP"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to evaluate the model\n",
        "def evaluate(model, loader, criterion, loader_length):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, (label, text, text_length) in enumerate(loader): \n",
        "            label = label.to(device) #(batch_size, )\n",
        "            text  = text.to(device)  #(seq len, batch_size)\n",
        "\n",
        "            predictions = model(text, text_length).squeeze(1) \n",
        "            \n",
        "            loss = criterion(predictions, label)\n",
        "            acc  = accuracy(predictions, label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / loader_length, epoch_acc / loader_length"
      ],
      "metadata": {
        "id": "MUFiJ9ahoYvZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to calculate training time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "vlViLxowp6TP"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "best_val_loss = float('inf')\n",
        "num_epochs      = 5\n",
        "\n",
        "save_path = f'models/{model.__class__.__name__}.pt'\n",
        "\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, train_loader_length)\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, val_loader_length)\n",
        "    \n",
        "    #for plotting\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_acc*100:.2f}%')"
      ],
      "metadata": {
        "id": "KhHAy6xFqGCb",
        "outputId": "4a812fb8-e23c-40c3-f3a7-ea359058d1ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-ff41c4a50ecb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-27eb019f629e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion, loader_length)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fvXYAidysmhT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}