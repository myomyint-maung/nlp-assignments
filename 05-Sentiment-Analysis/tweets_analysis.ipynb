{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Twitter tweets with snscrape\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "query = \"#bitcoin\"\n",
    "tweets = []\n",
    "limit = 100\n",
    "\n",
    "for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "\n",
    "    if len(tweets) == limit:\n",
    "        break\n",
    "    else:\n",
    "        if tweet.lang=='en':\n",
    "            tweets.append(str(tweet.rawContent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Tweet   100 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 928.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# Store the tweets in a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(tweets, columns=['Tweet'])\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@flabellina212 @elonmusk @BillyM2k @MoonlitMon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@elonmusk Thanks for explaining that meme, big...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Ebsolas @sxtvik @elonmusk @ahmadtariq07 1. Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@elonmusk What if there is no power though?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RonWaxman @elonmusk @ScottAdamsSays Cry harder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet\n",
       "0  @flabellina212 @elonmusk @BillyM2k @MoonlitMon...\n",
       "1  @elonmusk Thanks for explaining that meme, big...\n",
       "2  @Ebsolas @sxtvik @elonmusk @ahmadtariq07 1. Th...\n",
       "3        @elonmusk What if there is no power though?\n",
       "4    @RonWaxman @elonmusk @ScottAdamsSays Cry harder"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model environment\n",
    "import torch, torchtext\n",
    "from torch import nn\n",
    "import pytreebank\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sst = pytreebank.load_sst()\n",
    "train  = sst['train']\n",
    "\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for data in data_iter:\n",
    "        for _, text in data.to_labeled_lines():\n",
    "            yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train),\n",
    "                                  specials=['<unk>','<pad>','<bos>','<eos>'])\n",
    "\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "text_pipeline  = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "pad_idx = vocab['<pad>']\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        #put padding_idx so asking the embedding layer to ignore padding\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(emb_dim, \n",
    "                           hid_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hid_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        #text = [batch size, seq len]\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        #++ pack sequence ++\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), enforce_sorted=False, batch_first=True)\n",
    "        \n",
    "        #embedded = [batch size, seq len, embed dim]\n",
    "        packed_output, (hn, cn) = self.lstm(packed_embedded)  #if no h0, all zeroes\n",
    "        \n",
    "        #++ unpack in case we need to use it ++\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        #output = [batch size, seq len, hidden dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        #hn = [batch size, hidden dim * num directions]\n",
    "        \n",
    "        return self.fc(hn)\n",
    "\n",
    "input_dim       = len(vocab)\n",
    "hid_dim         = 256\n",
    "emb_dim         = 300\n",
    "output_dim      = 2\n",
    "num_layers      = 2\n",
    "bidirectional   = True\n",
    "dropout         = 0.5\n",
    "\n",
    "model = LSTM(input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LSTM:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([5, 512]) from checkpoint, the shape in current model is torch.Size([2, 512]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([5]) from checkpoint, the shape in current model is torch.Size([2]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Load the pretrained LSTM model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m save_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodels/\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(save_path, map_location\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(device)))\n",
      "File \u001b[1;32mc:\\Users\\MARC\\anaconda3\\envs\\my_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1666\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   1667\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1668\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1670\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 1671\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1672\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1673\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LSTM:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([5, 512]) from checkpoint, the shape in current model is torch.Size([2, 512]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([5]) from checkpoint, the shape in current model is torch.Size([2])."
     ]
    }
   ],
   "source": [
    "# Load the pretrained LSTM model\n",
    "save_path = f'models/{model.__class__.__name__}.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(save_path, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the sentiments of the tweets\n",
    "def predict(text, text_length):\n",
    "    with torch.no_grad():\n",
    "        output = model(text, text_length).squeeze(1)\n",
    "        predicted = torch.max(output.data, 1)[1]\n",
    "        return predicted\n",
    "\n",
    "sentiment = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweet = torch.tensor(text_pipeline(tweet))\n",
    "    tweet = tweet.reshape(1, -1)\n",
    "    tweet_length = torch.tensor([tweet.size(1)]).to(dtype=torch.int64)\n",
    "    prediction = int(predict(tweet, tweet_length))\n",
    "    sentiment.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the sentiments in the dataframe\n",
    "df['Sentiment'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@elonmusk @MKBHD Being scammed is Twitter's ow...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@elonmusk \"SMS is less secure \"\\nBut you have ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@DogeDillionaire @elonmusk Good pitch 👍. I thi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4-React to this situation yourself. The invest...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-Gala took the token and gave it to Pgala.\\n2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Sentiment\n",
       "0  @elonmusk @MKBHD Being scammed is Twitter's ow...          1\n",
       "1  @elonmusk \"SMS is less secure \"\\nBut you have ...          1\n",
       "2  @DogeDillionaire @elonmusk Good pitch 👍. I thi...          1\n",
       "3  4-React to this situation yourself. The invest...          1\n",
       "4  1-Gala took the token and gave it to Pgala.\\n2...          0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.value_counts of 0     1\n",
       "1     1\n",
       "2     1\n",
       "3     1\n",
       "4     0\n",
       "     ..\n",
       "95    1\n",
       "96    2\n",
       "97    3\n",
       "98    1\n",
       "99    1\n",
       "Name: Sentiment, Length: 100, dtype: int64>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Sentiment.value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My_Virtual_Environment",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7ca07084f99cae884d00a2401f5a915152405bd446c47609c53897335e04337"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
